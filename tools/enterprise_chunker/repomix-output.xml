This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
__init__.py
.claude/settings.local.json
chunker.py
CLAUDE.md
COMPREHENSIVE_GUIDE.md
config.py
enterprise-chunker-guide.md
enterprise-chunker.md
exceptions.py
models/__init__.py
models/chunk_metadata.py
models/content_features.py
models/enums.py
models/user.py
orchestrator.py
patterns/__init__.py
patterns/regex_patterns.py
readme.md
setup.py
strategies/__init__.py
strategies/base.py
strategies/fixed_size.py
strategies/formats/__init__.py
strategies/formats/json_chunker.py
strategies/formats/markdown_chunker.py
strategies/formats/react_vue_chunker.py
strategies/formats/smalltalk_chunker.py
strategies/semantic.py
tests/smalltalk-tests.txt
tests/test_chunker.py
tests/test_orchestrator.py
tests/test_strategies/test_formats/test_smalltalk.py
utils/__init__.py
utils/format_detection.py
utils/memory_optimization.py
utils/optimized_streaming.py
utils/parallel_processing.py
utils/performance.py
utils/token_estimation.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="__init__.py">
# enterprise_chunker/__init__.py

"""
EnterpriseChunker — Advanced text chunking utility for LLM workflows.
"""

from importlib.metadata import version, PackageNotFoundError

try:
    __version__ = version("enterprise_chunker")
except PackageNotFoundError:
    __version__ = "0.0.0"  # Default version if package is not installed

from .config import ChunkingOptions  # :contentReference[oaicite:0]{index=0}&#8203;:contentReference[oaicite:1]{index=1}
from .models.enums import (           # :contentReference[oaicite:2]{index=2}&#8203;:contentReference[oaicite:3]{index=3}
    ChunkingStrategy,
    TokenEstimationStrategy,
    ContentFormat,
)
from .chunker import (                 # :contentReference[oaicite:4]{index=4}&#8203;:contentReference[oaicite:5]{index=5}
    EnterpriseChunker,
    ChunkingContext,
)
from .models.chunk_metadata import (   # :contentReference[oaicite:6]{index=6}&#8203;:contentReference[oaicite:7]{index=7}
    ChunkMetadata,
    ChunkingResult,
)
from .models.content_features import (  # :contentReference[oaicite:8]{index=8}&#8203;:contentReference[oaicite:9]{index=9}
    ContentFeatures,
)

__all__ = [
    "ChunkingOptions",
    "ChunkingStrategy",
    "TokenEstimationStrategy",
    "ContentFormat",
    "EnterpriseChunker",
    "ChunkingContext",
    "ChunkMetadata",
    "ChunkingResult",
    "ContentFeatures",
    "__version__",
]
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(ls:*)",
      "Bash(find:*)"
    ],
    "deny": []
  }
}
</file>

<file path="chunker.py">
"""
EnterpriseChunker - Main chunker class implementation
"""

import io
import re
import time
import logging
import hashlib
import math
from typing import List, Dict, Any, Optional, Union, Generator, Tuple, cast

from enterprise_chunker.models.enums import ChunkingStrategy, TokenEstimationStrategy, ContentFormat
from enterprise_chunker.models.chunk_metadata import ChunkingResult
from enterprise_chunker.config import ChunkingOptions, ConfigManager
from enterprise_chunker.utils.token_estimation import estimate_tokens
from enterprise_chunker.utils.format_detection import detect_content_format
from enterprise_chunker.strategies.base import BaseChunkingStrategy

# Configure logging
logger = logging.getLogger(__name__)

# Maximum back-scan window size (in bytes) for finding safe split points
MAX_BACKSCAN_WINDOW = 8 * 1024  # 8 KB


class ChunkingStrategyFactory:
    """Factory for creating chunking strategy instances"""
    
    @staticmethod
    def create_strategy(
        strategy_type: ChunkingStrategy, 
        format_type: ContentFormat,
        operation_id: Optional[str] = None
    ) -> BaseChunkingStrategy:
        """
        Create a chunking strategy based on strategy type and content format
        
        Args:
            strategy_type: Type of chunking strategy
            format_type: Type of content format
            operation_id: Optional operation ID to set on the strategy
            
        Returns:
            Appropriate chunking strategy instance
        """
        strategy: BaseChunkingStrategy
        
        # Format-specific strategies take precedence in STRUCTURAL mode
        if strategy_type == ChunkingStrategy.STRUCTURAL:
            if format_type == ContentFormat.JSON:
                from enterprise_chunker.strategies.formats.json_chunker import JsonChunkingStrategy
                strategy = JsonChunkingStrategy()
            elif format_type == ContentFormat.XML:
                from enterprise_chunker.strategies.formats.xml_chunker import XmlChunkingStrategy
                strategy = XmlChunkingStrategy()
            elif format_type == ContentFormat.MARKDOWN:
                from enterprise_chunker.strategies.formats.markdown_chunker import MarkdownChunkingStrategy
                strategy = MarkdownChunkingStrategy()
            elif format_type == ContentFormat.CODE:
                from enterprise_chunker.strategies.formats.code_chunker import CodeChunkingStrategy
                strategy = CodeChunkingStrategy()
            elif format_type == ContentFormat.LOGS:
                from enterprise_chunker.strategies.formats.logs_chunker import LogsChunkingStrategy
                strategy = LogsChunkingStrategy()
            elif format_type == ContentFormat.CSV:
                from enterprise_chunker.strategies.formats.csv_chunker import CsvChunkingStrategy
                strategy = CsvChunkingStrategy()
            else:
                # Default to semantic if no format-specific strategy exists
                from enterprise_chunker.strategies.semantic import SemanticChunkingStrategy
                strategy = SemanticChunkingStrategy()
        
        # Generic strategies based on strategy_type
        elif strategy_type == ChunkingStrategy.SEMANTIC:
            from enterprise_chunker.strategies.semantic import SemanticChunkingStrategy
            strategy = SemanticChunkingStrategy()
        elif strategy_type == ChunkingStrategy.SENTENCE:
            from enterprise_chunker.strategies.sentence import SentenceChunkingStrategy
            strategy = SentenceChunkingStrategy()
        elif strategy_type == ChunkingStrategy.FIXED_SIZE:
            from enterprise_chunker.strategies.fixed_size import FixedSizeChunkingStrategy
            strategy = FixedSizeChunkingStrategy()
        else:
            # Default to semantic chunking
            from enterprise_chunker.strategies.semantic import SemanticChunkingStrategy
            strategy = SemanticChunkingStrategy()
        
        # Set operation ID if provided
        if operation_id:
            strategy.set_operation_id(operation_id)
            
        return strategy


class EnterpriseChunker:
    """Enterprise-grade text chunking utility for LLM processing"""
    
    def __init__(self, options: Optional[Dict[str, Any]] = None):
        """
        Initialize the chunker with configuration options
        
        Args:
            options: Optional configuration dictionary
        """
        self.options = ConfigManager.create_options(options)
    
    def adaptive_chunk_text(
        self, 
        text: str, 
        max_tokens_per_chunk: Optional[int] = None,
        overlap_tokens: Optional[int] = None,
        strategy: Optional[Union[str, ChunkingStrategy]] = None
    ) -> List[str]:
        """
        Main entry point: Adaptively chunk text based on content format
        
        Args:
            text: Text content to chunk
            max_tokens_per_chunk: Maximum tokens per chunk (overrides class settings)
            overlap_tokens: Number of tokens to overlap between chunks
            strategy: Chunking strategy to use
            
        Returns:
            List of text chunks optimized for processing
        """
        if not text:
            return []
            
        # Create operation-specific options
        op_options = ChunkingOptions(**vars(self.options))
        if max_tokens_per_chunk:
            op_options.max_tokens_per_chunk = max_tokens_per_chunk
        if overlap_tokens:
            op_options.overlap_tokens = overlap_tokens
        
        # Force-coerce string strategy to ChunkingStrategy enum after options merging
        if strategy:
            if isinstance(strategy, str):
                try:
                    op_options.chunking_strategy = ChunkingStrategy(strategy)
                except ValueError:
                    logger.warning(f"Invalid chunking strategy: {strategy}. Using default.")
            else:
                op_options.chunking_strategy = strategy
                
        # Generate a unique operation ID
        operation_id = self._generate_operation_id(text, op_options)
        
        try:
            # Detect content format if enabled
            detected_format = detect_content_format(text) if op_options.enable_format_detection else ContentFormat.TEXT
            
            # Choose chunking method based on format and strategy
            if op_options.chunking_strategy == ChunkingStrategy.ADAPTIVE:
                # In adaptive mode, select best strategy based on content format
                if detected_format in (
                    ContentFormat.JSON, 
                    ContentFormat.XML, 
                    ContentFormat.MARKDOWN, 
                    ContentFormat.CODE, 
                    ContentFormat.LOGS, 
                    ContentFormat.CSV
                ):
                    strategy_type = ChunkingStrategy.STRUCTURAL
                else:
                    strategy_type = ChunkingStrategy.SEMANTIC
            else:
                # Use the explicitly specified strategy
                strategy_type = op_options.chunking_strategy
            
            # Create the appropriate strategy with operation_id
            chunking_strategy = ChunkingStrategyFactory.create_strategy(
                strategy_type, 
                detected_format,
                operation_id
            )
            
            # Execute the chunking
            chunk_result = chunking_strategy.chunk(text, op_options)
            
            return chunk_result.chunks
            
        except Exception as e:
            logger.error(f"Error in adaptive_chunk_text: {str(e)}", exc_info=True)
            # Fall back to simple chunking - create strategy with operation_id in constructor
            from enterprise_chunker.strategies.fixed_size import FixedSizeChunkingStrategy
            fallback_strategy = FixedSizeChunkingStrategy()
            fallback_strategy.set_operation_id(operation_id)
            result = fallback_strategy.chunk(text, op_options)
            return result.chunks
    
    def chunk_stream(
        self, 
        stream: Union[str, io.TextIOBase], 
        **kwargs
    ) -> Generator[str, None, None]:
        """
        Process a text stream by dynamically chunking it as it's read
        
        Args:
            stream: Text stream to process
            **kwargs: Additional options for chunking
            
        Yields:
            Text chunks sequentially
        """
        op_options = ChunkingOptions(**{**vars(self.options), **kwargs})
        
        # Force-coerce string strategy to ChunkingStrategy enum after options merging
        strategy = kwargs.get('strategy')
        if strategy and isinstance(strategy, str):
            try:
                op_options.chunking_strategy = ChunkingStrategy(strategy)
            except ValueError:
                logger.warning(f"Invalid chunking strategy in stream: {strategy}. Using default.")
        
        # For string input, delegate to normal chunking
        if isinstance(stream, str):
            for chunk in self.adaptive_chunk_text(stream, **kwargs):
                yield chunk
            return
        
        buffer = ""
        previous_chunk = ""
        
        try:
            for line in stream:
                buffer += line
                
                # Process buffer when it gets large enough
                if len(buffer) >= op_options.stream_buffer_size:
                    # Find a safe place to split
                    split_index = self._find_safe_split_point(buffer, op_options)
                    chunk_text = buffer[:split_index]
                    
                    # Process with overlap from previous chunk
                    if previous_chunk:
                        chunk_text = self._add_overlap(previous_chunk, chunk_text, op_options.overlap_tokens)
                    
                    # Chunk the current buffer segment
                    chunks = self.adaptive_chunk_text(chunk_text, **kwargs)
                    
                    # Yield all but the last chunk (keep last for overlap context)
                    for i in range(len(chunks) - 1):
                        yield chunks[i]
                    
                    if chunks:
                        previous_chunk = chunks[-1]
                    
                    # Update buffer
                    buffer = buffer[split_index:]
            
            # Process any remaining content
            if buffer:
                if previous_chunk:
                    buffer = self._add_overlap(previous_chunk, buffer, op_options.overlap_tokens)
                
                chunks = self.adaptive_chunk_text(buffer, **kwargs)
                for chunk in chunks:
                    yield chunk
                    
        except Exception as e:
            logger.error(f"Error in chunk_stream: {str(e)}", exc_info=True)
            # Yield any remaining buffer if possible
            if buffer:
                yield buffer
    
    def with_max_tokens(self, max_tokens: int) -> 'EnterpriseChunker':
        """
        Fluent API for setting max tokens per chunk
        
        Args:
            max_tokens: Maximum tokens per chunk
            
        Returns:
            Self for chaining
        """
        self.options.max_tokens_per_chunk = max_tokens
        return self
    
    def with_overlap(self, overlap_tokens: int) -> 'EnterpriseChunker':
        """
        Fluent API for setting overlap tokens
        
        Args:
            overlap_tokens: Number of tokens to overlap
            
        Returns:
            Self for chaining
        """
        self.options.overlap_tokens = overlap_tokens
        return self
    
    def with_strategy(self, strategy: Union[str, ChunkingStrategy]) -> 'EnterpriseChunker':
        """
        Fluent API for setting chunking strategy
        
        Args:
            strategy: Chunking strategy to use
            
        Returns:
            Self for chaining
        """
        # Force-coerce string strategy to ChunkingStrategy enum
        if isinstance(strategy, str):
            try:
                self.options.chunking_strategy = ChunkingStrategy(strategy)
            except ValueError:
                logger.warning(f"Invalid chunking strategy: {strategy}. Ignoring.")
        else:
            self.options.chunking_strategy = strategy
        return self
    
    def chunk(self, text: str) -> List[str]:
        """
        Chunk text with current configuration
        
        Args:
            text: Text to chunk
            
        Returns:
            List of text chunks
        """
        return self.adaptive_chunk_text(text)
    
    def _generate_operation_id(self, text: str, options: ChunkingOptions) -> str:
        """
        Generate a unique ID for the chunking operation
        
        Args:
            text: Input text
            options: Chunking options
            
        Returns:
            Unique operation ID
        """
        # Create a digest from content sample and options
        content_sample = text[:100] + text[-100:] if len(text) > 200 else text
        options_str = f"{options.max_tokens_per_chunk}:{options.overlap_tokens}:{options.chunking_strategy.value}"
        
        hash_input = f"{content_sample}:{options_str}:{time.time()}"
        return hashlib.md5(hash_input.encode()).hexdigest()
    
    def _find_safe_split_point(self, buffer: str, options: ChunkingOptions) -> int:
        """
        Find a safe point to split a buffer for streaming
        
        Args:
            buffer: Text buffer
            options: Chunking options
            
        Returns:
            Index where it's safe to split
        """
        # Target 80% of buffer
        target_point = math.floor(len(buffer) * 0.8)
        
        # Limit back-scan window to 8 KB to avoid O(n²) regex on large buffers
        backscan_start = max(0, target_point - MAX_BACKSCAN_WINDOW)
        scan_window = buffer[backscan_start:target_point + MAX_BACKSCAN_WINDOW]
        
        # Compute offset for positions found in the scan window
        offset = backscan_start
        
        # Try to find a natural boundary near the target point
        # Look for paragraph breaks first
        paragraph_breaks = [offset + m.start() for m in re.finditer(r'\n\s*\n', scan_window)]
        for pos in paragraph_breaks:
            if pos >= target_point:
                return pos + 2  # Include the line break
        
        # Then look for line breaks
        line_breaks = [offset + m.start() for m in re.finditer(r'\n', scan_window)]
        for pos in line_breaks:
            if pos >= target_point:
                return pos + 1  # Include the line break
        
        # Last resort: split at target point
        return target_point
    
    def _add_overlap(self, previous_chunk: str, current_text: str, overlap_tokens: int) -> str:
        """
        Create overlap between chunks
        
        Args:
            previous_chunk: Previous chunk content
            current_text: Current text
            overlap_tokens: Number of tokens to overlap
            
        Returns:
            Text with overlap added
        """
        # Calculate approximate overlap in characters
        chars_per_token = 4.0
        overlap_chars = math.ceil(overlap_tokens * chars_per_token)
        
        if len(previous_chunk) <= overlap_chars:
            return previous_chunk + current_text
            
        # Try to find a natural boundary for the overlap
        overlap_text = previous_chunk[-overlap_chars:]
        
        # Look for paragraph or sentence boundaries in the overlap
        para_match = re.search(r'\n\s*\n', overlap_text)
        if para_match:
            # Found paragraph break, use text after it
            return previous_chunk[-(overlap_chars - para_match.end()):] + current_text
        
        sentence_match = re.search(r'(?<=[.!?])\s+(?=[A-Z])', overlap_text)
        if sentence_match:
            # Found sentence break, use text after it
            return previous_chunk[-(overlap_chars - sentence_match.end()):] + current_text
        
        # No natural boundary found, use raw overlap
        return overlap_text + current_text
    
    # Context manager methods for strategy-specific chunking
    
    def semantic_context(
        self, 
        max_tokens: Optional[int] = None,
        overlap: Optional[int] = None
    ) -> 'ChunkingContext':
        """
        Context manager for semantic chunking
        
        Args:
            max_tokens: Optional max tokens override
            overlap: Optional overlap tokens override
            
        Returns:
            Context manager
        """
        return ChunkingContext(self, ChunkingStrategy.SEMANTIC, max_tokens, overlap)
    
    def structural_context(
        self, 
        max_tokens: Optional[int] = None,
        overlap: Optional[int] = None
    ) -> 'ChunkingContext':
        """
        Context manager for structural chunking
        
        Args:
            max_tokens: Optional max tokens override
            overlap: Optional overlap tokens override
            
        Returns:
            Context manager
        """
        return ChunkingContext(self, ChunkingStrategy.STRUCTURAL, max_tokens, overlap)
    
    def fixed_size_context(
        self, 
        max_tokens: Optional[int] = None,
        overlap: Optional[int] = None
    ) -> 'ChunkingContext':
        """
        Context manager for fixed-size chunking
        
        Args:
            max_tokens: Optional max tokens override
            overlap: Optional overlap tokens override
            
        Returns:
            Context manager
        """
        return ChunkingContext(self, ChunkingStrategy.FIXED_SIZE, max_tokens, overlap)
    
    def sentence_context(
        self, 
        max_tokens: Optional[int] = None,
        overlap: Optional[int] = None
    ) -> 'ChunkingContext':
        """
        Context manager for sentence-based chunking
        
        Args:
            max_tokens: Optional max tokens override
            overlap: Optional overlap tokens override
            
        Returns:
            Context manager
        """
        return ChunkingContext(self, ChunkingStrategy.SENTENCE, max_tokens, overlap)


class ChunkingContext:
    """Context manager for temporary chunking configuration"""
    
    def __init__(
        self, 
        chunker: EnterpriseChunker,
        strategy: ChunkingStrategy,
        max_tokens: Optional[int] = None,
        overlap: Optional[int] = None
    ):
        """
        Initialize the context manager
        
        Args:
            chunker: EnterpriseChunker instance
            strategy: Chunking strategy to use
            max_tokens: Optional max tokens override
            overlap: Optional overlap tokens override
        """
        self.chunker = chunker
        self.strategy = strategy
        self.max_tokens = max_tokens
        self.overlap = overlap
        self.previous_strategy = None
        self.previous_max_tokens = None
        self.previous_overlap = None
    
    def __enter__(self) -> EnterpriseChunker:
        """Set temporary chunking configuration"""
        self.previous_strategy = self.chunker.options.chunking_strategy
        self.previous_max_tokens = self.chunker.options.max_tokens_per_chunk
        self.previous_overlap = self.chunker.options.overlap_tokens
        
        self.chunker.options.chunking_strategy = self.strategy
        if self.max_tokens is not None:
            self.chunker.options.max_tokens_per_chunk = self.max_tokens
        if self.overlap is not None:
            self.chunker.options.overlap_tokens = self.overlap
        
        return self.chunker
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """Restore previous chunking configuration"""
        self.chunker.options.chunking_strategy = self.previous_strategy
        self.chunker.options.max_tokens_per_chunk = self.previous_max_tokens
        self.chunker.options.overlap_tokens = self.previous_overlap
</file>

<file path="CLAUDE.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Enterprise Chunker is an advanced text chunking utility for LLM processing with intelligent content-aware strategies. It's part of a larger omt-insights project that includes both Python backend and React frontend components.

The project is located in the `/backend/enterprise_chunker/` directory within the omt-insights repository.

## Key Commands

### Testing
```bash
# Run tests
python -m pytest tests/

# Run tests with coverage
python -m pytest tests/ --cov=enterprise_chunker --cov-report=html

# Test a specific module
python -m pytest tests/test_chunker.py
python -m pytest tests/test_orchestrator.py
python -m pytest tests/test_strategies/test_formats/test_smalltalk.py
```

### Linting and Formatting
```bash
# Format code with black
black enterprise_chunker/
black tests/

# Sort imports
isort enterprise_chunker/
isort tests/

# Type checking
mypy enterprise_chunker/

# Linting
pylint enterprise_chunker/
```

### Development Setup
```bash
# Install package in development mode
pip install -e .

# Install with development dependencies
pip install -e .[dev]

# Install with all optional dependencies
pip install -e .[all]
```

### Building and Distribution
```bash
# Build distribution packages
python setup.py sdist bdist_wheel

# Upload to PyPI (requires credentials)
python -m twine upload dist/*
```

## Project Architecture

### Module Structure
```
enterprise_chunker/
├── __init__.py                    # Package initialization
├── chunker.py                     # Main EnterpriseChunker class
├── config.py                      # Configuration and options
├── exceptions.py                  # Custom exceptions
├── orchestrator.py                # Parallel processing and orchestration
├── models/                        # Data models and enums
│   ├── __init__.py
│   ├── chunk_metadata.py          # Metadata for chunks
│   ├── content_features.py        # Content analysis features
│   ├── enums.py                   # Enumeration types
│   └── user.py                    # User-related models
├── patterns/                      # Regex and pattern definitions  
│   ├── __init__.py
│   └── regex_patterns.py          # Regular expression patterns
├── strategies/                    # Chunking strategies
│   ├── __init__.py
│   ├── base.py                    # Base strategy class
│   ├── fixed_size.py              # Fixed-size chunking
│   ├── semantic.py                # Semantic chunking
│   └── formats/                   # Format-specific strategies
│       ├── __init__.py
│       ├── json_chunker.py        # JSON handling
│       ├── markdown_chunker.py    # Markdown handling
│       ├── react_vue_chunker.py   # React/Vue components
│       └── smalltalk_chunker.py   # Smalltalk code
├── utils/                         # Utility modules
│   ├── __init__.py
│   ├── format_detection.py        # Content format detection
│   ├── memory_optimization.py     # Memory management
│   ├── optimized_streaming.py     # Streaming utilities
│   ├── parallel_processing.py     # Parallel execution
│   ├── performance.py             # Performance monitoring
│   └── token_estimation.py        # Token counting
└── tests/                         # Test suite
    ├── test_chunker.py
    ├── test_orchestrator.py
    └── test_strategies/
        └── test_formats/
            └── test_smalltalk.py
```

### Key Classes and Their Responsibilities

1. **EnterpriseChunker** (`chunker.py`): Main entry point for chunking operations
   - Adaptive text chunking based on content format
   - Fluent API for configuration
   - Context managers for temporary settings

2. **ChunkingOptions** (`config.py`): Configuration management
   - Token limits and overlap settings
   - Strategy selection
   - Performance parameters

3. **SmartParallelChunker** (`orchestrator.py`): Advanced parallel processing
   - Dynamic resource management
   - Circuit breaker pattern
   - Performance monitoring

4. **Format-specific strategies** (`strategies/formats/`):
   - JsonChunkingStrategy: Preserves JSON structure
   - MarkdownChunkingStrategy: Respects markdown hierarchy
   - ReactVueChunkingStrategy: Handles React/Vue components
   - SmalltalkChunkingStrategy: Processes Smalltalk code

5. **Utilities** (`utils/`):
   - TokenEstimator: Estimates token count for text
   - MemoryManager: Handles memory-efficient processing
   - FormatDetector: Automatically detects content format

## Development Workflow

### Adding New Features

1. Create feature branch from main
2. Add tests for new functionality
3. Implement the feature
4. Run tests and linting
5. Update documentation if needed
6. Create pull request

### Testing Guidelines

1. Each new strategy should have comprehensive tests
2. Test edge cases and error conditions
3. Include performance tests for utility functions
4. Mock external dependencies appropriately

### Code Style

- Follow PEP 8 conventions
- Use type annotations throughout
- Document all public APIs
- Keep functions focused and modular
- Prefer composition over inheritance

### Performance Considerations

1. Use generators for large datasets
2. Implement proper memory management
3. Cache expensive computations
4. Profile performance-critical code
5. Add metrics for monitoring

## Environment Variables

The chunker can be configured via environment variables:

```bash
export CHUNKER_MAX_TOKENS_PER_CHUNK=4000
export CHUNKER_OVERLAP_TOKENS=200
export CHUNKER_CHUNKING_STRATEGY=semantic
export CHUNKER_TOKEN_STRATEGY=precision
```

## Integration Points

### Backend Integration
- Part of the omt-insights backend services
- Used by the main FastAPI application
- Integrated with other processing services

### Monitoring Integration
- Prometheus metrics endpoint
- Health check endpoints
- Performance dashboards

## Common Development Tasks

### Running the Development Server
For the main FastAPI application that uses Enterprise Chunker:
```bash
cd backend
python main.py
```

### Testing in Isolation
To test Enterprise Chunker separately:
```python
from enterprise_chunker import EnterpriseChunker

chunker = EnterpriseChunker()
chunks = chunker.chunk("Your test text here")
```

### Debugging Performance
```python
from enterprise_chunker.orchestrator import create_auto_chunker

chunker = create_auto_chunker(
    options=options,
    enable_metrics_server=True,
    metrics_port=8000
)
# Access metrics at http://localhost:8000/metrics
```

## Important Notes

1. The project uses a mixed Python/Node.js stack
2. Enterprise Chunker is a self-contained Python module
3. No custom configuration files for linting tools - uses defaults
4. Part of a larger project with frontend and other backend services
5. Uses semantic versioning for releases

## Dependencies

Core dependencies:
- Python 3.8+
- psutil>=5.9.0
- numpy>=1.22.0  
- prometheus_client>=0.14.0
- requests>=2.27.0

Development dependencies:
- pytest>=7.0.0
- pytest-cov>=4.0.0
- black>=23.0.0
- isort>=5.0.0
- mypy>=1.0.0
- pylint>=2.17.0
</file>

<file path="COMPREHENSIVE_GUIDE.md">
# 🚀 Enterprise Chunker: The Complete Guide (For Dummies to Expert Level)

## 📖 1. Overview & Introduction

### What is Enterprise Chunker?

Enterprise Chunker is like a smart text-splitting tool that breaks down large documents into smaller, digestible pieces for AI systems to process. Think of it as a skilled librarian who knows exactly how to organize a massive encyclopedia into manageable volumes that a student can read one at a time.

### The Chocolate Bar Analogy 🍫

Imagine you have a giant chocolate bar that's too big to eat in one bite. Enterprise Chunker breaks it into perfectly sized pieces that:
- Keep the flavor intact (preserve meaning)
- Don't crumble at the edges (maintain structure)
- Are just the right size for your mouth (fit AI processing limits)

### Why Does It Exist?

Modern AI language models (like ChatGPT or Claude) have a "reading limit" - they can only process a certain amount of text at once. Enterprise Chunker solves this by:
- Intelligently dividing large documents into processable chunks
- Preserving the document's meaning and structure
- Ensuring AI systems understand the full context

### Real-World Benefits

- **Process entire books**: Break down 500-page documents for AI analysis
- **Analyze code repositories**: Split large codebases while keeping functions intact
- **Handle structured data**: Process massive JSON files without breaking their structure
- **Maintain context**: Ensure AI understands relationships between document parts

---

## 🛠️ 2. Installation & Necessary Dependencies

### Prerequisites

Before installing Enterprise Chunker, ensure you have:
- Python 3.8 or higher installed
- pip (Python package manager)
- Basic command line knowledge

### Step-by-Step Installation

#### For Beginners (Basic Installation)

```bash
# Step 1: Install Enterprise Chunker
pip install enterprise-chunker

# That's it! You're ready to use the basic features
```

#### For Intermediate Users (With Development Tools)

```bash
# Step 1: Install with development dependencies
pip install enterprise-chunker[dev]

# This includes testing and linting tools
```

#### For Experts (Full Installation)

```bash
# Step 1: Clone the repository
git clone https://github.com/your-org/enterprise-chunker.git
cd enterprise-chunker

# Step 2: Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Step 3: Install in development mode with all dependencies
pip install -e .[all]
```

### Troubleshooting Common Installation Issues

**Issue: "pip: command not found"**
```bash
# Solution: Install pip
python -m ensurepip --upgrade
```

**Issue: "Python version not supported"**
```bash
# Solution: Check your Python version
python --version

# If needed, install Python 3.8+
# Visit https://python.org/downloads
```

**Issue: "Permission denied"**
```bash
# Solution: Use virtual environment or user install
pip install --user enterprise-chunker
```

---

## 🔍 3. Detailed Functional Breakdown

### a. Adaptive Chunking

#### ELI5 Explanation
Adaptive chunking is like having a smart assistant who looks at your document and automatically decides the best way to split it. If it's a story, it splits at chapter breaks. If it's code, it keeps functions together.

#### Real-World Example (For Dummies Style)
Imagine organizing a cookbook:
- Adaptive chunking recognizes it's a cookbook
- Splits by complete recipes, not in the middle of ingredient lists
- Keeps cooking instructions together

#### How to Use (Step-by-Step)

```python
from enterprise_chunker import EnterpriseChunker

# Step 1: Create a chunker
chunker = EnterpriseChunker()

# Step 2: Read your document
with open('my_document.txt', 'r') as file:
    text = file.read()

# Step 3: Let the chunker work its magic
chunks = chunker.adaptive_chunk_text(text)

# Step 4: Use your chunks
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {len(chunk)} characters")
```

#### Impact & Importance
- **Preserves meaning**: Keeps related information together
- **Saves processing time**: AI understands context better
- **Reduces errors**: Prevents breaking important structures

### b. Format-Specific Chunking

#### ELI5 Explanation
Different types of files need different splitting strategies. JSON files need to keep their curly braces matched, Markdown files should split at headings, and code files should keep functions intact.

#### Real-World Example (For Dummies Style)
It's like cutting different foods:
- **Pizza**: Cut along the pre-marked slices
- **Cake**: Cut in neat squares
- **Bread**: Slice evenly
- **Sandwich**: Cut diagonally

Each food has its natural cutting pattern!

#### How to Use (Step-by-Step)

```python
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy

chunker = EnterpriseChunker()

# For JSON files
json_chunks = chunker.adaptive_chunk_text(
    json_text,
    strategy=ChunkingStrategy.STRUCTURAL
)

# For Markdown documents
markdown_chunks = chunker.adaptive_chunk_text(
    markdown_text,
    strategy=ChunkingStrategy.STRUCTURAL
)

# For code files
code_chunks = chunker.adaptive_chunk_text(
    code_text,
    strategy=ChunkingStrategy.STRUCTURAL
)
```

#### Impact & Importance
- **Maintains validity**: JSON remains parseable, code remains runnable
- **Preserves structure**: Document organization is maintained
- **Improves AI understanding**: Context-aware splitting leads to better analysis

### c. Memory-Efficient Processing

#### ELI5 Explanation
Memory-efficient processing is like washing dishes as you cook instead of piling them all up. It processes documents piece by piece instead of loading everything at once.

#### Real-World Example (For Dummies Style)
Imagine moving houses:
- **Without memory efficiency**: Try to carry everything at once (exhausting!)
- **With memory efficiency**: Make multiple trips with manageable loads

#### How to Use (Step-by-Step)

```python
from enterprise_chunker import EnterpriseChunker

# For large files, use streaming
chunker = EnterpriseChunker()

# Process file without loading it all into memory
with open('huge_file.txt', 'r') as file:
    for chunk in chunker.chunk_stream(file):
        # Process each chunk as it's created
        process_chunk(chunk)
```

#### Impact & Importance
- **Handles massive files**: Process gigabyte-sized documents
- **Prevents crashes**: Avoids out-of-memory errors
- **Improves performance**: Starts processing immediately

### d. Parallel Processing

#### ELI5 Explanation
Parallel processing is like having multiple chefs in a kitchen - they can prepare different dishes simultaneously, making the overall meal ready faster.

#### Real-World Example (For Dummies Style)
Building a house:
- **Sequential**: One worker does everything (slow)
- **Parallel**: Plumber, electrician, and painter work simultaneously (fast)

#### How to Use (Step-by-Step)

```python
from enterprise_chunker.orchestrator import create_auto_chunker
from enterprise_chunker.config import ChunkingOptions

# Step 1: Configure options
options = ChunkingOptions(
    max_tokens_per_chunk=1000,
    overlap_tokens=100
)

# Step 2: Create parallel chunker
chunker = create_auto_chunker(
    options=options,
    mode="performance"  # Optimized for speed
)

# Step 3: Process with parallelism
chunks = chunker.chunk(large_text, my_chunking_function)
```

#### Impact & Importance
- **Dramatic speed improvement**: 4-6x faster on multi-core systems
- **Better resource utilization**: Uses available CPU cores
- **Scalable processing**: Handles large document collections efficiently

---

## 🎓 4. Gradual Complexity & Advanced Usage

### Beginner Level: Simple Configuration

```python
from enterprise_chunker import EnterpriseChunker

# Basic usage with defaults
chunker = EnterpriseChunker()
chunks = chunker.chunk("Your text here")
```

### Intermediate Level: Custom Configuration

```python
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy

# Configure chunk size and overlap
chunks = chunker.adaptive_chunk_text(
    text,
    max_tokens_per_chunk=2000,  # Larger chunks
    overlap_tokens=150,         # More context preservation
    strategy=ChunkingStrategy.SEMANTIC  # Specific strategy
)

# Using fluent API
result = chunker.with_max_tokens(1500)\
                .with_overlap(100)\
                .with_strategy(ChunkingStrategy.STRUCTURAL)\
                .chunk(document)
```

### Expert Level: Advanced Orchestration

```python
from enterprise_chunker.orchestrator import create_auto_chunker, DynamicConfig
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.models.enums import TokenEstimationStrategy

# Advanced configuration
config = DynamicConfig({
    'processing_timeout': 300.0,
    'max_retries': 3,
    'memory_safety': True,
    'enable_ml_segmentation': True
})

# Detailed options
options = ChunkingOptions(
    max_tokens_per_chunk=4000,
    overlap_tokens=200,
    token_strategy=TokenEstimationStrategy.PRECISION,
    preserve_structure=True,
    safety_margin=0.95,
    stream_buffer_size=200000
)

# Create sophisticated chunker
smart_chunker = create_auto_chunker(
    options=options,
    mode="balanced",
    memory_safety=True,
    config=config,
    enable_metrics_server=True,
    metrics_port=8000
)

# Process with priority
high_priority_chunks = smart_chunker.chunk_with_priority(
    critical_document,
    chunking_function,
    priority="high"
)

# Monitor performance
metrics = smart_chunker.get_metrics()
print(f"Throughput: {metrics['avg_throughput']:.2f} chunks/second")
```

---

## 📚 5. Step-by-Step Practical Examples & Tutorials

### Example 1: Processing a Book (Basic Level)

```python
from enterprise_chunker import EnterpriseChunker

def process_book(book_path):
    # Initialize chunker
    chunker = EnterpriseChunker()
    
    # Read the book
    with open(book_path, 'r', encoding='utf-8') as file:
        book_text = file.read()
    
    # Chunk it
    chapters = chunker.adaptive_chunk_text(
        book_text,
        max_tokens_per_chunk=3000,  # About 10-12 pages
        overlap_tokens=200          # Keep context between chapters
    )
    
    # Process each chapter
    for i, chapter in enumerate(chapters):
        print(f"Chapter {i+1}: {chapter[:100]}...")
        # Send to AI for analysis
        analyze_chapter(chapter)
    
    return chapters

# Usage
chapters = process_book('war_and_peace.txt')
print(f"Book split into {len(chapters)} readable chunks")
```

### Example 2: Analyzing a Code Repository (Intermediate Level)

```python
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy
import os

def analyze_codebase(repo_path):
    chunker = EnterpriseChunker()
    code_chunks = []
    
    # Walk through all Python files
    for root, dirs, files in os.walk(repo_path):
        for file in files:
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                
                # Read code file
                with open(file_path, 'r', encoding='utf-8') as f:
                    code = f.read()
                
                # Chunk with code-aware strategy
                chunks = chunker.adaptive_chunk_text(
                    code,
                    strategy=ChunkingStrategy.STRUCTURAL,
                    max_tokens_per_chunk=1500
                )
                
                # Store with metadata
                for chunk in chunks:
                    code_chunks.append({
                        'file': file_path,
                        'chunk': chunk,
                        'language': 'python'
                    })
    
    # Analyze chunks
    for chunk_data in code_chunks:
        analyze_code_quality(chunk_data)
    
    return code_chunks

# Usage
code_analysis = analyze_codebase('/path/to/repository')
print(f"Analyzed {len(code_analysis)} code chunks")
```

### Example 3: Real-time Document Processing System (Expert Level)

```python
from enterprise_chunker.orchestrator import create_auto_chunker
from enterprise_chunker.config import ChunkingOptions
import asyncio
import aiohttp
from typing import List, Dict

class DocumentProcessor:
    def __init__(self):
        # Configure chunker
        options = ChunkingOptions(
            max_tokens_per_chunk=2000,
            overlap_tokens=150
        )
        
        self.chunker = create_auto_chunker(
            options=options,
            mode="performance",
            enable_metrics_server=True,
            metrics_port=8000
        )
    
    async def process_document_stream(self, document_stream):
        """Process documents from a stream with parallel chunking"""
        tasks = []
        
        async for document in document_stream:
            # Create chunking task
            task = asyncio.create_task(
                self.chunk_and_analyze(document)
            )
            tasks.append(task)
            
            # Limit concurrent processing
            if len(tasks) >= 10:
                done, tasks = await asyncio.wait(
                    tasks, 
                    return_when=asyncio.FIRST_COMPLETED
                )
                # Handle completed tasks
                for task in done:
                    result = await task
                    await self.store_results(result)
        
        # Process remaining tasks
        if tasks:
            results = await asyncio.gather(*tasks)
            for result in results:
                await self.store_results(result)
    
    async def chunk_and_analyze(self, document):
        """Chunk document and analyze with AI"""
        # Prioritize based on document importance
        priority = self.determine_priority(document)
        
        # Chunk with priority
        chunks = self.chunker.chunk_with_priority(
            document['content'],
            self.custom_chunker,
            priority=priority
        )
        
        # Analyze chunks concurrently
        analyses = await asyncio.gather(*[
            self.analyze_with_ai(chunk) for chunk in chunks
        ])
        
        return {
            'document_id': document['id'],
            'chunks': chunks,
            'analyses': analyses,
            'metrics': self.chunker.get_metrics()
        }
    
    def custom_chunker(self, text):
        # Custom chunking logic
        return text.split('\n\n')
    
    def determine_priority(self, document):
        # Prioritize based on document metadata
        if document.get('urgent', False):
            return "high"
        elif document.get('size', 0) > 1000000:
            return "background"
        return "normal"
    
    async def analyze_with_ai(self, chunk):
        # Simulate AI analysis
        async with aiohttp.ClientSession() as session:
            async with session.post(
                'https://api.ai-service.com/analyze',
                json={'text': chunk}
            ) as response:
                return await response.json()
    
    async def store_results(self, results):
        # Store in database
        print(f"Storing results for document {results['document_id']}")
        # Implementation here

# Usage
async def main():
    processor = DocumentProcessor()
    
    # Simulate document stream
    async def document_stream():
        documents = [
            {'id': 1, 'content': 'Large document...', 'urgent': True},
            {'id': 2, 'content': 'Another document...'},
            # ... more documents
        ]
        for doc in documents:
            yield doc
    
    # Process documents
    await processor.process_document_stream(document_stream())
    
    # Check metrics
    metrics = processor.chunker.get_metrics()
    print(f"Processed {metrics['total_chunks_processed']} chunks")
    print(f"Average throughput: {metrics['avg_throughput']:.2f} chunks/second")

# Run the system
asyncio.run(main())
```

---

## 🚩 6. Red Flags, Common Issues & Troubleshooting

### Memory Issues

**Symptoms:**
- Program crashes with `MemoryError`
- System becomes slow or unresponsive
- "Out of memory" messages

**Solutions:**

```python
# 1. Enable memory safety mode
from enterprise_chunker.utils.memory_optimization import MemoryManager

memory_manager = MemoryManager(low_memory_mode=True)
with memory_manager.memory_efficient_context():
    chunks = chunker.chunk(large_text)

# 2. Use streaming for large files
with open('huge_file.txt', 'r') as file:
    for chunk in chunker.chunk_stream(file):
        process_chunk(chunk)

# 3. Create memory-safe chunker
chunker = create_auto_chunker(
    options=options,
    mode="memory-safe",
    memory_safety=True
)
```

### Incorrect Chunk Boundaries

**Symptoms:**
- Code is split in the middle of functions
- JSON chunks are invalid
- Markdown headers separated from content

**Solutions:**

```python
# 1. Use appropriate strategy
chunks = chunker.adaptive_chunk_text(
    code_text,
    strategy=ChunkingStrategy.STRUCTURAL  # For structured content
)

# 2. Increase overlap for better context
chunks = chunker.adaptive_chunk_text(
    text,
    overlap_tokens=300  # More overlap for complex documents
)

# 3. Enable format detection
from enterprise_chunker.config import ChunkingOptions

options = ChunkingOptions(
    enable_format_detection=True,
    preserve_structure=True
)
```

### Performance Problems

**Symptoms:**
- Slow processing speed
- High CPU usage
- Long wait times

**Solutions:**

```python
# 1. Use parallel processing
chunker = create_auto_chunker(
    options=options,
    mode="performance"
)

# 2. Adjust token estimation
options = ChunkingOptions(
    token_strategy=TokenEstimationStrategy.PERFORMANCE
)

# 3. Enable adaptive batch sizing
chunker = create_auto_chunker(
    options=options,
    adaptive_batch_sizing=True
)
```

### Troubleshooting Flowchart

```
Problem occurs
    ↓
Is it memory-related?
    Yes → Use streaming or memory-safe mode
    No ↓
Are chunks incorrectly split?
    Yes → Check strategy and format detection
    No ↓
Is it running too slowly?
    Yes → Enable parallel processing
    No ↓
Check logs for specific errors
```

---

## ✅ 7. Additional Tips & Best Practices

### Optimizing for Your Use Case

1. **Know Your LLM's Limits**
   ```python
   # For GPT-4 (8K context)
   options = ChunkingOptions(
       max_tokens_per_chunk=7500,  # Leave room for response
       safety_margin=0.9
   )
   
   # For Claude (100K context)
   options = ChunkingOptions(
       max_tokens_per_chunk=95000,  # Much larger chunks
       safety_margin=0.95
   )
   ```

2. **Adjust Overlap Based on Content**
   - Technical documentation: 10-15% overlap
   - Narrative content: 15-20% overlap
   - Code: 5-10% overlap

3. **Use Context Managers for Clean Code**
   ```python
   # Temporary configuration
   with chunker.semantic_context(max_tokens=1000):
       chunks = chunker.chunk(text)
   ```

### Performance Best Practices

1. **Stream Large Files**
   ```python
   # Always stream files over 10MB
   def process_large_file(filepath):
       with open(filepath, 'r') as file:
           for chunk in chunker.chunk_stream(file):
               yield process_chunk(chunk)
   ```

2. **Cache Results When Possible**
   ```python
   from functools import lru_cache
   
   @lru_cache(maxsize=100)
   def get_chunks(document_id):
       return chunker.chunk(get_document(document_id))
   ```

3. **Monitor Resource Usage**
   ```python
   # Enable metrics
   chunker = create_auto_chunker(
       options=options,
       enable_metrics_server=True
   )
   
   # Check metrics periodically
   metrics = chunker.get_metrics()
   if metrics['memory_percent'] > 80:
       logger.warning("High memory usage detected")
   ```

### Integration Best Practices

1. **Error Handling**
   ```python
   try:
       chunks = chunker.adaptive_chunk_text(text)
   except Exception as e:
       logger.error(f"Chunking failed: {e}")
       # Fallback to simple chunking
       chunks = chunker.adaptive_chunk_text(
           text,
           strategy=ChunkingStrategy.FIXED_SIZE
       )
   ```

2. **Logging and Monitoring**
   ```python
   import logging
   
   logging.basicConfig(level=logging.INFO)
   logger = logging.getLogger(__name__)
   
   # Log chunking operations
   logger.info(f"Chunking document: {document_id}")
   chunks = chunker.chunk(document)
   logger.info(f"Created {len(chunks)} chunks")
   ```

3. **Testing Your Implementation**
   ```python
   def test_chunking():
       test_cases = [
           ("Simple text", 100, 10),
           ('{"json": "data"}', 50, 5),
           ("# Markdown\n\nContent", 100, 20)
       ]
       
       for text, max_tokens, overlap in test_cases:
           chunks = chunker.adaptive_chunk_text(
               text,
               max_tokens_per_chunk=max_tokens,
               overlap_tokens=overlap
           )
           assert len(chunks) > 0
           assert all(len(chunk) > 0 for chunk in chunks)
   ```

---

## 📌 8. Glossary & Definitions (Jargon-Buster)

| Term | Definition |
|------|------------|
| **Chunk** | A piece of text that has been split from a larger document |
| **Token** | The basic unit of text that AI models process (roughly 3-4 characters) |
| **Context Window** | The maximum amount of text an AI model can process at once |
| **Overlap** | Text repeated between adjacent chunks to maintain context |
| **Strategy** | The method used to determine where to split text |
| **Boundary** | A natural breaking point in text (like a paragraph or section) |
| **Streaming** | Processing data continuously without loading it all at once |
| **Parallel Processing** | Using multiple CPU cores to process different parts simultaneously |
| **Circuit Breaker** | A pattern that stops operations when errors exceed a threshold |
| **Token Estimation** | Predicting how many tokens a piece of text will use |
| **Format Detection** | Automatically identifying the type of content (JSON, Markdown, etc.) |
| **Memory Safety** | Features that prevent out-of-memory errors |
| **Fluent API** | A programming style that allows method chaining |
| **Context Manager** | A Python feature for temporary configuration (using `with`) |

---

## 🚨 9. Limitations & Known Issues

### Current Limitations

1. **Language Support**
   - Optimized primarily for English text
   - Token estimation less accurate for non-Latin scripts
   - Some chunking strategies work better with English

2. **Binary Format Support**
   - Cannot directly process images, audio, or video
   - Requires text extraction before chunking
   - No built-in OCR or transcription

3. **Performance Constraints**
   - Large documents (>1GB) require significant memory
   - Parallel processing limited by available CPU cores
   - Some strategies slower than others

### Known Issues

1. **Mixed Content Types**
   - Documents with mixed formats may not chunk optimally
   - Boundary detection can be inconsistent

2. **Token Estimation Variance**
   - 5-10% variance from actual tokenization
   - Different AI models count tokens differently

3. **Memory Leaks**
   - Rare issues in very long-running processes
   - Recommended to restart periodically

### Workarounds

```python
# For non-English text
options = ChunkingOptions(
    safety_margin=0.8,  # More conservative
    token_strategy=TokenEstimationStrategy.PRECISION
)

# For mixed content
# Pre-process to separate different sections
sections = split_by_format(document)
chunks = []
for section in sections:
    chunks.extend(chunker.chunk(section))

# For memory leaks in long-running processes
import gc
import schedule

def cleanup():
    gc.collect()
    chunker.reset_metrics()

schedule.every(1).hours.do(cleanup)
```

---

## 🧩 10. Integration & Ecosystem Context

### Integration with AI Frameworks

#### LangChain Integration
```python
from langchain.text_splitter import CharacterTextSplitter
from enterprise_chunker import EnterpriseChunker

class EnterpriseChunkerWrapper(CharacterTextSplitter):
    def __init__(self, **kwargs):
        self.chunker = EnterpriseChunker()
        super().__init__(**kwargs)
    
    def split_text(self, text):
        return self.chunker.adaptive_chunk_text(
            text,
            max_tokens_per_chunk=self.chunk_size,
            overlap_tokens=self.chunk_overlap
        )

# Use in LangChain
text_splitter = EnterpriseChunkerWrapper(
    chunk_size=1000,
    chunk_overlap=100
)
```

#### LlamaIndex Integration
```python
from llama_index.node_parser import SimpleNodeParser
from enterprise_chunker import EnterpriseChunker

class EnterpriseNodeParser(SimpleNodeParser):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.chunker = EnterpriseChunker()
    
    def get_nodes_from_documents(self, documents):
        nodes = []
        for doc in documents:
            chunks = self.chunker.adaptive_chunk_text(
                doc.text,
                max_tokens_per_chunk=self.chunk_size
            )
            for chunk in chunks:
                nodes.append(self._create_node(chunk, doc))
        return nodes
```

### Database Integration

```python
import sqlite3
from enterprise_chunker import EnterpriseChunker

class ChunkStorage:
    def __init__(self, db_path):
        self.conn = sqlite3.connect(db_path)
        self.chunker = EnterpriseChunker()
        self._create_tables()
    
    def _create_tables(self):
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY,
                document_id TEXT,
                chunk_index INTEGER,
                content TEXT,
                tokens INTEGER,
                metadata TEXT
            )
        ''')
    
    def store_document(self, document_id, text):
        chunks = self.chunker.adaptive_chunk_text(text)
        
        for i, chunk in enumerate(chunks):
            self.conn.execute('''
                INSERT INTO chunks 
                (document_id, chunk_index, content, tokens)
                VALUES (?, ?, ?, ?)
            ''', (document_id, i, chunk, estimate_tokens(chunk)))
        
        self.conn.commit()
```

### API Integration

```python
from fastapi import FastAPI, UploadFile
from enterprise_chunker import EnterpriseChunker

app = FastAPI()
chunker = EnterpriseChunker()

@app.post("/chunk")
async def chunk_document(file: UploadFile):
    content = await file.read()
    text = content.decode('utf-8')
    
    chunks = chunker.adaptive_chunk_text(text)
    
    return {
        "filename": file.filename,
        "chunks": len(chunks),
        "data": chunks
    }

@app.get("/health")
async def health_check():
    metrics = chunker.get_metrics()
    return {
        "status": "healthy",
        "metrics": metrics
    }
```

---

## 📑 11. Summaries & Quick Reference Cheatsheet

### Quick Command Reference

```python
# Basic Usage
from enterprise_chunker import EnterpriseChunker
chunker = EnterpriseChunker()
chunks = chunker.chunk(text)

# Custom Configuration
chunks = chunker.adaptive_chunk_text(
    text,
    max_tokens_per_chunk=1000,
    overlap_tokens=100,
    strategy=ChunkingStrategy.SEMANTIC
)

# Streaming
with open('large_file.txt', 'r') as file:
    for chunk in chunker.chunk_stream(file):
        process(chunk)

# Parallel Processing
from enterprise_chunker.orchestrator import create_auto_chunker
chunker = create_auto_chunker(options, mode="performance")

# Memory Safety
from enterprise_chunker.utils.memory_optimization import MemoryManager
with MemoryManager(low_memory_mode=True).memory_efficient_context():
    chunks = chunker.chunk(text)
```

### Strategy Selection Guide

| Content Type | Recommended Strategy | Key Settings |
|-------------|---------------------|--------------|
| General Text | SEMANTIC | overlap=15-20% |
| JSON/XML | STRUCTURAL | preserve_structure=True |
| Markdown | STRUCTURAL | respect_sections=True |
| Code | STRUCTURAL | respect_syntax=True |
| Mixed Content | ADAPTIVE | auto-detect=True |
| Large Files | FIXED_SIZE | streaming=True |

### Performance Quick Fixes

| Problem | Solution |
|---------|----------|
| Slow chunking | Use `mode="performance"` |
| Out of memory | Enable streaming |
| Wrong boundaries | Check strategy selection |
| Inconsistent chunks | Increase overlap |
| Token count wrong | Use PRECISION strategy |

---

## 🖼️ 12. Visual Aids & Supporting Diagrams

### Chunking Process Flow

```
Document Input
    ↓
Format Detection
    ↓
┌──────────────┬──────────────┬──────────────┐
│   JSON?      │  Markdown?   │    Code?     │
└──────────────┴──────────────┴──────────────┘
    ↓              ↓              ↓
JSON Strategy  MD Strategy   Code Strategy
    ↓              ↓              ↓
    └──────────────┴──────────────┘
                ↓
        Boundary Detection
                ↓
        Chunk Creation
                ↓
        Size Validation
                ↓
        Output Chunks
```

### Memory Management Architecture

```
┌─────────────────────────────────┐
│      Document Stream            │
└─────────────────────────────────┘
                ↓
┌─────────────────────────────────┐
│      Buffer (Configurable)      │
└─────────────────────────────────┘
                ↓
┌─────────────────────────────────┐
│    Chunking Engine              │
│  ┌─────────┐  ┌─────────┐      │
│  │ Worker 1│  │ Worker 2│ ...  │
│  └─────────┘  └─────────┘      │
└─────────────────────────────────┘
                ↓
┌─────────────────────────────────┐
│      Output Stream              │
└─────────────────────────────────┘
```

### Strategy Selection Decision Tree

```
Is content structured?
├─ Yes
│  ├─ JSON/XML? → STRUCTURAL
│  ├─ Markdown? → STRUCTURAL
│  └─ Code? → STRUCTURAL
└─ No
   ├─ Need sentence preservation? → SENTENCE
   ├─ Need semantic boundaries? → SEMANTIC
   └─ Need consistent sizes? → FIXED_SIZE
```

---

## 🛡️ 13. Security, Privacy & Data Handling Guidelines

### Security Best Practices

1. **Input Validation**
   ```python
   def secure_chunk(text, max_size=10*1024*1024):  # 10MB limit
       if len(text) > max_size:
           raise ValueError("Document too large")
       
       # Sanitize input
       text = text.replace('\x00', '')  # Remove null bytes
       
       return chunker.chunk(text)
   ```

2. **Sensitive Data Handling**
   ```python
   import re
   
   def remove_sensitive_data(text):
       # Remove credit card numbers
       text = re.sub(r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b', 'XXXX-XXXX-XXXX-XXXX', text)
       
       # Remove SSNs
       text = re.sub(r'\b\d{3}-\d{2}-\d{4}\b', 'XXX-XX-XXXX', text)
       
       return text
   
   # Clean before chunking
   clean_text = remove_sensitive_data(original_text)
   chunks = chunker.chunk(clean_text)
   ```

3. **Access Control**
   ```python
   from functools import wraps
   
   def require_auth(func):
       @wraps(func)
       def wrapper(*args, **kwargs):
           if not check_authentication():
               raise PermissionError("Authentication required")
           return func(*args, **kwargs)
       return wrapper
   
   @require_auth
   def chunk_sensitive_document(document):
       return chunker.chunk(document)
   ```

### Privacy Considerations

1. **Data Retention**
   ```python
   class PrivacyAwareChunker:
       def __init__(self):
           self.chunker = EnterpriseChunker()
           self.temp_storage = {}
       
       def chunk_and_forget(self, text, document_id):
           chunks = self.chunker.chunk(text)
           # Don't store original text
           del text
           
           # Store only chunks with limited retention
           self.temp_storage[document_id] = {
               'chunks': chunks,
               'created_at': time.time()
           }
           
           # Auto-cleanup after 1 hour
           self.cleanup_old_data()
           
           return chunks
       
       def cleanup_old_data(self):
           current_time = time.time()
           for doc_id, data in list(self.temp_storage.items()):
               if current_time - data['created_at'] > 3600:
                   del self.temp_storage[doc_id]
   ```

2. **Audit Logging**
   ```python
   import logging
   import hashlib
   
   audit_logger = logging.getLogger('audit')
   
   def audit_chunk_operation(document_id, user_id):
       # Log operation without sensitive content
       document_hash = hashlib.sha256(document_id.encode()).hexdigest()
       audit_logger.info(f"User {user_id} chunked document {document_hash[:8]}")
   ```

### Compliance Guidelines

1. **GDPR Compliance**
   ```python
   class GDPRCompliantChunker:
       def __init__(self):
           self.chunker = EnterpriseChunker()
           self.consent_registry = {}
       
       def chunk_with_consent(self, text, user_id):
           if not self.has_consent(user_id):
               raise PermissionError("User consent required")
           
           return self.chunker.chunk(text)
       
       def right_to_deletion(self, user_id):
           # Remove all user data
           if user_id in self.consent_registry:
               del self.consent_registry[user_id]
           # Trigger deletion in connected systems
   ```

2. **Data Encryption**
   ```python
   from cryptography.fernet import Fernet
   
   class EncryptedChunker:
       def __init__(self, key):
           self.chunker = EnterpriseChunker()
           self.cipher = Fernet(key)
       
       def chunk_and_encrypt(self, text):
           chunks = self.chunker.chunk(text)
           encrypted_chunks = []
           
           for chunk in chunks:
               encrypted = self.cipher.encrypt(chunk.encode())
               encrypted_chunks.append(encrypted)
           
           return encrypted_chunks
       
       def decrypt_chunk(self, encrypted_chunk):
           return self.cipher.decrypt(encrypted_chunk).decode()
   ```

---

## 🚀 14. Further Resources & Next Steps

### Official Resources

- **GitHub Repository**: [github.com/your-org/enterprise-chunker](https://github.com/your-org/enterprise-chunker)
- **Documentation**: [enterprise-chunker.readthedocs.io](https://enterprise-chunker.readthedocs.io)
- **PyPI Package**: [pypi.org/project/enterprise-chunker](https://pypi.org/project/enterprise-chunker)

### Learning Path

#### For Beginners
1. Start with basic chunking examples
2. Experiment with different strategies
3. Try processing different file types
4. Learn about token limits and overlap

#### For Intermediate Users
1. Implement custom chunking strategies
2. Build integration with your AI pipeline
3. Optimize performance for your use case
4. Add monitoring and metrics

#### For Experts
1. Contribute to the project
2. Build extensions and plugins
3. Optimize for specific domains
4. Share best practices with community

### Community Resources

- **Discord Community**: [discord.gg/enterprise-chunker](https://discord.gg/enterprise-chunker)
- **Stack Overflow**: Tag questions with `enterprise-chunker`
- **YouTube Tutorials**: Search for "Enterprise Chunker tutorials"
- **Blog Posts**: Check our blog for updates and tips

### Related Tools and Frameworks

1. **LangChain**: Framework for building LLM applications
2. **LlamaIndex**: Data framework for LLM applications
3. **Pinecone**: Vector database for embeddings
4. **Weaviate**: Open-source vector database

### Next Steps

1. **Build a Simple Project**
   - Create a document Q&A system
   - Build a code analysis tool
   - Make a content summarizer

2. **Optimize for Production**
   - Add error handling
   - Implement monitoring
   - Set up automated testing

3. **Contribute Back**
   - Report bugs
   - Suggest features
   - Submit pull requests
   - Write documentation

---

This comprehensive guide covers Enterprise Chunker from absolute beginner to expert level, providing clear explanations, practical examples, and actionable guidance for users at all skill levels. Whether you're just starting or building production systems, this guide will help you effectively use Enterprise Chunker for your text processing needs.
</file>

<file path="config.py">
"""
Configuration management for the EnterpriseChunker
"""

import os
from typing import Dict, Any, Optional, Union, TypeVar, cast

from enterprise_chunker.models.enums import ChunkingStrategy, TokenEstimationStrategy


T = TypeVar('T')


class ConfigManager:
    """Configuration management with environment variable support"""
    
    # Single source of truth for default configuration values
    DEFAULT_CONFIG = {
        "max_tokens_per_chunk": 4000,
        "overlap_tokens": 200,
        "reserved_tokens": 1000,
        "chunking_strategy": ChunkingStrategy.ADAPTIVE,
        "token_strategy": TokenEstimationStrategy.BALANCED,
        "preserve_structure": True,
        "enable_format_detection": True,
        "format_detection_sample_size": 2500,
        "add_metadata_comments": True,
        "safety_margin": 0.9,
        "max_chunk_size_chars": 0,
        "target_chunk_ratio": 0.8,
        "stream_buffer_size": 100000,
        "cache_size": 1000,
        "respect_sentences": True,
    }
    
    ENV_PREFIX = "CHUNKER_"
    
    @classmethod
    def get_config(cls, overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Get configuration with environment variables and overrides
        
        Args:
            overrides: Optional configuration overrides
            
        Returns:
            Dictionary with complete configuration
        """
        # Deep copy by creating a new dictionary
        config = {k: v for k, v in cls.DEFAULT_CONFIG.items()}
        
        # Convert enum instances to string values for proper serialization and handling
        for key, value in config.items():
            if hasattr(value, 'value') and not isinstance(value, (str, int, float, bool)):
                config[key] = value.value
        
        # Environment variables override defaults
        for key in config:
            env_key = f"{cls.ENV_PREFIX}{key.upper()}"
            if env_key in os.environ:
                try:
                    config[key] = cls._parse_env_value(env_key, config[key])
                except (ValueError, TypeError) as e:
                    # Log error and keep default if parsing fails
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.error(f"Error parsing environment variable {env_key}: {e}. Using default value.")
                
        # Explicit overrides take precedence
        if overrides:
            config.update(overrides)
            
        # Convert string enum values to enum instances
        for enum_key, enum_class in [
            ("chunking_strategy", ChunkingStrategy),
            ("token_strategy", TokenEstimationStrategy)
        ]:
            if isinstance(config[enum_key], str):
                try:
                    config[enum_key] = enum_class(config[enum_key])
                except ValueError:
                    # Fallback to default if invalid enum value
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.error(f"Invalid value for {enum_key}: {config[enum_key]}. Using default.")
                    config[enum_key] = cls.DEFAULT_CONFIG[enum_key]
            
        return config
    
    @staticmethod
    def _parse_env_value(env_key: str, default_value: Any) -> Any:
        """
        Parse environment variable to appropriate type
        
        Args:
            env_key: Environment variable key
            default_value: Default value to determine type
            
        Returns:
            Parsed value with appropriate type
        """
        value = os.environ[env_key]
        
        # Convert to correct type based on default
        if isinstance(default_value, bool):
            return value.lower() in ('true', 'yes', '1', 'on')
        elif isinstance(default_value, int):
            return int(value)
        elif isinstance(default_value, float):
            return float(value)
        elif hasattr(default_value, 'value') and not isinstance(default_value, (str, int, float, bool)):
            # Handling enum string values
            return value
        else:
            return value
    
    @classmethod
    def create_options(cls, overrides: Optional[Dict[str, Any]] = None) -> 'ChunkingOptions':
        """
        Create ChunkingOptions with environment and override configuration
        
        Args:
            overrides: Optional configuration overrides
            
        Returns:
            ChunkingOptions instance
        """
        config = cls.get_config(overrides)
        return ChunkingOptions(**config)


class ChunkingOptions:
    """Configuration options for chunking"""
    
    def __init__(self, **kwargs):
        """
        Initialize chunking options from provided kwargs
        
        Args:
            **kwargs: Configuration parameters
        """
        # Get defaults from ConfigManager to avoid duplication
        defaults = ConfigManager.DEFAULT_CONFIG
        
        # Apply the provided kwargs with the defaults as fallback
        for key, default_value in defaults.items():
            setattr(self, key, kwargs.get(key, default_value))
        
    def get_effective_max_tokens(self) -> int:
        """
        Calculate effective maximum tokens, accounting for reserved tokens and safety margin
        
        Returns:
            Effective maximum tokens that should be used for chunking
        """
        # Apply both reserved_tokens and safety_margin to calculate the actual limit
        max_effective = int(self.max_tokens_per_chunk * self.safety_margin)
        max_with_reserved = max(0, self.max_tokens_per_chunk - self.reserved_tokens)
        
        # Use the more restrictive of the two approaches
        return min(max_effective, max_with_reserved)
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert options to dictionary
        
        Returns:
            Dictionary representation of options
        """
        result = {}
        for key in ConfigManager.DEFAULT_CONFIG:
            value = getattr(self, key)
            # Convert enum instances to their string representation
            if hasattr(value, 'value') and not isinstance(value, (str, int, float, bool)):
                value = value.value
            result[key] = value
        return result
</file>

<file path="enterprise-chunker-guide.md">
## Performance Monitoring

Enterprise Chunker provides comprehensive monitoring capabilities to help you understand performance and optimize your processing.

### Basic Metrics Collection

Get a snapshot of current performance:

```python
# Get performance metrics
metrics = smart_chunker.get_metrics()
print(f"Processed {metrics['total_chunks_processed']} chunks")
print(f"Average throughput: {metrics['avg_throughput']:.2f} chunks/second")
```

**Key metrics explained:**

- **total_chunks_processed**: Total number of chunks generated
- **total_bytes_processed**: Total amount of text processed
- **avg_processing_time**: Average time to process text
- **avg_throughput**: Average chunks processed per second
- **error_count**: Number of errors encountered
- **retry_count**: Number of retry attempts
- **strategy_switches**: Number of times strategy changed
- **decisions**: Counts of strategy decisions

**Beyond the basics - detailed metrics:**

```python
# Get comprehensive metrics
detailed_metrics = chunker.get_metrics()

# System information
system_health = detailed_metrics['system']['health_status']
circuit_breaker = detailed_metrics['system']['circuit_breaker']
memory_percent = detailed_metrics['system']['memory_percent']
system_load = detailed_metrics['system']['system_load']

# Performance metrics
avg_batch_size = detailed_metrics['avg_batch_size']
p90_processing_time = detailed_metrics.get('p90_processing_time')
strategy_decisions = detailed_metrics['decisions']

print(f"System Health: {system_health}")
print(f"Memory Usage: {memory_percent:.1f}%")
print(f"System Load: {system_load:.2f}")
print(f"Circuit Breaker: {circuit_breaker}")
print(f"Strategy Usage: {strategy_decisions}")
```

**Real-world example - performance dashboard:**

```python
def print_performance_dashboard(chunker):
    """Print a comprehensive performance dashboard"""
    metrics = chunker.get_metrics()
    
    # Format as a dashboard
    print("\n" + "="*50)
    print(" ENTERPRISE CHUNKER PERFORMANCE DASHBOARD ")
    print("="*50)
    
    # Processing statistics
    print("\n📊 PROCESSING STATISTICS")
    print(f"Total chunks processed: {metrics['total_chunks_processed']:,}")
    print(f"Total bytes processed: {metrics['total_bytes_processed']:,} bytes")
    print(f"Uptime: {metrics['uptime_seconds']/60:.1f} minutes")
    
    # Performance metrics
    print("\n⚡ PERFORMANCE METRICS")
    print(f"Average throughput: {metrics['avg_throughput']:.2f} chunks/second")
    print(f"Average processing time: {metrics['avg_processing_time']:.3f} seconds")
    
    if 'min_processing_time' in metrics:
        print(f"Min processing time: {metrics['min_processing_time']:.3f} seconds")
        print(f"Max processing time: {metrics['max_processing_time']:.3f} seconds")
        print(f"90th percentile time: {metrics.get('p90_processing_time', 'N/A')}")
    
    # Memory metrics
    print("\n💾 MEMORY METRICS")
    if 'avg_memory_usage' in metrics:
        print(f"Average memory usage: {metrics['avg_memory_usage']:.1f}%")
        print(f"Maximum memory usage: {metrics['max_memory_usage']:.1f}%")
    
    # System status
    print("\n🖥️ SYSTEM STATUS")
    print(f"Health status: {metrics['system']['health_status']}")
    print(f"Circuit breaker: {metrics['system']['circuit_breaker']}")
    print(f"System load: {metrics['system']['system_load']:.2f}")
    print(f"Memory percent: {metrics['system']['memory_percent']:.1f}%")
    print(f"CPU cores: {metrics['system']['cpu_count']}")
    
    # Strategy decisions
    print("\n🧠 STRATEGY DECISIONS")
    for strategy, count in metrics['decisions'].items():
        print(f"  {strategy}: {count} times")
    print(f"Strategy switches: {metrics['strategy_switches']}")
    
    # Errors and retries
    print("\n⚠️ ERRORS AND RETRIES")
    print(f"Error count: {metrics['error_count']}")
    print(f"Retry count: {metrics['retry_count']}")
    
    # Batch sizing
    print("\n📦 BATCH SIZING")
    print(f"Average batch size: {metrics['avg_batch_size']:.1f}")
    
    # Throughput trend
    if 'throughput_trend' in metrics:
        trend = metrics['throughput_trend']
        if trend:
            print("\n📈 THROUGHPUT TREND")
            current = trend[-1]
            average = sum(trend) / len(trend)
            if len(trend) > 1:
                previous = trend[-2]
                change = ((current - previous) / previous) * 100 if previous > 0 else 0
                print(f"  Current: {current:.2f} chunks/s ({change:+.1f}% change)")
            else:
                print(f"  Current: {current:.2f} chunks/s")
            print(f"  Average: {average:.2f} chunks/s")
    
    print("="*50)
```

### Prometheus Metrics Integration

For production systems, you can expose metrics via Prometheus:

```python
from enterprise_chunker import start_monitoring_server

# Start metrics server on port 8000
start_monitoring_server(port=8000)

# Create chunker with metrics enabled
smart_chunker = create_auto_chunker(
    options,
    enable_metrics_server=True,
    metrics_port=8000
)
```

**How to access metrics:**
1. Configure Prometheus to scrape `http://your-server:8000/metrics`
2. Metrics are exposed in standard Prometheus format
3. Create Grafana dashboards for visualization

**Available Prometheus metrics:**
- `chunker_throughput`: Chunks processed per second
- `chunker_memory`: Memory usage percentage
- `chunker_workers`: Worker utilization percentage
- `chunker_processing_time`: Time to process text
- `chunker_errors`: Number of processing errors
- `chunker_retries`: Number of retry attempts
- `chunker_batch_size`: Current batch size

**Example Prometheus configuration:**

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'enterprise_chunker'
    scrape_interval: 15s
    static_configs:
      - targets: ['localhost:8000']
```

**Real-world example - health check endpoint:**

```python
# For a Flask application
from flask import Flask, jsonify
app = Flask(__name__)

# Global chunker instance
chunker = create_auto_chunker(options, enable_metrics_server=True)

@app.route('/health')
def health():
    """API health endpoint with chunker metrics"""
    metrics = chunker.get_metrics()
    
    # Simplify for API response
    health_data = {
        'status': 'healthy',
        'chunker': {
            'health': metrics['system']['health_status'],
            'circuit_breaker': metrics['system']['circuit_breaker'],
            'memory_percent': metrics['system']['memory_percent'],
            'processed_chunks': metrics['total_chunks_processed'],
            'throughput': metrics['avg_throughput'],
            'errors': metrics['error_count']
        }
    }
    
    # Determine overall status
    if metrics['system']['health_status'] == 'CRITICAL':
        health_data['status'] = 'critical'
    elif metrics['system']['health_status'] == 'WARNING':
        health_data['status'] = 'warning'
    elif metrics['error_count'] > 10:
        health_data['status'] = 'degraded'
        
    status_code = 200 if health_data['status'] == 'healthy' else 500
    return jsonify(health_data), status_code
```

### Custom Metrics Tracking

For specialized monitoring needs, you can build custom tracking:

```python
class ChunkerPerformanceTracker:
    """Custom performance tracker for Enterprise Chunker"""
    
    def __init__(self, chunker):
        self.chunker = chunker
        self.metrics_history = []
        self.start_time = time.time()
        
        # Start background collection
        self.running = True
        self.collection_thread = threading.Thread(target=self._collect_metrics)
        self.collection_thread.daemon = True
        self.collection_thread.start()
    
    def _collect_metrics(self):
        """Collect metrics at regular intervals"""
        while self.running:
            try:
                # Get current metrics
                current = self.chunker.get_metrics()
                
                # Add timestamp
                current['timestamp'] = time.time()
                
                # Store in history
                self.metrics_history.append(current)
                
                # Limit history size
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-1000:]
                    
                # Sleep for interval
                time.sleep(5)
            except Exception as e:
                print(f"Error collecting metrics: {e}")
                time.sleep(10)
    
    def stop(self):
        """Stop metrics collection"""
        self.running = False
        if self.collection_thread.is_alive():
            self.collection_thread.join(timeout=1.0)
    
    def get_throughput_history(self):
        """Get throughput history over time"""
        return [
            (m['timestamp'] - self.start_time, m.get('avg_throughput', 0))
            for m in self.metrics_history if 'avg_throughput' in m
        ]
    
    def get_memory_history(self):
        """Get memory usage history over time"""
        return [
            (m['timestamp'] - self.start_time, 
             m['system']['memory_percent'] if 'system' in m else 0)
            for m in self.metrics_history
        ]
    
    def get_errors_over_time(self):
        """Get error count over time"""
        return [
            (m['timestamp'] - self.start_time, m.get('error_count', 0))
            for m in self.metrics_history if 'error_count' in m
        ]
    
    def get_strategy_distribution(self):
        """Get distribution of strategy decisions"""
        if not self.metrics_history:
            return {}
            
        latest = self.metrics_history[-1]
        if 'decisions' not in latest:
            return {}
            
        decisions = latest['decisions']
        total = sum(decisions.values())
        
        if total == 0:
            return {}
            
        return {k: v / total for k, v in decisions.items()}
    
    def plot_metrics(self):
        """Plot key metrics if matplotlib is available"""
        try:
            import matplotlib.pyplot as plt
            
            # Create figure with subplots
            fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))
            
            # Plot throughput
            throughput_data = self.get_throughput_history()
            if throughput_data:
                times, values = zip(*throughput_data)
                ax1.plot(times, values, 'b-')
                ax1.set_title('Throughput (chunks/second)')
                ax1.set_ylabel('Chunks/second')
                ax1.grid(True, alpha=0.3)
            
            # Plot memory usage
            memory_data = self.get_memory_history()
            if memory_data:
                times, values = zip(*memory_data)
                ax2.plot(times, values, 'r-')
                ax2.set_title('Memory Usage (%)')
                ax2.set_ylabel('Memory %')
                ax2.grid(True, alpha=0.3)
            
            # Plot errors
            error_data = self.get_errors_over_time()
            if error_data:
                times, values = zip(*error_data)
                ax3.plot(times, values, 'y-')
                ax3.set_title('Error Count')
                ax3.set_ylabel('Errors')
                ax3.set_xlabel('Time (seconds)')
                ax3.grid(True, alpha=0.3)
            
            plt.tight_layout()
            return plt
        except ImportError:
            print("Matplotlib not available for plotting")
            return None
```

**Using the custom tracker:**

```python
# Initialize tracker
tracker = ChunkerPerformanceTracker(chunker)

# Process documents
for doc in documents:
    chunks = chunker.chunk(doc, chunker_func)
    # ... process chunks ...

# Get performance insights
print("Strategy distribution:")
distribution = tracker.get_strategy_distribution()
for strategy, percentage in distribution.items():
    print(f"  {strategy}: {percentage:.1%}")

# Visualize (if matplotlib available)
plt = tracker.plot_metrics()
if plt:
    plt.savefig('chunker_performance.png')
    plt.show()

# Stop tracker when done
tracker.stop()
```

## Extending Enterprise Chunker

Enterprise Chunker is designed to be extensible. This section explains how to customize and extend its functionality for specific use cases.

### Custom Chunking Strategy

Create your own chunking strategy for specialized content types:

```python
from enterprise_chunker.strategies.base import BaseChunkingStrategy
from enterprise_chunker.models.enums import ContentFormat, ChunkingStrategy

class MyCustomStrategy(BaseChunkingStrategy):
    def __init__(self):
        super().__init__(ContentFormat.TEXT)
        
    def detect_boundaries(self, text, options):
        # Custom boundary detection
        boundaries = []
        
        # Example: chunk on specific pattern
        import re
        for match in re.finditer(r'SECTION \d+:', text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'is_section': True,
                'section_name': match.group(0)
            })
        
        # Always add a boundary at the beginning if none exists
        if not boundaries or boundaries[0]['index'] > 0:
            boundaries.insert(0, {
                'index': 0,
                'end': 0,
                'text': '',
                'is_section': False
            })
            
        return boundaries
        
    def _get_chunking_strategy(self):
        return ChunkingStrategy.SEMANTIC
```

**Using your custom strategy:**

```python
# Create chunker with factory
from enterprise_chunker.chunker import ChunkingStrategyFactory

# Register your strategy
ChunkingStrategyFactory.register_strategy('custom', MyCustomStrategy)

# Use your strategy
chunker = EnterpriseChunker()
chunks = chunker.adaptive_chunk_text(text, strategy='custom')
```

**Real-world example - medical document chunker:**

```python
class MedicalDocumentStrategy(BaseChunkingStrategy):
    """Strategy specialized for medical documents"""
    
    def __init__(self):
        super().__init__(ContentFormat.TEXT)
        
    def detect_boundaries(self, text, options):
        boundaries = []
        
        # Detect section headers common in medical documents
        section_patterns = [
            r'(?:^|\n)(?:HISTORY OF PRESENT ILLNESS|HPI):',
            r'(?:^|\n)(?:PAST MEDICAL HISTORY|PMH):',
            r'(?:^|\n)(?:MEDICATIONS|MEDS):',
            r'(?:^|\n)(?:PHYSICAL EXAMINATION|PE):',
            r'(?:^|\n)(?:LABORATORY DATA|LABS):',
            r'(?:^|\n)(?:ASSESSMENT(?:\s+AND\s+PLAN)?|A(?:/|&)P):',
            r'(?:^|\n)(?:IMPRESSION):',
            r'(?:^|\n)(?:PLAN):',
            r'(?:^|\n)(?:DIAGNOSIS|DX):',
        ]
        
        # Combine patterns
        combined_pattern = '|'.join(section_patterns)
        
        # Find all section headers
        import re
        for match in re.finditer(combined_pattern, text, re.IGNORECASE):
            section_name = match.group(0).strip()
            
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': section_name,
                'is_section': True,
                'section_name': section_name
            })
        
        # Add a boundary at the beginning
        if not boundaries or boundaries[0]['index'] > 0:
            boundaries.insert(0, {
                'index': 0,
                'end': 0,
                'text': '',
                'is_section': False,
                'section_name': 'Document Start'
            })
            
        return boundaries
    
    def _create_context_tracker(self):
        """Create custom context tracker for medical documents"""
        return {
            'sections': [],
            'current_section': None,
            'patient_id': None,  # Will extract if found
            'doc_type': None     # Will detect document type
        }
    
    def _update_context_tracker(self, context_tracker, boundary):
        """Update context with medical document information"""
        if not context_tracker:
            return
            
        # Update section tracking
        if boundary.get('is_section', False):
            section_name = boundary.get('section_name', '')
            context_tracker['current_section'] = section_name
            
            if section_name not in context_tracker['sections']:
                context_tracker['sections'].append(section_name)
        
        # Try to extract patient ID if not found yet
        if context_tracker['patient_id'] is None:
            text = boundary.get('text', '')
            # Look for MRN or patient ID pattern
            import re
            id_match = re.search(r'(?:MRN|Patient ID):?\s*(\d+)', text)
            if id_match:
                context_tracker['patient_id'] = id_match.group(1)
    
    def _get_preserved_context(self, context_tracker):
        """Get medical document context"""
        if not context_tracker:
            return ""
            
        context_lines = []
        
        # Add document type if known
        if context_tracker['doc_type']:
            context_lines.append(f"Document Type: {context_tracker['doc_type']}")
        
        # Add patient ID if found (masked for privacy)
        if context_tracker['patient_id']:
            masked_id = 'XXX-XX-' + context_tracker['patient_id'][-4:]
            context_lines.append(f"Patient ID: {masked_id}")
        
        # Add sections seen so far
        if context_tracker['sections']:
            context_lines.append("Document Sections:")
            for section in context_tracker['sections']:
                context_lines.append(f"- {section}")
        
        return "\n".join(context_lines)
    
    def _get_chunking_strategy(self):
        return ChunkingStrategy.SEMANTIC
```

### Custom Token Estimator

For specialized token counting needs:

```python
from enterprise_chunker.utils.token_estimation import BaseTokenEstimator

class MyTokenEstimator(BaseTokenEstimator):
    def _calculate_estimate(self, features, text):
        # Custom token calculation
        
        # Simple example - count words and adjust for content type
        word_count = features.word_count
        
        # Apply custom adjustments
        if features.has_code_blocks:
            # Code typically has more tokens per word
            adjusted_count = word_count * 1.3
        elif features.has_cjk:
            # CJK characters often map 1:1 with tokens
            adjusted_count = word_count * 0.8 + len(re.findall(r'[\u4e00-\u9fff]', text))
        else:
            # Default adjustment
            adjusted_count = word_count * 1.2
            
        return math.ceil(adjusted_count)
```

**Registering your estimator:**

```python
from enterprise_chunker.utils.token_estimation import TokenEstimatorFactory

# Register your custom estimator
TokenEstimatorFactory.register_estimator('my_custom', MyTokenEstimator)

# Use in options
options = ChunkingOptions(token_strategy='my_custom')
chunker = EnterpriseChunker(options)
```

### Custom Format Detection

Extend format detection for specialized file formats:

```python
from enterprise_chunker.utils.format_detection import FormatDetector

class ExtendedFormatDetector(FormatDetector):
    def detect_format(self, text):
        # First try built-in detection
        format_type, confidence = super().detect_format(text)
        
        # If confidence is low, try our custom formats
        if confidence < 0.5:
            # Check for FHIR (healthcare standard)
            if '"resourceType": "Patient"' in text or '"resourceType": "Observation"' in text:
                return ContentFormat.JSON, 0.9
                
            # Check for HL7 format (healthcare)
            if text.startswith('MSH|^~\\&|'):
                return ContentFormat.TEXT, 0.9
                
            # Check for custom format
            if text.startswith('%%CUSTOM_FORMAT%%'):
                return ContentFormat.TEXT, 0.9
                
        return format_type, confidence
```

**Using custom format detector:**

```python
# Create custom chunker with extended format detection
from enterprise_chunker.chunker import EnterpriseChunker

class CustomChunker(EnterpriseChunker):
    def __init__(self, options=None):
        super().__init__(options)
        # Replace format detector with our extended version
        self._format_detector = ExtendedFormatDetector()
        
    def adaptive_chunk_text(self, text, **kwargs):
        # Use custom format detection
        if self.options.enable_format_detection:
            format_type, confidence = self._format_detector.detect_format(text)
            # Rest of the method remains the same...
```

## Environment Variables

Enterprise Chunker supports configuration via environment variables, which is particularly useful in containerized environments and cloud deployments.

```bash
# Set maximum tokens per chunk
export CHUNKER_MAX_TOKENS_PER_CHUNK=2000

# Set chunking strategy
export CHUNKER_CHUNKING_STRATEGY=semantic

# Set token estimation strategy
export CHUNKER_TOKEN_STRATEGY=precision

# Enable/disable format detection
export CHUNKER_ENABLE_FORMAT_DETECTION=true

# Set overlap tokens
export CHUNKER_OVERLAP_TOKENS=100
```

**Complete list of environment variables:**

| Environment Variable | Default | Description |
|----------------------|---------|-------------|
| `CHUNKER_MAX_TOKENS_PER_CHUNK` | 4000 | Maximum tokens per chunk |
| `CHUNKER_OVERLAP_TOKENS` | 200 | Number of tokens to overlap |
| `CHUNKER_RESERVED_TOKENS` | 1000 | Buffer tokens for safety |
| `CHUNKER_CHUNKING_STRATEGY` | adaptive | Strategy for chunking |
| `CHUNKER_TOKEN_STRATEGY` | balanced | Strategy for token estimation |
| `CHUNKER_PRESERVE_STRUCTURE` | true | Preserve document structure |
| `CHUNKER_ENABLE_FORMAT_DETECTION` | true | Auto-detect content format |
| `CHUNKER_FORMAT_DETECTION_SAMPLE_SIZE` | 2500 | Sample size for detection |
| `CHUNKER_ADD_METADATA_COMMENTS` | true | Add metadata comments |
| `CHUNKER_SAFETY_MARGIN` | 0.9 | Safety margin for token limits |
| `CHUNKER_MAX_CHUNK_SIZE_CHARS` | 0 | Max chunk size in chars |
| `CHUNKER_TARGET_CHUNK_RATIO` | 0.8 | Target ratio for chunk sizes |
| `CHUNKER_STREAM_BUFFER_SIZE` | 100000 | Buffer size for streaming |
| `CHUNKER_CACHE_SIZE` | 1000 | Cache size for token estimation |
| `CHUNKER_RESPECT_SENTENCES` | true | Break at sentence boundaries |

**Environment-aware configuration:**

```python
import os

def get_env_config():
    """Get configuration from environment variables"""
    config = {}
    
    # Check for environment variables
    if 'CHUNKER_MAX_TOKENS_PER_CHUNK' in os.environ:
        config['max_tokens_per_chunk'] = int(os.environ.get('CHUNKER_MAX_TOKENS_PER_CHUNK'))
        
    if 'CHUNKER_OVERLAP_TOKENS' in os.environ:
        config['overlap_tokens'] = int(os.environ.get('CHUNKER_OVERLAP_TOKENS'))
        
    if 'CHUNKER_CHUNKING_STRATEGY' in os.environ:
        config['chunking_strategy'] = os.environ.get('CHUNKER_CHUNKING_STRATEGY')
        
    # Additional env vars...
    
    return config

# Create chunker with environment configuration
chunker = EnterpriseChunker(get_env_config())
```

**Docker example:**

```Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

# Set chunker configuration
ENV CHUNKER_MAX_TOKENS_PER_CHUNK=3000
ENV CHUNKER_OVERLAP_TOKENS=150
ENV CHUNKER_CHUNKING_STRATEGY=semantic
ENV CHUNKER_TOKEN_STRATEGY=precision

CMD ["python", "process_documents.py"]
```

**Kubernetes ConfigMap example:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: chunker-config
data:
  CHUNKER_MAX_TOKENS_PER_CHUNK: "3000"
  CHUNKER_OVERLAP_TOKENS: "150"
  CHUNKER_CHUNKING_STRATEGY: "semantic"
  CHUNKER_TOKEN_STRATEGY: "precision"
  CHUNKER_MEMORY_SAFETY: "true"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: document-processor
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: processor# Enterprise Chunker

Enterprise Chunker is an advanced text chunking utility that intelligently breaks down large documents for processing by Large Language Models (LLMs). Unlike basic chunking methods, Enterprise Chunker preserves document structure, maintains semantic coherence between chunks, and automatically adapts to different content types.

- [Summary and Best Practices](#summary-and-best-practices)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Core Concepts](#core-concepts)
- [Basic Usage](#basic-usage)
- [Advanced Usage](#advanced-usage)
- [Configuration Options](#configuration-options)
- [Chunking Strategies](#chunking-strategies)
- [Format-Specific Chunking](#format-specific-chunking)
- [Memory Optimization](#memory-optimization)
- [Parallel Processing](#parallel-processing)
- [Performance Monitoring](#performance-monitoring)
- [Extending Enterprise Chunker](#extending-enterprise-chunker)
- [Environment Variables](#environment-variables)
- [API Reference](#api-reference)
- [Troubleshooting](#troubleshooting)

## Summary and Best Practices

Enterprise Chunker is designed to solve one of the most critical problems in LLM processing: how to break down large documents intelligently so they can be processed effectively while preserving meaning and context.

### Key Dependencies

To get the most out of Enterprise Chunker, consider installing these dependencies:

| Dependency | Purpose | Installation |
|------------|---------|-------------|
| **prometheus_client** | Metrics and monitoring | `pip install prometheus_client>=0.14.0` |
| **psutil** | System resource monitoring | `pip install psutil>=5.9.0` |
| **numpy** | Enhanced analytics and calculations | `pip install numpy>=1.22.0` |
| **requests** | Remote configuration updates | `pip install requests>=2.27.0` |

### Best Practices

✅ **Do This** | ❌ **Avoid This**
-------------- | ---------------
Set reasonable token limits based on your LLM | Using the default 4000 tokens without checking your model's actual context window
Use `ADAPTIVE` strategy for most content | Setting a specific strategy when content is mixed
Enable memory optimization for files over 10MB | Loading large files entirely into memory
Use parallel processing for batch operations | Sequential processing for large document sets
Match chunking strategy to content type | Using the same approach for all content types
Test with samples of your actual data | Assuming default settings will work optimally
Configure overlap based on content coherence needs | Setting zero overlap or excessive overlap
Monitor performance metrics in production | Ignoring resource usage in high-volume scenarios
Use streaming for very large files | Loading entire documents when not necessary
Scale worker count based on available CPU cores | Oversubscribing CPU resources

### Red Flags When Deploying

🚩 **High memory usage spikes**: Adjust batch size, enable memory safety, or use streaming

🚩 **Slow processing of large files**: Switch to `PERFORMANCE` token strategy and consider parallel processing

🚩 **Uneven chunk sizes**: Review strategy selection, adjust safety margin, or use fixed-size chunking

🚩 **Missing content in processed output**: Ensure overlap settings are appropriate for your content

🚩 **System crashes during processing**: Implement circuit breaker pattern and memory monitoring

🚩 **Poor chunk boundaries in specialized content**: Use format-specific strategies instead of generic ones

### Key Metrics to Monitor

- Chunks processed per second
- Memory usage during processing
- Processing time per document
- Error and retry rates
- Token estimation accuracy

## Installation

```bash
# From PyPI
pip install enterprise-chunker

# From source
git clone https://github.com/your-org/enterprise-chunker.git
cd enterprise-chunker
pip install -e .

# With all optional dependencies
pip install "enterprise-chunker[all]"
```

Optional dependency groups:
- `monitoring`: Prometheus metrics (`pip install "enterprise-chunker[monitoring]"`)
- `advanced`: System monitoring and analytics (`pip install "enterprise-chunker[advanced]"`)
- `remote_config`: Remote configuration capabilities (`pip install "enterprise-chunker[remote_config]"`)
- `all`: All optional dependencies (`pip install "enterprise-chunker[all]"`)

**Which Dependencies Should You Choose?**

* **Small projects or testing**: No additional dependencies needed
* **Production systems**: At minimum, install `advanced` for resource monitoring
* **High-volume applications**: Install `all` dependencies for full capabilities
* **Microservices**: Consider `monitoring` for observability in distributed systems

**Tip**: If you're deploying in a resource-constrained environment like a serverless function, you might want to avoid the heavier dependencies like numpy to reduce cold start time.

## Quick Start

Let's get you up and running with Enterprise Chunker in just a few minutes:

```python
from enterprise_chunker import EnterpriseChunker

# Initialize chunker with default settings
chunker = EnterpriseChunker()

# Simple chunking with automatic format detection
with open('document.md', 'r') as f:
    text = f.read()
    
chunks = chunker.chunk(text)

# Process chunks with your LLM
for chunk in chunks:
    response = process_with_llm(chunk)
```

**What's happening here?**

1. We create an instance of the `EnterpriseChunker` with default settings
2. We read in a document (in this case a Markdown file)
3. The chunker automatically:
   - Detects that it's Markdown content
   - Chooses appropriate chunking boundaries (headings, paragraphs, etc.)
   - Splits the content into properly sized chunks
   - Adds context between chunks to maintain coherence
4. We process each chunk with our LLM

**Tips for getting started:**

- Start with default settings and adjust as needed
- For your first tests, try a medium-sized document (1-5 pages)
- Check your chunk sizes by printing `len(chunk)` or token count
- Examine chunk boundaries to see if they make semantic sense
- If chunks look odd, try a specific strategy (see Chunking Strategies section)

**Quick troubleshooting:**

- If chunks are too large, reduce `max_tokens_per_chunk`
- If context is lost between chunks, increase `overlap_tokens`
- If processing is slow, consider using parallel chunking
- If memory usage is high, use streaming with `chunk_stream()`

This basic approach works for most simple use cases. For more advanced scenarios, check out the following sections.


## Core Concepts

Let's break down the fundamental concepts behind Enterprise Chunker in plain language:

### Chunks and Chunking

Think of a large document like a book. When you want an AI to understand the whole book, you can't just feed it all at once - most LLMs have limits on how much they can "see" at one time (typically 4K-32K tokens). Chunking is like tearing the book into manageable chapters that the AI can digest one at a time.

**What makes Enterprise Chunker different from simple chunking?**

Regular chunking just counts characters or tokens and cuts the text every X tokens - like ripping pages out blindly. This can break paragraphs mid-sentence or split important concepts. Enterprise Chunker is more like a skilled editor who:

- Looks for natural breaks (headings, paragraphs, code blocks)
- Keeps related content together (like keeping JSON object properties together)
- Creates smooth transitions between chunks
- Adds "previously on..." context to remind the AI what it saw in earlier chunks

The chunks produced are:
- Within token limits for your LLM (configurable based on model)
- Structure-preserving (not cutting in the middle of a code block or table)
- Semantically coherent (keeping related concepts together)
- Contextually connected (with appropriate overlap between chunks)

**Tip**: Think about your content type when chunking. A legal document needs different chunking than source code or a CSV file.

### Token Estimation

LLMs don't process text character by character - they break it down into "tokens," which might be words, parts of words, or even single characters for some languages. Knowing how many tokens a chunk contains is crucial.

Enterprise Chunker provides three ways to count tokens, each with different trade-offs:

- **BALANCED** (default): Like a utility player in sports - does a good job at both speed and accuracy for most content types
   - *When to use*: For most general text processing where you need reasonable accuracy but also care about performance
   - *Tip*: This works well for mixed content

- **PRECISION**: Like counting with a microscope - very accurate but takes longer
   - *When to use*: When you're close to token limits and need to be absolutely sure
   - *Tip*: Great for billing scenarios where token counts affect costs

- **PERFORMANCE**: Like quick estimation - rough count but very fast
   - *When to use*: For processing huge volumes of text when approximate counts are acceptable
   - *Tip*: Can be 5-10x faster than PRECISION mode

**Real-world insight**: Token estimation is surprisingly complex - emoji, code, and non-English languages all tokenize differently. Unicode characters like emojis can take multiple tokens each!

### Content Formats

Different document types need different chunking approaches. Enterprise Chunker automatically detects these formats:

- **JSON**: Treats objects and arrays intelligently, keeps related properties together
  - *Example benefit*: A JSON configuration stays valid and usable in each chunk
  - *Tip*: Great for API responses or configuration files

- **Markdown**: Respects heading levels, lists, and code blocks
  - *Example benefit*: A level 2 heading stays with its content, making chunks more meaningful
  - *Tip*: Works wonderfully for documentation or articles

- **XML/HTML**: Understands tags, attributes, and nested structures
  - *Example benefit*: HTML tables aren't broken mid-row
  - *Tip*: Perfect for web content or SOAP/XML messages

- **Code**: Identifies methods, classes, and code blocks
  - *Example benefit*: Function definitions stay intact
  - *Tip*: Excellent for source code or script files

- **Plain text**: Finds natural paragraph and sentence boundaries
  - *Example benefit*: Paragraphs and sentences stay coherent
  - *Tip*: Works well for essays, books, or unstructured text

- **CSV**: Respects data rows and columns
  - *Example benefit*: Data rows aren't split mid-row
  - *Tip*: Ideal for tabular data

- **Logs**: Identifies log entries, timestamps, and severity levels
  - *Example benefit*: Related log entries stay together
  - *Tip*: Perfect for system logs or application logs

**Pro tip**: While automatic format detection works well, if you know your document type in advance, explicitly setting the strategy can improve results.

## Basic Usage

Let's explore the everyday usage patterns of Enterprise Chunker with practical examples and explanations:

### Simple Chunking

Here's the most basic way to use Enterprise Chunker:

```python
from enterprise_chunker import EnterpriseChunker

# Initialize with defaults
chunker = EnterpriseChunker()

# Chunk text
chunks = chunker.chunk(text)
```

**What's happening behind the scenes:**
1. The chunker analyzes your text to detect its format
2. It selects the appropriate chunking strategy automatically
3. It applies default settings (4000 max tokens, 200 overlap tokens)
4. It returns a list of text chunks ready for processing

**When to use this approach:**
- When you're just getting started with the library
- For quick experiments or prototyping
- When working with typical document sizes (under 50 pages)
- When default settings are appropriate for your model

**Tip**: Print the number of chunks with `len(chunks)` and examine a few chunks to verify they look reasonable before processing.

### Configuring Chunk Size

For more control over chunk size and overlap:

```python
# Specify maximum tokens per chunk and overlap
chunks = chunker.adaptive_chunk_text(
    text,
    max_tokens_per_chunk=1000,  # Smaller chunks
    overlap_tokens=50           # Less overlap
)

# Alternative: Using fluent API (chain-style)
chunks = chunker.with_max_tokens(1000).with_overlap(50).chunk(text)
```

**Why adjust these parameters?**

- **max_tokens_per_chunk**: Controls how much text is in each chunk
  - *Smaller values* (500-1000): Create more, smaller chunks
    - Better for: Shorter context window models, more precise answers
    - Trade-off: More API calls, higher processing cost
  - *Larger values* (4000+): Create fewer, larger chunks
    - Better for: Models with large context windows, maintaining context
    - Trade-off: May hit token limits, potentially wasteful

- **overlap_tokens**: Controls how much context is shared between chunks
  - *Smaller values* (20-50): Less repetition, more efficient
    - Better for: Well-structured content, lower cost
    - Trade-off: Might miss cross-chunk connections
  - *Larger values* (200-500): More context preservation
    - Better for: Complex documents, ensuring coherence
    - Trade-off: Duplicate information, higher token usage

**Real-world example**: For a technical manual, you might use larger chunks (3000 tokens) with modest overlap (100 tokens) to keep related sections together. For a legal contract, you might use smaller chunks (1000 tokens) with higher overlap (300 tokens) to ensure critical clauses aren't misunderstood due to missing context.

### Chunking Streams and Files

For larger files that might not fit in memory:

```python
# Process a file stream
with open('large_file.txt', 'r') as f:
    for chunk in chunker.chunk_stream(f):
        process_chunk(chunk)
```

**How streaming differs from regular chunking:**
1. Reads and processes the file incrementally instead of loading it all at once
2. Uses significantly less memory for large files
3. Begins generating chunks immediately, without waiting for full file read
4. Returns a generator instead of a list, so chunks are processed one at a time

**When to use streaming:**
- For files larger than 10MB
- In memory-constrained environments
- When processing time is a concern
- For real-time or interactive applications

**Pro tip**: Combine with a generator for your LLM processing to create a processing pipeline:

```python
def process_document(file_path):
    with open(file_path, 'r') as f:
        for chunk in chunker.chunk_stream(f):
            yield process_with_llm(chunk)
            
# Then use in an async web application:
for result in process_document('huge_file.txt'):
    await send_to_client(result)
```

### Context Managers for Strategy Selection

When you need different chunking strategies for different content:

```python
# Use semantic chunking for a specific operation
with chunker.semantic_context(max_tokens=2000, overlap=100):
    narrative_chunks = chunker.chunk(story_text)
    
# Use structural chunking for code files
with chunker.structural_context():
    code_chunks = chunker.chunk(code_text)
```

**What context managers do:**
- Temporarily change the chunking behavior without creating a new chunker
- Automatically restore previous settings after the context block
- Allow you to use different strategies for different content types

**When to use different contexts:**
- **semantic_context**: For narrative text, articles, books
  - *Tip*: Great for preserving paragraph and section meaning
- **structural_context**: For code, JSON, XML, HTML
  - *Tip*: Keeps structural elements intact (brackets, tags, etc.)
- **fixed_size_context**: For content where structure is less important
  - *Tip*: Most efficient when raw speed matters more than boundaries
- **sentence_context**: For legal or very precise text
  - *Tip*: Ensures no sentence is ever broken mid-way

**Practical example**: Processing a Jupyter notebook with both code and markdown:

```python
# Process notebook cells with appropriate strategy
for cell in notebook['cells']:
    if cell['cell_type'] == 'markdown':
        with chunker.semantic_context():
            markdown_chunks = chunker.chunk(cell['source'])
            process_markdown(markdown_chunks)
    elif cell['cell_type'] == 'code':
        with chunker.structural_context():
            code_chunks = chunker.chunk(cell['source'])
            analyze_code(code_chunks)
```

## Advanced Usage

Ready to take your chunking to the next level? These advanced features help you handle large-scale processing, optimize performance, and monitor your system.

### Smart Parallel Chunking

When you need to process large volumes of text efficiently:

```python
from enterprise_chunker import create_auto_chunker, ChunkingOptions

# Create options
options = ChunkingOptions(
    max_tokens_per_chunk=4000,
    overlap_tokens=200,
    preserve_structure=True
)

# Create auto-tuned chunker
smart_chunker = create_auto_chunker(
    options=options,
    mode="balanced",  # Options: "performance", "balanced", "memory-safe", "auto"
    memory_safety=True
)

# Process text
chunks = smart_chunker.chunk(text, lambda x: x.split('\n'))
```

**What makes this "smart" parallel chunking?**

1. **Automatic resource detection**: Identifies available CPU cores and memory
2. **Dynamic strategy selection**: Switches between simple and advanced chunking based on content complexity
3. **Batch processing**: Groups content into optimal batches for parallel processing
4. **Fault tolerance**: Includes circuit breaker pattern to prevent system overload
5. **Self-tuning**: Adjusts batch size and worker count based on performance metrics

**Mode selection explained:**
- **"performance"**: Maximizes speed at the cost of higher memory usage
  - *When to use*: Batch processing when resources are plentiful
  - *Tip*: Great for overnight jobs or dedicated processing servers
- **"balanced"**: Good trade-off between speed and resource usage (recommended)
  - *When to use*: Most production scenarios
  - *Tip*: The best default for most use cases
- **"memory-safe"**: Prioritizes memory efficiency over speed
  - *When to use*: Memory-constrained environments or extremely large documents
  - *Tip*: Use for cloud functions or containers with memory limits
- **"auto"**: Detects system capabilities and chooses optimal settings
  - *When to use*: When you're unsure of the runtime environment
  - *Tip*: Good for applications deployed in various environments

**Real-world insight**: On an 8-core system, parallel chunking can be 4-6x faster than sequential processing for large documents.

**Warning sign**: If you see memory usage spiking, switch to "memory-safe" mode or enable streaming.

### Streaming with Parallel Processing

For extremely large files that need efficient processing:

```python
# Create streaming processor
from enterprise_chunker.utils.optimized_streaming import ChunkProcessor

processor = ChunkProcessor(options)

# Process large file with parallel optimization
for chunk in processor.process_large_file('huge_document.txt', chunker_func):
    process_with_llm(chunk)
```

**How this works under the hood:**
1. The file is read in optimized buffers instead of all at once
2. Each buffer is processed in parallel as it's read
3. Memory mapping is used when available for more efficient file access
4. Natural boundaries (paragraphs, code blocks) are detected for clean splits
5. Results are yielded in sequence even though processing happens in parallel

**When to use this approach:**
- For files larger than 100MB
- When processing needs to start before the entire file is read
- In production systems processing many documents
- When memory constraints are a concern

**Practical example**: Processing a 500MB log file:
```python
# Define your chunking function
def my_chunker_func(text):
    # Split on timestamp patterns
    return re.split(r'\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\]', text)

# Process with streaming optimization
results = []
for chunk in processor.process_large_file('system.log', my_chunker_func):
    # Process immediately rather than collecting all chunks
    analysis = analyze_log_chunk(chunk)
    results.append(analysis)
    
    # You could also send to a queue for distributed processing
    # queue.send(chunk)
```

**Pro tip**: For web applications, combine this with async processing to handle file uploads without blocking:

```python
async def process_uploaded_file(file_path):
    processor = ChunkProcessor(options)
    
    # Start processing in an executor to avoid blocking
    loop = asyncio.get_event_loop()
    for chunk in await loop.run_in_executor(
        None, 
        lambda: list(processor.process_large_file(file_path, chunker_func))
    ):
        yield chunk
```

### Dynamic Configuration

For systems that need runtime configuration changes:

```python
from enterprise_chunker import DynamicConfig

# Create dynamic config
config = DynamicConfig({
    'processing_timeout': 300,  # 5 minutes
    'max_retries': 3,
    'memory_safety': True
})

# Update configuration at runtime
config.set('processing_timeout', 600)  # Increase to 10 minutes

# Use in smart chunker
smart_chunker = create_auto_chunker(options, config=config)
```

**What dynamic configuration enables:**
1. Changing behavior without restarting your application
2. A/B testing different chunking settings
3. Adapting to varying workloads automatically
4. Centralized configuration for distributed systems

**Beyond the basics - remote configuration:**
```python
# Enable remote configuration updates
config.start_auto_update(
    url="https://your-config-server.com/chunker-config.json",
    interval=300.0  # Check every 5 minutes
)

# Later, stop auto-updates
config.stop_auto_update()
```

**Real-world application**: Automatically adjust processing parameters based on server load:

```python
def update_config_based_on_load():
    # Check system load
    load = os.getloadavg()[0] / os.cpu_count()
    
    if load > 0.8:  # High load
        config.update({
            'max_workers': 2,
            'memory_safety': True,
            'processing_timeout': 600  # Longer timeout
        })
    else:  # Normal load
        config.update({
            'max_workers': os.cpu_count(),
            'memory_safety': False,
            'processing_timeout': 300
        })
    
# Run this check periodically
import threading
threading.Timer(60.0, update_config_based_on_load).start()
```

**Warning**: When using remote configuration, ensure your config source is secure and validated to prevent security issues.

### Metrics and Monitoring

For production systems, monitoring is crucial:

```python
from enterprise_chunker import start_monitoring_server

# Start Prometheus metrics server
start_monitoring_server(port=8000)

# Get performance metrics
metrics = smart_chunker.get_metrics()
print(f"Processed {metrics['total_chunks_processed']} chunks")
print(f"Average throughput: {metrics['avg_throughput']:.2f} chunks/second")
```

**Key metrics to watch:**
- **Processing throughput**: Chunks processed per second
- **Memory usage**: Peak and average memory consumption
- **Error rates**: Frequency of processing failures
- **Worker utilization**: How effectively parallel workers are being used
- **Batch sizing**: Average batch size for parallel processing
- **Strategy distribution**: Which strategies are being selected

**Visualizing with Grafana:**
1. Configure Prometheus to scrape the metrics endpoint
2. Set up a Grafana dashboard with key metrics
3. Create alerts for anomalies (high error rates, memory spikes)

**Metrics in practice - detecting and resolving issues:**

```python
# Periodic health check
def health_check():
    metrics = smart_chunker.get_metrics()
    
    # Check for warning signs
    if metrics['error_count'] > 10:
        logging.warning("High error count detected, investigating...")
        
    if metrics['system']['memory_percent'] > 85:
        logging.warning("Memory pressure detected")
        # Force garbage collection
        smart_chunker._memory_manager.reduce_memory_usage(force=True)
        
    if metrics['avg_throughput'] < 1.0:  # Less than 1 chunk/second
        logging.warning("Performance degradation detected")
        # Reset circuit breaker if open
        if metrics['system']['circuit_breaker'] == "OPEN":
            smart_chunker.circuit_breaker_status = "CLOSED"
            
    # Log health status
    logging.info(f"System health: {metrics['system']['health_status']}")
```

**Production tip**: Create a dedicated health endpoint in your application that returns chunker metrics along with other system health information.

## Configuration Options

Enterprise Chunker is highly configurable to match your specific needs. The `ChunkingOptions` class is the control center for fine-tuning the chunking behavior. Let's look at each option in depth:

| Option | Default | Description | When to Adjust |
|--------|---------|-------------|----------------|
| `max_tokens_per_chunk` | 4000 | Maximum tokens per chunk | Adjust based on your LLM's context window |
| `overlap_tokens` | 200 | Number of tokens to overlap between chunks | Increase for complex documents needing more context |
| `reserved_tokens` | 1000 | Buffer tokens kept in reserve | Reduce for more efficient token usage, increase for safety |
| `chunking_strategy` | ADAPTIVE | Strategy for chunking | Change when you know your content type exactly |
| `token_strategy` | BALANCED | Strategy for token estimation | Switch to PERFORMANCE for speed, PRECISION for accuracy |
| `preserve_structure` | True | Preserve document structure | Disable only for simple text when structure doesn't matter |
| `enable_format_detection` | True | Auto-detect content format | Disable if you know format and want to skip detection |
| `format_detection_sample_size` | 2500 | Sample size for format detection | Increase for more accurate but slower detection |
| `add_metadata_comments` | True | Add metadata comments to chunks | Disable if you need clean chunks without metadata |
| `safety_margin` | 0.9 | Safety margin for token limits (90%) | Lower for more aggressive chunking, raise for more safety |
| `max_chunk_size_chars` | 0 | Max chunk size in chars (0 = use tokens) | Set when character count matters more than tokens |
| `target_chunk_ratio` | 0.8 | Target ratio for chunk sizes | Adjust for more consistent chunk sizes |
| `stream_buffer_size` | 100000 | Buffer size for streaming (100KB) | Increase for faster streaming of large files |
| `cache_size` | 1000 | Cache size for token estimation | Increase if processing repetitive content |
| `respect_sentences` | True | Try to break at sentence boundaries | Disable only for non-narrative content |

### Detailed Explanation and Examples

**max_tokens_per_chunk**  
This is the maximum size of each chunk in tokens.

```python
# For GPT-3.5-Turbo (4K context)
options = ChunkingOptions(max_tokens_per_chunk=3500)  # Leave room for response

# For Claude 3 Opus (200K context)
options = ChunkingOptions(max_tokens_per_chunk=180000)  # Much larger chunks
```

**overlap_tokens**  
How many tokens should be repeated between chunks to maintain context.

```python
# For factual content (minimal overlap needed)
options = ChunkingOptions(overlap_tokens=50)

# For narrative content (more context preservation)
options = ChunkingOptions(overlap_tokens=400)

# For complex technical documentation
options = ChunkingOptions(overlap_tokens=300)
```

**Tip**: The right overlap depends on your content. Too little and you lose context between chunks; too much wastes tokens. Start with 5-10% of your max_tokens_per_chunk.

**chunking_strategy**  
Determines how text is analyzed for chunking boundaries.

```python
from enterprise_chunker.models.enums import ChunkingStrategy

# For source code
options = ChunkingOptions(chunking_strategy=ChunkingStrategy.STRUCTURAL)

# For narrative text
options = ChunkingOptions(chunking_strategy=ChunkingStrategy.SEMANTIC)

# For mixed content (default)
options = ChunkingOptions(chunking_strategy=ChunkingStrategy.ADAPTIVE)
```

**token_strategy**  
Controls the approach to estimating token counts.

```python
from enterprise_chunker.models.enums import TokenEstimationStrategy

# For high-volume processing when exact counts aren't critical
options = ChunkingOptions(token_strategy=TokenEstimationStrategy.PERFORMANCE)

# For billing-critical applications
options = ChunkingOptions(token_strategy=TokenEstimationStrategy.PRECISION)
```

**preserve_structure and respect_sentences**  
These options control how strictly the chunker tries to maintain document structure.

```python
# For code or structured data
options = ChunkingOptions(
    preserve_structure=True,
    respect_sentences=False  # Code doesn't have traditional sentences
)

# For legal documents where sentence integrity is critical
options = ChunkingOptions(
    preserve_structure=True,
    respect_sentences=True
)
```

**safety_margin**  
A multiplier applied to max_tokens_per_chunk to prevent going over limits.

```python
# More aggressive chunking (95% of max)
options = ChunkingOptions(safety_margin=0.95)

# Very conservative (80% of max)
options = ChunkingOptions(safety_margin=0.8)
```

**Real-world example: Configuring for different document types**

```python
# Configuration for code repositories
code_options = ChunkingOptions(
    max_tokens_per_chunk=3000,
    overlap_tokens=150,
    chunking_strategy=ChunkingStrategy.STRUCTURAL,
    preserve_structure=True,
    respect_sentences=False,
    safety_margin=0.9
)

# Configuration for legal documents
legal_options = ChunkingOptions(
    max_tokens_per_chunk=2000,  # Smaller chunks for precise processing
    overlap_tokens=300,         # More overlap to maintain context
    chunking_strategy=ChunkingStrategy.SENTENCE,
    preserve_structure=True,
    respect_sentences=True,
    safety_margin=0.85
)

# Configuration for high-performance log processing
logs_options = ChunkingOptions(
    max_tokens_per_chunk=5000,
    overlap_tokens=100,
    chunking_strategy=ChunkingStrategy.STRUCTURAL,
    token_strategy=TokenEstimationStrategy.PERFORMANCE,
    stream_buffer_size=200000,  # Larger buffer for faster processing
    cache_size=2000
)
```

**Pro tip**: Creating preset configurations for different document types in your application can simplify usage and ensure consistent processing.

**Fine-tuning for token efficiency**

If you need to maximize token efficiency (e.g., to minimize API costs):

```python
efficient_options = ChunkingOptions(
    max_tokens_per_chunk=4000,
    overlap_tokens=100,           # Minimal overlap
    reserved_tokens=500,          # Smaller reserve
    safety_margin=0.95,           # Use more of the available tokens
    add_metadata_comments=False,  # Skip metadata comments
)
```

**Warning signs to adjust configuration:**
- Chunks consistently much smaller than your max_tokens_per_chunk → Increase target_chunk_ratio
- Content appearing truncated or missing → Check overlap_tokens and safety_margin
- Processing very slow → Consider adjusting token_strategy or streaming parameters
- Memory issues → Reduce stream_buffer_size or enable memory_safety

## Chunking Strategies

Think of chunking strategies as different methods an editor might use to divide a book into chapters. Each has its own strengths and is suited for different types of content. Enterprise Chunker supports multiple approaches through the `ChunkingStrategy` enum:

### ADAPTIVE (Default)
The smart, all-purpose strategy that automatically selects the best approach based on content type.

```python
from enterprise_chunker.models.enums import ChunkingStrategy

chunker.adaptive_chunk_text(text, strategy=ChunkingStrategy.ADAPTIVE)
```

**How it works:** 
1. Analyzes a sample of your content
2. Determines the most likely format (JSON, Markdown, code, etc.)
3. Selects the optimal strategy for that format
4. Applies format-specific optimizations

**When to use ADAPTIVE:**
- When processing mixed or unknown content types
- In general-purpose applications
- When you want the system to "just work" without manual tuning
- For batch processing diverse document collections

**Under the hood:** If it detects JSON, it will use structural chunking; for narrative text, it will use semantic chunking; for code, it will apply code-aware chunking, and so on.

**Pro tip:** Even with automatic detection, providing hints about your content type can improve results:

```python
# Help the chunker with a hint about content
if filename.endswith('.json'):
    chunker.with_strategy(ChunkingStrategy.STRUCTURAL).chunk(text)
else:
    chunker.with_strategy(ChunkingStrategy.ADAPTIVE).chunk(text)
```

### SEMANTIC
The "meaning-preserving" strategy that focuses on natural language boundaries.

```python
chunks = chunker.adaptive_chunk_text(text, strategy=ChunkingStrategy.SEMANTIC)
```

**How it works:**
1. Identifies semantic boundaries like paragraphs, sections, and topic changes
2. Preserves these boundaries during chunking
3. Adds semantic context between related chunks
4. Avoids breaking mid-paragraph or mid-idea when possible

**When to use SEMANTIC:**
- For narrative text, articles, books, and essays
- When content coherence is critical
- For content where understanding "flows" of ideas matters
- When processing natural language documents

**Real-world example:** A customer service chatbot processing product manuals would benefit from semantic chunking to keep related instructions together.

**What it preserves:**
- Paragraph integrity
- Section coherence
- List completeness
- Quote boundaries

**Practical results:**
```python
# Semantic chunking on a textbook
with chunker.semantic_context():
    chapter_chunks = chunker.chunk(chapter_text)
    
    # First chunk might include the introduction and first section
    # Second chunk might start at a natural section break
    # Context is preserved between sections
```

### STRUCTURAL
The "structure-respecting" strategy that preserves formatting and organizational elements.

```python
chunks = chunker.adaptive_chunk_text(text, strategy=ChunkingStrategy.STRUCTURAL)
```

**How it works:**
1. Identifies structural elements (tags, brackets, blocks, etc.)
2. Maintains the structural integrity of these elements
3. Ensures chunks contain valid and usable structures
4. Preserves hierarchical relationships when possible

**When to use STRUCTURAL:**
- For JSON, XML, HTML, or other structured formats
- For source code in any language
- When processing configuration files
- For any content where syntax matters

**Real-world example:** Processing API responses, where each JSON object needs to remain valid:

```python
# Process a large API response
api_json = requests.get("https://api.example.com/large-dataset").json()
json_text = json.dumps(api_json)

with chunker.structural_context():
    json_chunks = chunker.chunk(json_text)
    
    # Each chunk will contain valid JSON
    for chunk in json_chunks:
        chunk_data = json.loads(chunk)  # Won't raise exceptions
        process_json_chunk(chunk_data)
```

**What it preserves:**
- JSON/XML validity
- Tag matching and nesting
- Code block integrity
- Function and class boundaries
- Table structures

**Tip:** For highly structured content like programming languages, structural chunking often performs better than semantic chunking.

### FIXED_SIZE
The "simple and predictable" strategy that focuses on consistent chunk sizes.

```python
chunks = chunker.adaptive_chunk_text(text, strategy=ChunkingStrategy.FIXED_SIZE)
```

**How it works:**
1. Divides text into chunks of approximately equal token counts
2. Applies overlap between chunks
3. Attempts to break at reasonable boundaries when possible
4. Prioritizes consistent sizing over structural preservation

**When to use FIXED_SIZE:**
- When predictable chunk sizes are more important than boundaries
- For performance benchmarking or testing
- When processing homogenous content
- When token usage predictability matters
- In memory-constrained environments

**Real-world example:** Creating training data for fine-tuning a model where consistent chunk sizes are desired:

```python
# Create fixed-size chunks for training data
training_chunks = []
for document in training_documents:
    with chunker.fixed_size_context(max_tokens=1024, overlap=0):
        document_chunks = chunker.chunk(document)
        training_chunks.extend(document_chunks)

# All chunks will be close to 1024 tokens
```

**Pro tip:** Even with fixed-size chunking, the system still tries to break at reasonable points like paragraph boundaries when possible, making it superior to naive character-based chunking.

### SENTENCE
The "precision-focused" strategy that strictly preserves sentence integrity.

```python
chunks = chunker.adaptive_chunk_text(text, strategy=ChunkingStrategy.SENTENCE)
```

**How it works:**
1. Identifies sentence boundaries using punctuation and capitalization
2. Never breaks mid-sentence under any circumstance
3. Keeps related sentences together when possible
4. Groups sentences to fit within token limits

**When to use SENTENCE:**
- For legal documents where sentence integrity is crucial
- When extracting facts or statements that shouldn't be fragmented
- For content where each sentence carries complete meaning
- When processing formal or technical writing

**Real-world example:** Processing legal contracts where partial sentences could change meaning:

```python
# Process a legal contract with sentence preservation
with chunker.sentence_context():
    contract_chunks = chunker.chunk(contract_text)
    
    # Each chunk will only contain complete sentences
    # No sentence will ever be split between chunks
```

**Special capability:** The SENTENCE strategy includes enhanced detection for various sentence styles, including:
- Standard periods (e.g., "This is a sentence.")
- Question marks and exclamation points
- Semicolon-separated independent clauses
- Quoted speech with proper punctuation
- Bulleted or numbered lists

**Comparative Example:**

Given this text:
```
The agreement shall terminate on December 31, 2023. In the event of early termination, 
the following conditions apply: (1) All outstanding invoices must be paid. 
(2) Confidential materials must be returned. No partial refunds will be issued.
```

**FIXED_SIZE might chunk as:**
```
Chunk 1: "The agreement shall terminate on December 31, 2023. In the event of early termination, the following conditions apply: (1) All outstanding invoices must be paid."

Chunk 2: "(2) Confidential materials must be returned. No partial refunds will be issued."
```

**SENTENCE will ensure:**
```
Chunk 1: "The agreement shall terminate on December 31, 2023."

Chunk 2: "In the event of early termination, the following conditions apply: (1) All outstanding invoices must be paid. (2) Confidential materials must be returned."

Chunk 3: "No partial refunds will be issued."
```

**Choosing the Right Strategy - Decision Guide:**

1. **Default approach**: Start with ADAPTIVE
2. **If processing narrative text**: Use SEMANTIC
3. **If processing structured data or code**: Use STRUCTURAL
4. **If consistent sizing matters most**: Use FIXED_SIZE
5. **If sentence integrity is critical**: Use SENTENCE

**Expert tip:** For the absolute best results, use a hybrid approach based on content type detection in your application:

```python
def choose_strategy(content_type, filename):
    if content_type.startswith('application/json'):
        return ChunkingStrategy.STRUCTURAL
    elif content_type.startswith('text/html'):
        return ChunkingStrategy.STRUCTURAL
    elif filename.endswith(('.py', '.js', '.java', '.cpp')):
        return ChunkingStrategy.STRUCTURAL
    elif filename.endswith(('.md', '.txt')):
        return ChunkingStrategy.SEMANTIC
    elif filename.endswith(('.doc', '.pdf')):
        return ChunkingStrategy.SENTENCE
    else:
        return ChunkingStrategy.ADAPTIVE
```

## Format-Specific Chunking

Enterprise Chunker shines in its ability to handle different content formats intelligently. Let's explore how it handles specific formats with special understanding of their structure and requirements.

### JSON Chunking

JSON is a structured data format where preserving valid structure is critical - a half-broken JSON object is useless. Enterprise Chunker's JSON handling maintains valid JSON structure in each chunk.

```python
json_chunks = chunker.adaptive_chunk_text(json_text, strategy=ChunkingStrategy.STRUCTURAL)
```

**How JSON chunking works:**
1. Parses the entire JSON structure first
2. Identifies logical boundaries (objects, arrays, properties)
3. Creates chunks along these boundaries
4. Adds metadata to maintain context between chunks
5. Ensures each chunk is valid, parseable JSON

Each chunk is a valid JSON object with metadata that looks like this:

```json
{
  "_chunk_info": {
    "index": 0,          // The chunk's position in sequence
    "type": "json_array", // Type of JSON structure
    "total": 3,          // Total number of chunks
    "has_overlap": false // Whether this chunk has overlap with previous
  },
  "data": [...]          // The actual JSON content
}
```

**When chunking JSON arrays:**
- Array items are grouped to fit within token limits
- Related items are kept together when possible
- The `data` field contains a valid array that can be parsed directly

**When chunking JSON objects:**
- Related properties are kept together
- Property names are tracked between chunks
- Nested objects are handled intelligently

**Example: Processing a large JSON API response**

```python
# Fetch a large JSON dataset
response = requests.get("https://api.example.com/large-dataset")
json_text = response.text

# Chunk with structural awareness
json_chunks = chunker.adaptive_chunk_text(json_text, strategy=ChunkingStrategy.STRUCTURAL)

# Each chunk can be parsed as valid JSON
for chunk in json_chunks:
    chunk_data = json.loads(chunk)  # Always valid JSON
    
    # Access chunk info
    chunk_index = chunk_data["_chunk_info"]["index"]
    total_chunks = chunk_data["_chunk_info"]["total"]
    
    # Access actual data - note that we can use normal JSON parsing!
    items = chunk_data["data"] 
    
    print(f"Processing chunk {chunk_index+1}/{total_chunks} with {len(items)} items")
```

**Pro tip:** You can reconstruct the original JSON by tracking the chunk info and merging appropriately:

```python
def reconstruct_json_array(chunks):
    # Parse all chunks
    parsed_chunks = [json.loads(chunk) for chunk in chunks]
    
    # Sort by index if needed
    parsed_chunks.sort(key=lambda x: x["_chunk_info"]["index"])
    
    # Combine all data
    all_items = []
    for chunk in parsed_chunks:
        all_items.extend(chunk["data"])
        
    return all_items
```

**Advanced usage - JSON path targeting:**
When dealing with deeply nested JSON, you can target specific paths for chunking:

```python
# Focus chunking on a specific path
from enterprise_chunker.strategies.formats.json_chunker import JsonChunkingStrategy

strategy = JsonChunkingStrategy()
strategy.target_path = "$.results.items"  # JSONPath expression

# Now chunking will focus on that path
chunks = strategy.chunk(json_text, options)
```

### Markdown Chunking

Markdown is a format where heading structure creates a natural hierarchy of content. Enterprise Chunker preserves this structure when chunking markdown documents.

```python
markdown_chunks = chunker.adaptive_chunk_text(markdown_text, strategy=ChunkingStrategy.STRUCTURAL)
```

**How Markdown chunking works:**
1. Identifies heading levels (# Header, ## Subheader, etc.)
2. Creates a hierarchical outline of the document
3. Chunks at appropriate heading boundaries
4. Preserves heading context across chunks
5. Maintains list structure, code blocks, and other markdown elements

A typical markdown chunk includes context from previous chunks:

```markdown
<!-- Context from previous chunk -->
# Previous Section

<!-- Current content -->
## Current Subsection
Content text...
```

**Special handling for markdown elements:**
- **Headers**: Used as primary chunk boundaries
- **Lists**: Kept intact, never split mid-list
- **Code blocks**: Preserved completely in one chunk when possible
- **Tables**: Kept intact without splitting rows
- **Block quotes**: Maintained as complete units
- **Images and links**: Preserved with proper syntax

**Real-world example: Processing documentation**

```python
# Load a markdown document
with open("documentation.md", "r") as f:
    markdown_text = f.read()

# Process with markdown awareness
chunks = chunker.adaptive_chunk_text(markdown_text, strategy=ChunkingStrategy.STRUCTURAL)

# Extract headings hierarchy from each chunk
for i, chunk in enumerate(chunks):
    headings = re.findall(r'^(#+)\s+(.*)', chunk, re.MULTILINE)
    print(f"Chunk {i+1} main heading: {headings[0][1] if headings else 'No heading'}")
```

**Practical application: Building a document outline**

```python
def extract_document_structure(markdown_text):
    chunker = EnterpriseChunker()
    chunks = chunker.adaptive_chunk_text(markdown_text, strategy=ChunkingStrategy.STRUCTURAL)
    
    document_outline = []
    for chunk in chunks:
        # Extract all headings with their levels
        headings = re.findall(r'^(#+)\s+(.*)', chunk, re.MULTILINE)
        for h_marks, h_text in headings:
            level = len(h_marks)  # Number of # symbols
            document_outline.append((level, h_text))
    
    return document_outline
```

**Pro tip:** When processing documentation, combine with semantic understanding:

```python
# Process technical docs intelligently
def process_technical_docs(markdown_text):
    # First split by structural boundaries
    chunker = EnterpriseChunker()
    structural_chunks = chunker.adaptive_chunk_text(
        markdown_text, 
        strategy=ChunkingStrategy.STRUCTURAL
    )
    
    results = []
    for chunk in structural_chunks:
        # Analyze heading level to determine importance
        headings = re.findall(r'^(#+)\s+(.*)', chunk, re.MULTILINE)
        if not headings:
            continue
            
        main_heading = headings[0]
        heading_level = len(main_heading[0])  # Number of # symbols
        heading_text = main_heading[1]
        
        # Apply different processing based on section type
        if "Installation" in heading_text:
            # Extract installation requirements
            requirements = extract_requirements(chunk)
            results.append({"type": "installation", "data": requirements})
            
        elif "API Reference" in heading_text or heading_level <= 2:
            # Important reference section
            api_details = extract_api_details(chunk)
            results.append({"type": "api_reference", "data": api_details})
            
        else:
            # Regular content
            summary = summarize_section(chunk)
            results.append({"type": "content", "data": summary})
    
    return results
```

### Code Chunking

Enterprise Chunker has specialized handling for various programming languages that respects the structure of code.

```python
code_chunks = chunker.adaptive_chunk_text(code_text, strategy=ChunkingStrategy.STRUCTURAL)
```

**How code chunking works:**
1. Detects the programming language (Python, JavaScript, Java, etc.)
2. Identifies structural elements (functions, classes, methods)
3. Respects scope and nesting (brackets, indentation)
4. Preserves import statements and context between chunks
5. Adds necessary context comments between chunks

**Languages with special handling:**
- **Python**: Respects indentation-based scope, detects classes and functions
- **JavaScript/TypeScript**: Understands function declarations, classes, and modules
- **Java/C#**: Recognizes class structure, methods, and package/namespace
- **React/Vue**: Special handling for component structure
- **Smalltalk**: Specialized for Smalltalk method and class handling

**Example: Processing a Python codebase**

```python
# Process a Python file
with open("source_code.py", "r") as f:
    python_code = f.read()

# Chunk with code awareness
code_chunks = chunker.adaptive_chunk_text(
    python_code, 
    strategy=ChunkingStrategy.STRUCTURAL,
    max_tokens_per_chunk=1500
)

# Analyze chunks
for i, chunk in enumerate(code_chunks):
    # Check for function definitions
    functions = re.findall(r'def\s+(\w+)\s*\(', chunk)
    classes = re.findall(r'class\s+(\w+)', chunk)
    
    print(f"Chunk {i+1}:")
    print(f"  Classes: {', '.join(classes) or 'None'}")
    print(f"  Functions: {', '.join(functions) or 'None'}")
```

**Special feature: React/Vue component chunking**

The chunker has specialized handling for React and Vue components:

```python
# Detect if a file is a React component
from enterprise_chunker.strategies.formats.react_vue_chunker import ReactVueChunkingStrategy

strategy = ReactVueChunkingStrategy()
is_component = strategy._detect_react_component(code)  # Or _detect_vue_component

if is_component:
    # Use special component chunking
    chunks = chunker.adaptive_chunk_text(code, strategy=ChunkingStrategy.STRUCTURAL)
```

**Pro tip for code analysis:** When analyzing code, combine with language-specific parsing:

```python
# For serious code analysis, combine with ast module
import ast

def analyze_python_code(code_text):
    # First chunk the code structurally
    chunker = EnterpriseChunker()
    chunks = chunker.adaptive_chunk_text(code_text, strategy=ChunkingStrategy.STRUCTURAL)
    
    # Analyze each chunk with AST
    for chunk in chunks:
        try:
            # Parse chunk into AST
            module = ast.parse(chunk)
            
            # Extract function and class definitions
            functions = [node.name for node in module.body if isinstance(node, ast.FunctionDef)]
            classes = [node.name for node in module.body if isinstance(node, ast.ClassDef)]
            
            # Deeper analysis...
            for node in ast.walk(module):
                if isinstance(node, ast.Call):
                    # Analyze function calls
                    pass
        except SyntaxError:
            # Some chunks might not be complete statements
            # This is expected when chunking code
            pass
```

### HTML/XML Chunking

HTML and XML are structured formats with nested tags. Enterprise Chunker preserves this structure:

```python
html_chunks = chunker.adaptive_chunk_text(html_text, strategy=ChunkingStrategy.STRUCTURAL)
```

**How HTML/XML chunking works:**
1. Parses the document structure to understand tag hierarchy
2. Identifies logical sections (divs, sections, articles)
3. Ensures tags are properly balanced in each chunk
4. Preserves parent context when splitting nested structures
5. Adds metadata comments to maintain context

**Special handling:**
- HTML semantics (headers, sections, articles) guide chunking
- Table structures are kept intact
- List elements are kept together
- Script and style blocks are not broken internally
- Tag attributes are preserved intact

**Example: Processing an HTML document**

```python
from bs4 import BeautifulSoup

# Process an HTML file
with open("webpage.html", "r") as f:
    html_content = f.read()

# Chunk with HTML awareness
html_chunks = chunker.adaptive_chunk_text(html_content, strategy=ChunkingStrategy.STRUCTURAL)

# Analyze chunks with Beautiful Soup
for i, chunk in enumerate(html_chunks):
    soup = BeautifulSoup(chunk, 'html.parser')
    
    # Extract key elements
    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    paragraphs = soup.find_all('p')
    
    print(f"Chunk {i+1}:")
    print(f"  Headings: {len(headings)}")
    print(f"  Paragraphs: {len(paragraphs)}")
    
    if headings:
        print(f"  Main heading: {headings[0].text.strip()}")
```

### CSV and Tabular Data Chunking

Spreadsheet data requires special handling to maintain row integrity and header context:

```python
csv_chunks = chunker.adaptive_chunk_text(csv_text, strategy=ChunkingStrategy.STRUCTURAL)
```

**How CSV chunking works:**
1. Identifies the delimiter (comma, tab, semicolon)

### Code Chunking

Respects method and class boundaries:

```python
code_chunks = chunker.adaptive_chunk_text(code_text, strategy=ChunkingStrategy.STRUCTURAL)
```

Special support for:
- Python, JavaScript, Java, C#
- React and Vue components
- Smalltalk

### XML/HTML Chunking

Preserves tag structure and nesting:

```python
xml_chunks = chunker.adaptive_chunk_text(xml_text, strategy=ChunkingStrategy.STRUCTURAL)
```

## Memory Optimization

For processing large files with minimal memory impact:

```python
from enterprise_chunker.utils.memory_optimization import MemoryManager

# Create memory manager
memory_manager = MemoryManager(low_memory_mode=True)

# Process with memory optimization
with memory_manager.memory_efficient_context(memory_limit_mb=1000):
    # Process large document
    content = memory_manager.get_content(large_content_or_path)
    chunks = chunker.chunk(content)
```

Memory-efficient streaming:

```python
from enterprise_chunker.utils.memory_optimization import MemoryEfficientIterator

# Create iterator with processor function
iterator = MemoryEfficientIterator(processor_func)

# Process file with minimal memory usage
for result in iterator.iter_file('huge_file.txt'):
    process_result(result)
```

## Parallel Processing

The orchestration layer provides sophisticated parallel processing:

```python
from enterprise_chunker import SmartParallelChunker, ChunkingOptions

# Create options
options = ChunkingOptions(max_tokens_per_chunk=2000)

# Create parallel chunker
parallel_chunker = SmartParallelChunker(
    options=options,
    size_threshold=100_000,  # 100KB threshold for strategy selection
    complexity_threshold=0.5,
    memory_safety=True,
    adaptive_batch_sizing=True
)

# Process with automatic strategy selection
chunks = parallel_chunker.chunk(text, lambda x: x.split('\n'))
```

### Quality of Service (QoS) Prioritization

```python
# Process with high priority
high_priority_chunks = parallel_chunker.chunk_with_priority(
    text, 
    chunker_func,
    priority='high'  # 'high', 'normal', or 'background'
)
```

## Performance Monitoring

```python
# Get detailed metrics
metrics = parallel_chunker.get_metrics()

print(f"System health: {metrics['system']['health_status']}")
print(f"Circuit breaker: {metrics['system']['circuit_breaker']}")
print(f"Average throughput: {metrics['avg_throughput']:.2f} chunks/second")
print(f"Average processing time: {metrics['avg_processing_time']:.3f}s")
print(f"Memory usage: {metrics['system']['memory_percent']:.1f}%")

# Reset metrics
parallel_chunker.reset_metrics()
```

Enable Prometheus metrics:

```python
from enterprise_chunker import start_monitoring_server

# Start metrics server on port 8000
start_monitoring_server(port=8000)

# Create chunker with metrics enabled
smart_chunker = create_auto_chunker(
    options,
    enable_metrics_server=True,
    metrics_port=8000
)
```

## Extending Enterprise Chunker

### Custom Chunking Strategy

```python
from enterprise_chunker.strategies.base import BaseChunkingStrategy
from enterprise_chunker.models.enums import ContentFormat, ChunkingStrategy

class MyCustomStrategy(BaseChunkingStrategy):
    def __init__(self):
        super().__init__(ContentFormat.TEXT)
        
    def detect_boundaries(self, text, options):
        # Custom boundary detection
        boundaries = []
        # ...
        return boundaries
        
    def _get_chunking_strategy(self):
        return ChunkingStrategy.SEMANTIC
```

### Custom Token Estimator

```python
from enterprise_chunker.utils.token_estimation import BaseTokenEstimator

class MyTokenEstimator(BaseTokenEstimator):
    def _calculate_estimate(self, features, text):
        # Custom token calculation
        # ...
        return estimated_tokens
```

## Environment Variables

Enterprise Chunker can be configured with environment variables:

```bash
# Set maximum tokens per chunk
export CHUNKER_MAX_TOKENS_PER_CHUNK=2000

# Set chunking strategy
export CHUNKER_CHUNKING_STRATEGY=semantic

# Set token estimation strategy
export CHUNKER_TOKEN_STRATEGY=precision

# Enable/disable format detection
export CHUNKER_ENABLE_FORMAT_DETECTION=true

# Set overlap tokens
export CHUNKER_OVERLAP_TOKENS=100
```

## API Reference

### EnterpriseChunker

```python
class EnterpriseChunker:
    def __init__(self, options: Optional[Dict[str, Any]] = None):
        """Initialize with options dictionary"""
        
    def adaptive_chunk_text(
        self, 
        text: str, 
        max_tokens_per_chunk: Optional[int] = None,
        overlap_tokens: Optional[int] = None,
        strategy: Optional[Union[str, ChunkingStrategy]] = None
    ) -> List[str]:
        """Main chunking method with adaptable options"""
        
    def chunk_stream(
        self, 
        stream: Union[str, io.TextIOBase], 
        **kwargs
    ) -> Generator[str, None, None]:
        """Process text stream with chunking"""
        
    def with_max_tokens(self, max_tokens: int) -> 'EnterpriseChunker':
        """Fluent API for setting max tokens"""
        
    def with_overlap(self, overlap_tokens: int) -> 'EnterpriseChunker':
        """Fluent API for setting overlap tokens"""
        
    def with_strategy(self, strategy: ChunkingStrategy) -> 'EnterpriseChunker':
        """Fluent API for setting chunking strategy"""
        
    def chunk(self, text: str) -> List[str]:
        """Simple chunking with current configuration"""
        
    # Context managers for specific chunking strategies
    def semantic_context(self, max_tokens=None, overlap=None):
        """Context for semantic chunking"""
        
    def structural_context(self, max_tokens=None, overlap=None):
        """Context for structural chunking"""
        
    def fixed_size_context(self, max_tokens=None, overlap=None):
        """Context for fixed-size chunking"""
        
    def sentence_context(self, max_tokens=None, overlap=None):
        """Context for sentence-based chunking"""
```

### SmartParallelChunker

```python
class SmartParallelChunker:
    def __init__(
        self,
        options: ChunkingOptions,
        size_threshold: int = 100_000,
        complexity_threshold: float = 0.5,
        sample_size: int = 1000,
        force_strategy: Optional[str] = None,
        timeout: float = 300.0,
        max_retries: int = 3,
        memory_safety: bool = True,
        adaptive_batch_sizing: bool = True,
        health_check_enabled: bool = True,
        worker_count_override: Optional[int] = None,
        resource_monitor_interval: float = 5.0,
        config: Optional[DynamicConfig] = None,
    ):
        """Initialize smart parallel chunker"""
        
    def chunk(self, text: str, chunker_func: Callable[[str], List[str]]) -> List[str]:
        """Process text with automatic strategy selection"""
        
    def stream_chunks(
        self,
        segment_gen: Generator[str, None, None],
        chunker_func: Callable[[str], List[str]]
    ) -> Generator[str, None, None]:
        """Stream chunks with memory efficiency"""
        
    def chunk_with_priority(
        self, 
        text: str, 
        chunker_func: Callable[[str], List[str]], 
        priority: str = 'normal'
    ) -> List[str]:
        """Process with QoS priority"""
        
    def get_metrics(self) -> Dict[str, Any]:
        """Get detailed performance metrics"""
        
    def reset_metrics(self) -> None:
        """Reset performance metrics"""
        
    def shutdown(self) -> None:
        """Gracefully shut down and release resources"""
```

### Factory Functions

```python
def create_auto_chunker(
    options: ChunkingOptions,
    mode: str = "auto",
    memory_safety: bool = True,
    timeout: float = 300.0,
    config: Optional[DynamicConfig] = None,
    enable_metrics_server: bool = False,
    metrics_port: int = 8000,
) -> SmartParallelChunker:
    """Create pre-configured chunker instance"""
    
def start_monitoring_server(port: int = 8000) -> bool:
    """Start Prometheus metrics server"""
```

## Troubleshooting

### Performance Issues

If you're experiencing slow processing:

1. **Use the right token strategy**: For very large files, use `TokenEstimationStrategy.PERFORMANCE`
2. **Adjust batch size**: Set `adaptive_batch_sizing=True` in `SmartParallelChunker`
3. **Disable memory safety**: If memory isn't a concern, set `memory_safety=False`

### Memory Problems

If you're running into memory issues:

1. **Enable memory safety**: Set `memory_safety=True`
2. **Use streaming**: Process files with `chunk_stream()` instead of loading entirely
3. **Reduce overlap**: Lower `overlap_tokens` to reduce redundancy

### Inaccurate Chunking

If chunks aren't preserving structure correctly:

1. **Try structural strategy**: Use `ChunkingStrategy.STRUCTURAL` explicitly
2. **Format-specific chunking**: Use specialized chunkers for JSON, Markdown, etc.
3. **Check format detection**: Make sure format is being detected correctly

### Uneven Chunk Sizes

If chunks vary too much in size:

1. **Adjust safety margin**: Lower `safety_margin` (e.g., 0.8 instead of 0.9)
2. **Use fixed size chunking**: Try `ChunkingStrategy.FIXED_SIZE` 
3. **Set max_chunk_size_chars**: Add explicit character limit

### Error Recovery

The system includes automatic error recovery:

1. **Retry mechanism**: Failed operations retry automatically
2. **Circuit breaker**: Prevents cascading failures under system pressure
3. **Fallback strategies**: If advanced chunking fails, simpler methods are used

For persistent issues, check logs and consider enabling metrics for detailed performance data.

---

This guide covers the core functionality of Enterprise Chunker. For more specific use cases or custom integrations, refer to the codebase or contact the maintainers.
</file>

<file path="enterprise-chunker.md">
# Enterprise Chunker: The Ultimate Guide

![Enterprise Chunker](https://via.placeholder.com/800x200?text=Enterprise+Chunker)

[![Launch Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/enterprise-chunker/examples/main?filepath=tutorials/getting_started.ipynb) 
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enterprise-chunker/examples/blob/main/tutorials/getting_started.ipynb)
[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://enterprise-chunker.readthedocs.io/en/latest/api/)
[![PyPI version](https://badge.fury.io/py/enterprise-chunker.svg)](https://badge.fury.io/py/enterprise-chunker)

<div class="search-container">
    <input type="text" id="search-input" placeholder="Search documentation...">
    <div id="search-results"></div>
</div>

---

## 🟢 **1. Overview & Introduction**

### What is Enterprise Chunker?

Enterprise Chunker is a powerful and intelligent text [chunking](#chunking) library designed to break down large documents into smaller, meaningful pieces (or "chunks") that can be processed by [Large Language Models](#large-language-model-llm) (LLMs) like GPT-4 or Claude. 

### Why does it exist?

Most LLMs have a limited "[context window](#context-window)" - they can only process a certain amount of text at once (typically several thousand [tokens](#token)). When you need to work with larger documents like research papers, code files, or lengthy reports, you need to split them into smaller pieces that fit within these limits. But splitting text randomly can break the meaning and structure of the content.

Enterprise Chunker solves this problem by intelligently splitting content while preserving its meaning and structure.

### Who is it designed for?

- **Developers** building applications that use LLMs
- **Data scientists** working with large text datasets
- **AI engineers** creating [retrieval-augmented generation](#retrieval-augmented-generation-rag) (RAG) systems
- **Enterprise software teams** processing large volumes of documents

### How does it fit into the bigger picture?

In the AI/LLM ecosystem, Enterprise Chunker sits between your raw documents and the LLM processing pipeline:

```
Raw Documents → Enterprise Chunker → Optimized Chunks → LLM Processing → Insights/Responses
```

It's a critical component in any system that needs to feed large documents into LLMs efficiently.

### The Librarian Analogy

Think of Enterprise Chunker as an expert librarian. If you brought a massive 1,000-page book to a student who can only read 4 pages at a time, a good librarian wouldn't just tear the book into 250 random 4-page sections. Instead, they would carefully divide it by chapters, sections, and natural breaks in the content, ensuring each chunk makes sense on its own while preserving the overall structure.

Enterprise Chunker does exactly this for your text data - it finds the natural "chapters" and "sections" in your content to create meaningful chunks that LLMs can process effectively.

---

## 🚧 **2. Necessary Dependencies & Installation**

### Required Dependencies

Enterprise Chunker has been designed with minimal external dependencies to ensure easy integration into your projects:

**Core Dependencies:**
- Python 3.8 or higher
- `psutil` - For system resource monitoring
- `numpy` - For numerical operations (optional but recommended)
- `prometheus_client` - For metrics collection (optional)
- `requests` - For remote configuration updates (optional)

**Format-Specific Dependencies (Optional):**
- `regex` - Enhanced regular expression support for complex pattern matching

### Installation

You can install Enterprise Chunker using pip:

```bash
# Core installation
pip install enterprise-chunker

# With all optional dependencies
pip install enterprise-chunker[all]

# Development version (with testing utilities)
pip install enterprise-chunker[dev]
```

### Version Compatibility

Enterprise Chunker is designed to work with:
- Python 3.8+
- Most major LLM frameworks and APIs
- Common document processing pipelines

### Potential Installation Issues

**Problem:** Import errors related to missing optional dependencies

**Solution:** Install the specific optional dependency:
```bash
pip install regex  # For enhanced pattern matching
pip install numpy  # For advanced numerical operations
pip install prometheus_client  # For metrics
```

**Problem:** Version conflicts with existing packages

**Solution:** Consider using a virtual environment:
```bash
python -m venv chunker-env
source chunker-env/bin/activate  # On Windows: chunker-env\Scripts\activate
pip install enterprise-chunker
```

---

## 🎈 **3. For Beginners: Setting Up a Starter Project (Extra Detailed)**

Let's create a simple project from scratch that uses Enterprise Chunker to process a document.

### Step 1: Set up your environment

First, let's create a folder for our project and set up a virtual environment:

```bash
# Create a project folder
mkdir my-chunker-project
cd my-chunker-project

# Create a virtual environment
python -m venv venv

# Activate the virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### Step 2: Install Enterprise Chunker

With your virtual environment activated, install Enterprise Chunker:

```bash
pip install enterprise-chunker
```

### Step 3: Create a simple text file to process

Create a file named `sample.txt` in your project folder with some text to chunk. You can use any text editor (like Notepad, VS Code, etc.). Here's how to do it from the command line:

```bash
# On Windows:
echo "# Sample Document\n\nThis is a sample document with multiple paragraphs.\n\n## Section 1\n\nHere is some content for section 1.\n\n## Section 2\n\nHere is content for section 2." > sample.txt

# On macOS/Linux:
cat > sample.txt << EOF
# Sample Document

This is a sample document with multiple paragraphs.

## Section 1

Here is some content for section 1.

## Section 2

Here is content for section 2.
EOF
```

### Step 4: Create your first chunking script

Using any text editor, create a file named `simple_chunker.py` with the following code:

```python
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy

# Initialize the chunker
chunker = EnterpriseChunker()

# Read the sample file
with open('sample.txt', 'r') as file:
    text = file.read()

# Break the text into chunks
chunks = chunker.adaptive_chunk_text(
    text=text,
    max_tokens_per_chunk=100,  # Max tokens per chunk
    overlap_tokens=20,         # Overlap between chunks
    strategy=ChunkingStrategy.SEMANTIC  # Use semantic chunking
)

# Print the chunks
print(f"Split into {len(chunks)} chunks:")
for i, chunk in enumerate(chunks):
    print(f"\n--- Chunk {i+1} ---")
    print(chunk[:100] + "..." if len(chunk) > 100 else chunk)
```

### Step 5: Run your script

Run the script from your command line:

```bash
python simple_chunker.py
```

You should see output showing how the document was split into chunks, with each chunk preserving the semantic structure of the text.

### Step 6: Explore different chunking strategies

Now, let's modify our script to try different chunking strategies. Update the `simple_chunker.py` file:

```python
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy

# Initialize the chunker
chunker = EnterpriseChunker()

# Read the sample file
with open('sample.txt', 'r') as file:
    text = file.read()

# Try different strategies
strategies = [
    ChunkingStrategy.SEMANTIC,   # Semantic boundaries
    ChunkingStrategy.FIXED_SIZE, # Fixed size chunks
    ChunkingStrategy.SENTENCE,   # Split by sentences
    ChunkingStrategy.STRUCTURAL, # Format-aware splitting
    ChunkingStrategy.ADAPTIVE    # Automatically select best strategy
]

# Test each strategy
for strategy in strategies:
    print(f"\n\n=== Testing {strategy.value} strategy ===\n")
    
    chunks = chunker.adaptive_chunk_text(
        text=text,
        max_tokens_per_chunk=100,
        overlap_tokens=20,
        strategy=strategy
    )
    
    print(f"Split into {len(chunks)} chunks:")
    for i, chunk in enumerate(chunks):
        print(f"\n--- Chunk {i+1} ---")
        print(chunk[:100] + "..." if len(chunk) > 100 else chunk)
```

Run the updated script to see how different strategies handle the same text:

```bash
python simple_chunker.py
```

Congratulations! You've successfully set up and run your first Enterprise Chunker project. You've seen how it can split text using different strategies while preserving the meaning and structure of the content.

---

## 🧩 **4. Detailed Functional Breakdown**

Let's explore the main functions and features of Enterprise Chunker:

### 4.1. Adaptive Chunking

#### a. **ELI5 Explanation**
[Adaptive chunking](#adaptive-chunking) is like having a smart assistant who knows exactly how to divide up a book based on what kind of book it is. If it's a novel, it might split by chapters; if it's a technical manual, it might split by sections; if it's a cookbook, it might split by recipes.

#### b. **Real-World Example**
Imagine you're organizing a library. For fiction books, you organize by author and genre. For reference books, you organize by subject and subtopic. For magazines, you organize by date and issue. Enterprise Chunker does the same thing with text - it recognizes the type of content and applies the most appropriate splitting method.

#### c. **How to Use (Step-by-Step)**
```python
from enterprise_chunker import EnterpriseChunker

chunker = EnterpriseChunker()

# The chunker will automatically detect the format and apply the best strategy
chunks = chunker.adaptive_chunk_text(
    text=your_text,
    max_tokens_per_chunk=1000,  # Adjust based on your LLM's limits
    overlap_tokens=100          # Amount of overlap between chunks
)
```

#### d. **Impact & Importance**
Adaptive chunking dramatically improves how well LLMs understand your documents because they receive chunks that preserve natural boundaries in the text. This leads to better comprehension, more accurate responses, and fewer hallucinations or misunderstandings.

### 4.2. Format-Specific Chunking

#### a. **ELI5 Explanation**
Different types of files need different ways of splitting them. JSON files have special structures with curly braces and square brackets. Markdown files have headings and lists. Code files have functions and classes. Format-specific chunking understands these different structures and splits files in ways that respect their unique formats.

#### b. **Real-World Example**
Think about cutting different foods: you cut a pizza along the slices, a cake in wedges or squares, and a sandwich diagonally. Each food has a natural way to be divided. Similarly, Enterprise Chunker knows how to "cut" different file formats along their natural lines.

#### c. **How to Use (Step-by-Step)**
```python
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy

chunker = EnterpriseChunker()

# For a JSON file:
json_chunks = chunker.adaptive_chunk_text(
    text=json_text,
    strategy=ChunkingStrategy.STRUCTURAL  # Use structural awareness
)

# For a Markdown file:
markdown_chunks = chunker.adaptive_chunk_text(
    text=markdown_text,
    strategy=ChunkingStrategy.STRUCTURAL  # Same strategy, different internal handling
)

# For React or Vue components:
react_chunks = chunker.adaptive_chunk_text(
    text=react_component_code,
    strategy=ChunkingStrategy.STRUCTURAL
)
```

#### d. **Impact & Importance**
Format-specific chunking ensures that specialized file formats maintain their structure and meaning after chunking. This is crucial for code files, structured data (JSON/XML), and formatted documents (Markdown), where breaking at arbitrary points could destroy the document's semantics.

### 4.3. Memory-Efficient Processing

#### a. **ELI5 Explanation**
When processing really big documents, a computer can run out of memory - just like you might run out of space on a desk when working with too many papers. Memory-efficient processing is like having a clever filing system that only keeps a small portion of the document on your desk at any time.

#### b. **Real-World Example**
Imagine you're digitizing a massive book library. Instead of scanning the entire library at once (which would require enormous storage), you scan one shelf at a time, process those books, and then move to the next shelf. Enterprise Chunker uses similar techniques to handle gigabytes of text without using gigabytes of memory.

#### c. **How to Use (Step-by-Step)**
```python
from enterprise_chunker import EnterpriseChunker
from enterprise_chunker.utils.memory_optimization import MemoryManager

# Create a memory manager with low memory mode enabled
memory_manager = MemoryManager(low_memory_mode=True)

# Use it with the chunking process
with memory_manager.memory_efficient_context():
    chunker = EnterpriseChunker()
    
    # Process a large file in streaming mode
    with open('very_large_file.txt', 'r') as file:
        # Process the file as a stream instead of loading it all at once
        chunks = list(chunker.chunk_stream(file))
```

#### d. **Impact & Importance**
Memory-efficient processing allows you to work with extremely large documents (gigabytes in size) even on machines with limited memory. Without these optimizations, your program might crash when trying to process large corporate documents, research papers, or code repositories.

### 4.4. Parallel Processing

#### a. **ELI5 Explanation**
[Parallel processing](#parallel-processing) is like having multiple helpers work on different parts of a task simultaneously. Instead of one person reading through a whole book, ten people each read a chapter at the same time, making the entire process much faster.

#### b. **Real-World Example**
Consider an assembly line in a factory where different workers perform different tasks simultaneously, rather than having one worker complete the entire product. This parallel approach significantly speeds up production. The same concept applies to chunking large documents.

#### c. **How to Use (Step-by-Step)**
```python
from enterprise_chunker.orchestrator import create_auto_chunker
from enterprise_chunker.config import ChunkingOptions

# Create chunking options
options = ChunkingOptions(
    max_tokens_per_chunk=1000,
    overlap_tokens=100
)

# Create a smart parallel chunker
parallel_chunker = create_auto_chunker(
    options=options,
    mode="performance"  # Optimize for speed
)

# Define a chunking function
def my_chunker_func(text):
    # Your chunking logic here
    return [text]  # Simple example

# Process text with parallel execution
chunks = parallel_chunker.chunk(large_text, my_chunker_func)
```

#### d. **Impact & Importance**
Parallel processing can dramatically reduce the time needed to chunk large documents. What might take minutes sequentially could be completed in seconds with parallel processing, making it essential for applications with real-time requirements or batch processing of large document collections.

### 4.5. Token Estimation

#### a. **ELI5 Explanation**
LLMs process text in units called "[tokens](#token)" (roughly equivalent to parts of words). Token estimation is like predicting how many bites it will take to eat a sandwich before you start eating. This helps you plan better and avoid trying to fit too much text into one chunk.

#### b. **Real-World Example**
When packing for a trip, you estimate how many clothes will fit in your suitcase before actually packing. Similarly, token estimation predicts how many tokens a piece of text will use, helping you make better chunking decisions without trial and error.

#### c. **How to Use (Step-by-Step)**
```python
from enterprise_chunker.utils.token_estimation import estimate_tokens
from enterprise_chunker.models.enums import TokenEstimationStrategy

# Estimate tokens with default balanced strategy
token_count = estimate_tokens("Your text goes here")

# Choose a specific estimation strategy
precise_count = estimate_tokens(
    "Your text goes here",
    strategy=TokenEstimationStrategy.PRECISION  # More accurate but slower
)

performance_count = estimate_tokens(
    "Your text goes here",
    strategy=TokenEstimationStrategy.PERFORMANCE  # Faster but less accurate
)
```

#### d. **Impact & Importance**
Accurate token estimation ensures you create chunks that fit within an LLM's [context window](#context-window) without wasting space. This optimizes both cost (as many LLM APIs charge per token) and performance (by maximizing the use of available context).

---

## 📈 **5. Gradual Complexity & Advanced Usage**

As you become more comfortable with the basics of Enterprise Chunker, you can explore more advanced features to handle complex scenarios and optimize performance.

### Beginner Level: Basic Configuration

At the most basic level, you can configure the chunker with simple settings:

```python
from enterprise_chunker import EnterpriseChunker

# Create a chunker with default settings
chunker = EnterpriseChunker()

# Simple chunking with basic parameters
chunks = chunker.adaptive_chunk_text(
    text=your_document,
    max_tokens_per_chunk=1000,
    overlap_tokens=100
)
```

This approach works well for most standard documents and provides a good balance of simplicity and effectiveness.

### Intermediate Level: Specialized Strategies and Fluent API

As you gain familiarity, you can leverage specialized chunking strategies and the fluent API for more control:

```python
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy

# Create a base chunker
chunker = EnterpriseChunker()

# Use the fluent API for more readable configuration
result = chunker \
    .with_max_tokens(2000) \
    .with_overlap(150) \
    .with_strategy(ChunkingStrategy.SEMANTIC) \
    .chunk(your_document)

# Use context managers for temporary configuration changes
with chunker.semantic_context(max_tokens=1500, overlap=200):
    semantic_chunks = chunker.chunk(document1)

with chunker.structural_context(max_tokens=1000, overlap=100):
    structural_chunks = chunker.chunk(document2)
```

This level of control allows you to tailor the chunking process to specific document types or requirements.

### Expert Level: Advanced Orchestration and Optimization

For power users, Enterprise Chunker offers sophisticated orchestration capabilities with automatic resource management:

```python
from enterprise_chunker.orchestrator import create_auto_chunker, DynamicConfig
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.models.enums import ChunkingStrategy

# Create a custom configuration
config = DynamicConfig({
    'processing_timeout': 120.0,
    'max_retries': 3,
    'memory_safety': True,
    'dynamic_batch_sizing': True,
    'enable_ml_segmentation': False,
})

# Create options with detailed parameters
options = ChunkingOptions(
    max_tokens_per_chunk=1500,
    overlap_tokens=200,
    chunking_strategy=ChunkingStrategy.ADAPTIVE,
    token_strategy=TokenEstimationStrategy.PRECISION,
    preserve_structure=True,
    safety_margin=0.95,
    stream_buffer_size=200000,
)

# Create an advanced auto-tuning chunker
smart_chunker = create_auto_chunker(
    options=options,
    mode="balanced",  # Other options: "performance", "memory-safe"
    memory_safety=True,
    timeout=300.0,
    config=config,
    enable_metrics_server=True,
    metrics_port=8000
)

# Define your chunking function
def advanced_chunking(text):
    # Custom logic here
    return [text]  # Simplified example

# Process with priority-based QoS
high_priority_chunks = smart_chunker.chunk_with_priority(
    critical_document, 
    advanced_chunking,
    priority="high"
)

# Get performance metrics
metrics = smart_chunker.get_metrics()
print(f"Processed {metrics['total_chunks_processed']} chunks")
print(f"Average throughput: {metrics['avg_throughput']:.2f} chunks/second")
```

At this level, you gain access to:
- Dynamic resource scaling
- Performance monitoring and metrics
- Quality of Service (QoS) prioritization
- Memory safety protections
- [Circuit breaker](#circuit-breaker) patterns for fault tolerance
- Parallel processing optimizations

### Expert Feature: Format-Specific Custom Options

For specialized document formats, you can leverage format-specific strategies with custom configurations:

```python
# For JSON documents with deep nesting
json_chunks = chunker.adaptive_chunk_text(
    text=complex_json,
    strategy=ChunkingStrategy.STRUCTURAL,
    max_tokens_per_chunk=2000,  # Larger chunks for preserving structure
    overlap_tokens=50  # Less overlap needed for structured data
)

# For Markdown documents with complex formatting
markdown_chunks = chunker.adaptive_chunk_text(
    text=markdown_doc,
    strategy=ChunkingStrategy.STRUCTURAL,
    max_tokens_per_chunk=1000,
    overlap_tokens=200  # More overlap to maintain context
)

# For code files (like React components)
code_chunks = chunker.adaptive_chunk_text(
    text=react_code,
    strategy=ChunkingStrategy.STRUCTURAL,
    max_tokens_per_chunk=1500
)
```

These specialized configurations ensure optimal handling of different document types while respecting their unique structures.

---

## 📘 **6. Step-by-Step Practical Examples & Tutorials**

### Example 1: Processing a Large Markdown Article (Beginner)

Let's walk through processing a large Markdown article, like a blog post or documentation.

```python
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy

# Step 1: Initialize the chunker
chunker = EnterpriseChunker()

# Step 2: Load the Markdown file
with open('large_article.md', 'r', encoding='utf-8') as file:
    markdown_text = file.read()

# Step 3: Configure chunking settings
max_tokens = 1000  # Adjust based on your LLM's limits
overlap = 100      # Amount of overlap between chunks

# Step 4: Process the document
chunks = chunker.adaptive_chunk_text(
    text=markdown_text,
    max_tokens_per_chunk=max_tokens,
    overlap_tokens=overlap,
    strategy=ChunkingStrategy.STRUCTURAL  # Good for Markdown
)

# Step 5: Use the chunks (here we'll just print info about them)
print(f"Split document into {len(chunks)} chunks")
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {len(chunk)} characters")
    # Here you would typically send the chunk to an LLM, store it, etc.
```

**Input:** A Markdown article with headings, lists, code blocks, etc.
**Output:** A series of chunks that respect the Markdown structure, with each chunk maintaining proper context.

### Example 2: Creating a Document Q&A System (Intermediate)

This example shows how to build a simple question-answering system using Enterprise Chunker and an LLM API.

```python
from enterprise_chunker import EnterpriseChunker
import requests  # For API calls to your LLM provider
import json

# Step 1: Initialize the chunker
chunker = EnterpriseChunker()

# Step 2: Load and chunk a document
with open('technical_manual.txt', 'r', encoding='utf-8') as file:
    document = file.read()

chunks = chunker.adaptive_chunk_text(
    text=document,
    max_tokens_per_chunk=1500,
    overlap_tokens=150
)

# Step 3: Define a function to query the LLM with a chunk
def query_llm(chunk, question):
    # This is a placeholder - replace with your actual LLM API call
    api_url = "https://your-llm-api-endpoint.com/generate"
    
    payload = {
        "model": "your-model-name",
        "prompt": f"Based on the following text, answer the question.\n\nText: {chunk}\n\nQuestion: {question}\n\nAnswer:",
        "max_tokens": 200
    }
    
    response = requests.post(api_url, json=payload)
    return response.json()["text"]

# Step 4: Create a function to answer questions about the document
def answer_question(document_chunks, question):
    all_answers = []
    
    # Query each chunk and collect answers
    for i, chunk in enumerate(document_chunks):
        print(f"Querying chunk {i+1}/{len(document_chunks)}...")
        answer = query_llm(chunk, question)
        all_answers.append({"chunk": i+1, "answer": answer})
    
    # Find the best answer (or combine them - this is simplified)
    # In a real system, you might use embeddings to find relevant chunks first
    best_answer = max(all_answers, key=lambda x: len(x["answer"]))
    
    return best_answer["answer"]

# Step 5: Answer questions
question = "How do I troubleshoot network connectivity issues?"
answer = answer_question(chunks, question)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

**Input:** A technical manual or documentation file and a user question
**Output:** An answer to the question based on the content of the document

### Example 3: Advanced Streaming Processing of Gigabyte-Scale Logs (Expert)

This example demonstrates how to process extremely large log files in a memory-efficient way while extracting structured information.

```python
from enterprise_chunker.orchestrator import create_auto_chunker
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.models.enums import ChunkingStrategy
import json
import re
from datetime import datetime
import concurrent.futures

# Step 1: Define a pattern for log entry extraction
log_pattern = re.compile(r'(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2})\s+(\w+)\s+\[([^\]]+)\]\s+(.*)')

# Step 2: Configure chunking options for log processing
options = ChunkingOptions(
    max_tokens_per_chunk=2000,
    overlap_tokens=100,
    chunking_strategy=ChunkingStrategy.STRUCTURAL
)

# Step 3: Create an optimized chunker for large files
chunker = create_auto_chunker(
    options=options,
    mode="memory-safe",  # Optimize for memory efficiency
    memory_safety=True
)

# Step 4: Define processing function for log chunks
def process_log_chunk(chunk):
    extracted_logs = []
    for line in chunk.splitlines():
        match = log_pattern.match(line)
        if match:
            timestamp, level, component, message = match.groups()
            extracted_logs.append({
                "timestamp": timestamp,
                "level": level,
                "component": component,
                "message": message
            })
    return extracted_logs

# Step 5: Process large log file with streaming and parallel execution
def analyze_logs(log_filepath, output_filepath):
    start_time = datetime.now()
    print(f"Starting log analysis at {start_time}")
    
    all_logs = []
    error_count = 0
    warning_count = 0
    
    # Process the file in streaming mode
    with open(log_filepath, 'r') as log_file:
        # Use the chunker's streaming interface
        chunk_stream = chunker.stream_chunks(log_file, process_log_chunk)
        
        # Process each batch of extracted logs
        for log_batch in chunk_stream:
            all_logs.extend(log_batch)
            
            # Count errors and warnings
            for log in log_batch:
                if log["level"] == "ERROR":
                    error_count += 1
                elif log["level"] == "WARNING":
                    warning_count += 1
    
    # Generate report
    report = {
        "total_logs": len(all_logs),
        "error_count": error_count,
        "warning_count": warning_count,
        "processing_time": str(datetime.now() - start_time),
        "sample_errors": [log for log in all_logs if log["level"] == "ERROR"][:10]
    }
    
    # Save to output file
    with open(output_filepath, 'w') as output_file:
        json.dump(report, output_file, indent=2)
    
    print(f"Analysis complete. Processed {len(all_logs)} log entries.")
    print(f"Found {error_count} errors and {warning_count} warnings.")
    print(f"Total processing time: {datetime.now() - start_time}")

# Step 6: Execute the analysis
analyze_logs("massive_server_logs.txt", "log_analysis_report.json")
```

**Input:** A gigabyte-scale log file with millions of entries
**Output:** A structured JSON report summarizing the log data without exhausting system memory

---

## 🚩 **7. Red Flags, Common Issues & Troubleshooting**

### Out of Memory Errors

**Symptoms:**
- Program crashes with `MemoryError`
- System becomes unresponsive when processing large files

**Causes:**
- Loading entire large documents into memory
- Not using streaming interfaces for large files
- Insufficient memory safety settings

**Solutions:**
1. Enable memory safety mode:
   ```python
   from enterprise_chunker.utils.memory_optimization import MemoryManager
   
   memory_manager = MemoryManager(low_memory_mode=True)
   with memory_manager.memory_efficient_context():
       # Your chunking code here
   ```

2. Use streaming interfaces for large files:
   ```python
   with open('large_file.txt', 'r') as file:
       for chunk in chunker.chunk_stream(file):
           # Process chunk
   ```

3. Create memory-safe chunker configuration:
   ```python
   from enterprise_chunker.orchestrator import create_auto_chunker
   
   chunker = create_auto_chunker(
       options=options,
       mode="memory-safe",
       memory_safety=True
   )
   ```

### Chunking Produces Inappropriate Boundaries

**Symptoms:**
- Code fragments are broken mid-function
- JSON data is split in the middle of objects or arrays
- Markdown headings are separated from their content

**Causes:**
- Using the wrong chunking strategy for the content type
- Not enabling format detection
- Insufficient overlap between chunks

**Solutions:**
1. Use the appropriate strategy for your content type:
   ```python
   # For code files:
   code_chunks = chunker.adaptive_chunk_text(
       text=code,
       strategy=ChunkingStrategy.STRUCTURAL
   )
   
   # For markdown:
   markdown_chunks = chunker.adaptive_chunk_text(
       text=markdown,
       strategy=ChunkingStrategy.STRUCTURAL
   )
   
   # For general text:
   text_chunks = chunker.adaptive_chunk_text(
       text=text,
       strategy=ChunkingStrategy.SEMANTIC
   )
   ```

2. Ensure format detection is enabled:
   ```python
   from enterprise_chunker.config import ChunkingOptions
   
   options = ChunkingOptions(
       enable_format_detection=True,
       # Other options...
   )
   ```

3. Increase overlap for better context preservation:
   ```python
   chunks = chunker.adaptive_chunk_text(
       text=text,
       overlap_tokens=300  # Increase from default
   )
   ```

### Performance Issues

**Symptoms:**
- Chunking takes too long to complete
- System CPU usage spikes during chunking
- Batch processing is slow

**Causes:**
- Not utilizing parallel processing
- Using high-precision settings for large files
- Inefficient batch sizing

**Solutions:**
1. Use parallel processing for better performance:
   ```python
   from enterprise_chunker.orchestrator import create_auto_chunker
   
   chunker = create_auto_chunker(
       options=options,
       mode="performance"  # Optimize for speed
   )
   ```

2. Adjust token estimation strategy for large files:
   ```python
   from enterprise_chunker.models.enums import TokenEstimationStrategy
   
   options = ChunkingOptions(
       token_strategy=TokenEstimationStrategy.PERFORMANCE
   )
   ```

3. Enable adaptive batch sizing:
   ```python
   chunker = create_auto_chunker(
       options=options,
       adaptive_batch_sizing=True
   )
   ```

### Import Errors

**Symptoms:**
- `ImportError` or `ModuleNotFoundError` when importing
- Missing functionality despite installing the package

**Causes:**
- Incomplete installation
- Missing optional dependencies
- Python environment issues

**Solutions:**
1. Install with all dependencies:
   ```bash
   pip install enterprise-chunker[all]
   ```

2. Check Python environment:
   ```bash
   # Verify which Python is being used
   python --version
   which python  # on Unix/Mac
   where python  # on Windows
   
   # Verify package installation
   pip list | grep enterprise-chunker
   ```

3. Install specific missing dependencies:
   ```bash
   pip install psutil numpy prometheus_client regex
   ```

### Chunking Result Differences Across Runs

**Symptoms:**
- Different chunk boundaries between runs on the same document
- Inconsistent chunk counts

**Causes:**
- Using adaptive strategies with dynamic behavior
- System resource fluctuations affecting decisions
- Random sampling in some algorithms

**Solutions:**
1. Use a deterministic strategy for consistent results:
   ```python
   # Fixed-size strategy is more deterministic
   chunks = chunker.adaptive_chunk_text(
       text=text,
       strategy=ChunkingStrategy.FIXED_SIZE
   )
   ```

2. Set a fixed operation ID for reproducibility:
   ```python
   from enterprise_chunker.strategies.base import BaseChunkingStrategy
   
   strategy = BaseChunkingStrategy(ContentFormat.TEXT)
   strategy.set_operation_id("fixed-id-for-reproducibility")
   ```

3. Control system resource variables:
   ```python
   chunker = create_auto_chunker(
       options=options,
       worker_count_override=4  # Fix worker count
   )
   ```

---

## ✅ **8. Best Practices & Additional Tips**

### Optimizing Chunking Settings

1. **Match Token Limits to Your LLM:**
   ```python
   # For OpenAI GPT-4 (~8K tokens)
   chunks = chunker.adaptive_chunk_text(
       text=text,
       max_tokens_per_chunk=7500,  # Leave room for response
       overlap_tokens=200
   )
   
   # For Claude (~100K tokens)
   chunks = chunker.adaptive_chunk_text(
       text=text,
       max_tokens_per_chunk=90000,  # Leave room for response
       overlap_tokens=1000
   )
   ```

2. **Adjust Overlap Based on Document Type:**
   - Technical/reference content: 10-15% overlap
   - Narrative/prose: 15-20% overlap
   - Code/structured data: 5-10% overlap

3. **Set Appropriate Safety Margins:**
   ```python
   options = ChunkingOptions(
       safety_margin=0.9,  # 90% of max token limit
       reserved_tokens=1000  # Keep 1000 tokens for responses
   )
   ```

### Memory and Performance Optimizations

1. **Stream Large Files:**
   Always use streaming for files over 10MB:
   ```python
   with open('large_file.txt', 'r') as file:
       for chunk in chunker.chunk_stream(file):
           process_chunk(chunk)
   ```

2. **Monitor Resource Usage:**
   ```python
   from enterprise_chunker.utils.memory_optimization import MemoryMonitor
   
   monitor = MemoryMonitor()
   with monitor.monitor_operation("process_document"):
       # Your chunking code
       pass
   ```

3. **Use Process-Based Parallelism for CPU-Bound Tasks:**
   ```python
   from enterprise_chunker.utils.parallel_processing import ParallelChunker
   
   parallel_chunker = ParallelChunker(
       options=options,
       max_workers=os.cpu_count(),
       use_processes=True  # Use processes instead of threads
   )
   ```

### Integration Patterns

1. **Pre-chunk Documents for Retrieval Systems:**
   Pre-process and store chunks rather than chunking on-the-fly for better performance in retrieval systems.

2. **Maintain Chunk Metadata:**
   ```python
   from enterprise_chunker.models.chunk_metadata import ChunkMetadata
   
   # Store metadata with chunks for better context
   result = chunker.chunk(text)
   for i, chunk in enumerate(result.chunks):
       metadata = result.chunk_metadata[i]
       store_in_database(chunk, metadata)
   ```

3. **Use Format Detection for Mixed Document Collections:**
   ```python
   from enterprise_chunker.utils.format_detection import detect_content_format
   
   # Detect format before processing
   format_type = detect_content_format(document)
   print(f"Detected format: {format_type}")
   ```

### Tools That Work Well with Enterprise Chunker

1. **Vector Databases** (Pinecone, Weaviate, Milvus, etc.) for storing and retrieving chunks based on semantic similarity

2. **Document Processing Pipelines** like Unstructured.io or Apache Tika for extracting text from various formats before chunking

3. **LLM Frameworks** like LangChain or LlamaIndex that can use the chunks for retrieval-augmented generation

4. **Monitoring Tools** like Prometheus + Grafana for tracking chunking performance metrics

---

## 📌 **9. Glossary & Definitions (Jargon-Buster)**

| Term | Definition |
|------|------------|
| <a id="adaptive-chunking"></a>**Adaptive Chunking** | A chunking strategy that automatically selects the best method based on content characteristics. |
| <a id="boundary-detection"></a>**Boundary Detection** | Finding natural points in text where it can be divided with minimal loss of meaning. |
| <a id="chunk"></a>**Chunk** | A segment of text created by dividing a larger document, optimized for processing by an LLM. |
| <a id="chunking"></a>**Chunking** | The process of dividing large text or documents into smaller, manageable pieces that can be processed by LLMs or other systems. |
| <a id="circuit-breaker"></a>**Circuit Breaker** | A pattern that prevents cascading failures by stopping operations when errors exceed a threshold. |
| <a id="context-window"></a>**Context Window** | The maximum amount of text (measured in tokens) that an LLM can process at once. |
| <a id="fixed-size-chunking"></a>**Fixed-Size Chunking** | A strategy that creates chunks of approximately equal size. |
| <a id="format-detection"></a>**Format Detection** | Automatically identifying the format of a document (JSON, Markdown, code, etc.). |
| <a id="large-language-model-llm"></a>**Large Language Model (LLM)** | Advanced AI models like GPT-4 or Claude that process and generate human-like text. |
| <a id="memory-safety"></a>**Memory Safety** | Features that prevent out-of-memory errors when processing large documents. |
| <a id="overlap"></a>**Overlap** | Text that is repeated between adjacent chunks to maintain context. |
| <a id="parallel-processing"></a>**Parallel Processing** | Utilizing multiple CPU cores or threads to process different segments of text simultaneously for faster chunking. |
| <a id="qos"></a>**QoS** | Quality of Service - controlling how resources are allocated based on request priority. |
| <a id="retrieval-augmented-generation-rag"></a>**Retrieval-Augmented Generation (RAG)** | An approach where chunks of text are retrieved from a knowledge base and fed to an LLM. |
| <a id="semantic-chunking"></a>**Semantic Chunking** | A chunking strategy that preserves the meaning and natural boundaries in text. |
| <a id="streaming"></a>**Streaming** | Processing data continuously in small pieces rather than all at once. |
| <a id="structural-chunking"></a>**Structural Chunking** | A chunking strategy that respects the structure of formatted content like JSON, Markdown, or code. |
| <a id="token"></a>**Token** | The basic unit of text that LLMs process. In English, a token is typically 4-5 characters or about 3/4 of a word. |
| <a id="token-estimation"></a>**Token Estimation** | Predicting how many tokens a text will use in an LLM without actually sending it to the LLM. |

---

## 🚨 **10. Limitations & Known Issues**

### Current Limitations

1. **Non-Text Format Support**
   - Enterprise Chunker focuses on text-based formats and does not directly support binary formats (images, audio, video)
   - **Workaround:** Convert binary formats to text (e.g., transcribe audio, use OCR for images) before chunking

2. **Language Limitations**
   - Token estimation is optimized for English and may be less accurate for other languages, particularly non-Latin scripts
   - **Workaround:** Use a higher safety margin for non-English content (0.8 instead of 0.9)

3. **Very Specialized Format Support**
   - While the library handles common formats (JSON, Markdown, code), some niche formats may not have specialized handlers
   - **Workaround:** Use the semantic or fixed-size strategy for unsupported formats

4. **Performance on Very Low Memory Systems**
   - Even with memory optimizations, processing gigabyte-scale documents requires at least 1GB of RAM
   - **Workaround:** Split extremely large files before processing or use external preprocessing tools

### Known Issues

1. **Inconsistent Boundaries in Mixed Content**
   - When documents contain mixed formats (e.g., Markdown with embedded JSON), boundary detection may be suboptimal
   - **Workaround:** Pre-process mixed documents to separate different format sections

2. **Token Estimation Discrepancies**
   - Token estimates may differ from actual LLM tokenization by 5-10%, especially for specialized tokens
   - **Workaround:** Use appropriate safety margins in your configuration

3. **Thread Safety in Some Utility Classes**
   - Some utility classes may not be fully thread-safe in all scenarios
   - **Workaround:** Use separate instances for concurrent operations

4. **Memory Leaks in Long-Running Processes**
   - Some rare memory leaks can occur in very long-running processes (days or weeks)
   - **Workaround:** Restart services periodically or implement watchdog monitoring

### Planned Improvements

1. **Enhanced Language Support**
   - Improved token estimation for non-Latin scripts and specialized languages

2. **Additional Format Handlers**
   - Support for more specialized formats like CSV, LaTeX, and programming languages

3. **GPU Acceleration**
   - Optional GPU acceleration for computationally intensive operations

4. **LLM-Guided Chunking**
   - Using LLMs themselves to determine optimal chunk boundaries for complex documents

---

## 🌐 **11. Integration & Ecosystem Context**

Enterprise Chunker is designed to work seamlessly with other components in the AI/LLM ecosystem. Here are some common integration patterns:

### Integration with LLM Frameworks

#### LangChain Integration

```python
from langchain.text_splitter import EnterpriseChunkerSplitter
from langchain.document_loaders import TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# Load document
loader = TextLoader("my-document.txt")
documents = loader.load()

# Use Enterprise Chunker as a text splitter
text_splitter = EnterpriseChunkerSplitter(
    chunk_size=1000,
    chunk_overlap=100,
    strategy="semantic"
)

# Split documents
chunks = text_splitter.split_documents(documents)

# Create vector store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# Query the vector store
query = "What is the main topic of the document?"
docs = vectorstore.similarity_search(query)
```

#### LlamaIndex Integration

```python
from llama_index.node_parser import EnterpriseChunkerNodeParser
from llama_index import VectorStoreIndex, SimpleDirectoryReader

# Load documents
documents = SimpleDirectoryReader("data").load_data()

# Use Enterprise Chunker as a node parser
node_parser = EnterpriseChunkerNodeParser(
    chunk_size=1024,
    chunk_overlap=100,
    strategy="adaptive"
)

# Create index
index = VectorStoreIndex.from_documents(
    documents, node_parser=node_parser
)

# Query the index
query_engine = index.as_query_engine()
response = query_engine.query("What does the document say about climate change?")
```

### Integration with Vector Databases

#### Pinecone Example

```python
import pinecone
from enterprise_chunker import EnterpriseChunker

# Initialize services
pinecone.init(api_key="your-api-key", environment="your-environment")
index = pinecone.Index("your-index-name")
chunker = EnterpriseChunker()

# Process document
with open("document.txt", "r") as file:
    text = file.read()

chunks = chunker.adaptive_chunk_text(text)

# Create embeddings (example using OpenAI, but you can use any embedding provider)
import openai
openai.api_key = "your-openai-key"

def get_embedding(text):
    response = openai.Embedding.create(
        input=text,
        model="text-embedding-ada-002"
    )
    return response["data"][0]["embedding"]

# Store chunks in Pinecone
for i, chunk in enumerate(chunks):
    embedding = get_embedding(chunk)
    
    # Store in Pinecone
    index.upsert(
        vectors=[(f"doc1-chunk-{i}", embedding, {"text": chunk})]
    )
```

### Integration with Document Processing Pipelines

#### Unstructured.io Example

```python
from unstructured.partition.pdf import partition_pdf
from enterprise_chunker import EnterpriseChunker

# Extract text from PDF using Unstructured
elements = partition_pdf("document.pdf")
text = "\n\n".join([el.text for el in elements])

# Process with Enterprise Chunker
chunker = EnterpriseChunker()
chunks = chunker.adaptive_chunk_text(text)

# Further processing with chunks...
```

### Integration with Custom AI Applications

```python
from enterprise_chunker import EnterpriseChunker
from enterprise_chunker.models.enums import ChunkingStrategy
import asyncio
import aiohttp

# Initialize chunker
chunker = EnterpriseChunker()

# Process document
with open("large_document.txt", "r") as file:
    document = file.read()

chunks = chunker.adaptive_chunk_text(
    document,
    strategy=ChunkingStrategy.SEMANTIC
)

# Async processing with LLM API
async def process_chunk(chunk, session):
    async with session.post(
        "https://api.openai.com/v1/chat/completions",
        headers={"Authorization": f"Bearer {openai_api_key}"},
        json={
            "model": "gpt-4",
            "messages": [
                {"role": "system", "content": "Summarize the following text."},
                {"role": "user", "content": chunk}
            ]
        }
    ) as response:
        result = await response.json()
        return result["choices"][0]["message"]["content"]

async def process_all_chunks(chunks):
    async with aiohttp.ClientSession() as session:
        tasks = [process_chunk(chunk, session) for chunk in chunks]
        return await asyncio.gather(*tasks)

# Run async processing
summaries = asyncio.run(process_all_chunks(chunks))

# Combine summaries or further process them
final_summary = "\n\n".join(summaries)
```

This demonstrates how Enterprise Chunker can fit into various parts of an AI application stack, from document preprocessing to integration with LLMs and vector databases.

---

## 🔐 **12. Security, Privacy & Data Handling Guidelines**

When working with Enterprise Chunker and processing documents, especially in enterprise environments, follow these security and privacy best practices:

### Data Handling Best Practices

1. **Sensitive Data Identification**
   - Before chunking, scan documents for sensitive information (PII, credentials, etc.)
   - Consider using a data loss prevention (DLP) tool in your pipeline

2. **Data Minimization**
   - Only process the portions of documents needed for your specific use case
   - Remove or redact sensitive sections before chunking

3. **Secure Storage of Chunks**
   - Encrypt chunks at rest when storing them in databases or file systems
   - Implement appropriate access controls for your chunk storage

4. **Transmission Security**
   - Use secure channels (HTTPS/TLS) when transmitting chunks to LLM APIs
   - Consider VPNs or private endpoints for enterprise deployments

### Enterprise Chunker Security Considerations

1. **Memory Management**
   - Use the memory safety features to prevent DoS-like issues with very large files
   ```python
   from enterprise_chunker.orchestrator import create_auto_chunker
   
   chunker = create_auto_chunker(
       options=options,
       memory_safety=True
   )
   ```

2. **Input Validation**
   - Validate and sanitize input documents before processing
   - Implement size limits for inputs based on your system capabilities

3. **Resource Limitations**
   - Set appropriate timeouts to prevent processing hangs
   ```python
   chunker = create_auto_chunker(
       options=options,
       timeout=60.0  # 60-second timeout
   )
   ```

4. **Monitoring & Logging**
   - Enable metrics collection to detect unusual patterns
   - Implement logging but be careful not to log sensitive document content
   ```python
   # Enable metrics server
   chunker = create_auto_chunker(
       options=options,
       enable_metrics_server=True,
       metrics_port=8000
   )
   ```

### Compliance Considerations

1. **GDPR Compliance**
   - Ensure you have proper legal basis for processing personal data
   - Implement data retention policies for stored chunks
   - Be prepared to fulfill data subject access and deletion requests

2. **HIPAA Compliance (for healthcare data)**
   - Ensure any health information is appropriately de-identified before processing
   - Implement additional encryption for health-related chunks
   - Maintain comprehensive audit logs for all processing

3. **Financial Regulations (for financial documents)**
   - Implement stricter access controls for financial data chunks
   - Consider additional validation steps for financial content

4. **Data Residency**
   - Process documents in the appropriate geographic region based on regulatory requirements
   - Consider using region-specific LLM APIs for processing chunks

### Example: Secure Document Processing Pipeline

```python
from enterprise_chunker import EnterpriseChunker
import logging
import hashlib

# Configure secure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("processing.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("secure-processor")

# Initialize chunker
chunker = EnterpriseChunker()

# Load document securely
def secure_load_document(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
            
        # Log document hash for audit purposes (not the content itself)
        doc_hash = hashlib.sha256(content.encode()).hexdigest()
        logger.info(f"Processing document: {file_path} (SHA256: {doc_hash})")
        
        return content
    except Exception as e:
        logger.error(f"Error loading document {file_path}: {str(e)}")
        raise

# Process document with privacy considerations
def process_with_privacy(content, max_tokens=1000):
    # Validate input size
    if len(content) > 10 * 1024 * 1024:  # 10MB limit
        logger.warning("Document exceeds size limit, truncating")
        content = content[:10 * 1024 * 1024]
    
    # Process the document
    try:
        chunks = chunker.adaptive_chunk_text(
            text=content,
            max_tokens_per_chunk=max_tokens
        )
        
        logger.info(f"Document successfully chunked into {len(chunks)} parts")
        return chunks
    except Exception as e:
        logger.error(f"Error during chunking: {str(e)}")
        raise

# Example usage
try:
    document = secure_load_document("confidential_report.txt")
    chunks = process_with_privacy(document)
    
    # Further secure processing...
    
except Exception as e:
    logger.critical(f"Processing failed: {str(e)}")
```

By following these security and privacy guidelines, you can ensure that your document processing pipeline using Enterprise Chunker meets enterprise security requirements and compliance standards.

---

## 🖼️ **13. Visual Aids & Supporting Diagrams**

### Enterprise Chunker System Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                     Enterprise Chunker System                    │
└─────────────────────────────────────────────────────────────────┘
                               │
                 ┌─────────────┴─────────────┐
                 │                           │
┌────────────────▼─────────────┐ ┌───────────▼────────────┐
│      Document Processing      │ │   Chunking Strategies  │
│                              │ │                        │
│ ┌──────────┐   ┌───────────┐ │ │ ┌────────┐ ┌─────────┐ │
│ │ Document │──▶│  Format   │ │ │ │Semantic│ │Structural│ │
│ │  Input   │   │ Detection │─┼─┼─▶        │ │         │ │
│ └──────────┘   └───────────┘ │ │ └────────┘ └─────────┘ │
│                              │ │ ┌────────┐ ┌─────────┐ │
│ ┌──────────┐   ┌───────────┐ │ │ │Fixed-  │ │Sentence │ │
│ │ Streaming│◀──│  Memory   │ │ │ │Size    │ │         │ │
│ │ Buffer   │   │ Management│ │ │ └────────┘ └─────────┘ │
│ └──────────┘   └───────────┘ │ │      ┌─────────┐      │
└──────────────────────────────┘ │      │Adaptive │      │
                                 │      │         │      │
┌──────────────────────────────┐ │      └─────────┘      │
│    Format-Specific Chunkers   │ └────────────────────────┘
│                              │
│ ┌─────────┐  ┌─────────────┐ │ ┌────────────────────────┐
│ │  JSON   │  │  Markdown   │ │ │   Performance Layer     │
│ └─────────┘  └─────────────┘ │ │                        │
│ ┌─────────┐  ┌─────────────┐ │ │ ┌────────┐ ┌─────────┐ │
│ │React/Vue│  │  Smalltalk  │ │ │ │Parallel│ │Token    │ │
│ └─────────┘  └─────────────┘ │ │ │Process │ │Estimator│ │
└──────────────────────────────┘ │ └────────┘ └─────────┘ │
                                 │ ┌────────┐ ┌─────────┐ │
┌──────────────────────────────┐ │ │Circuit │ │ Memory  │ │
│     Resource Management      │ │ │Breaker │ │ Monitor │ │
│                              │ │ └────────┘ └─────────┘ │
│ ┌──────────┐  ┌────────────┐ │ └────────────────────────┘
│ │ Dynamic  │  │ Auto-tuning│ │
│ │ Config   │  │            │ │ ┌────────────────────────┐
│ └──────────┘  └────────────┘ │ │      Output Layer      │
│ ┌──────────┐  ┌────────────┐ │ │                        │
│ │ Metrics  │  │ Health     │ │ │ ┌────────┐ ┌─────────┐ │
│ │ Server   │  │ Monitoring │ │ │ │Chunks  │ │Metadata │ │
│ └──────────┘  └────────────┘ │ │ │        │ │         │ │
└──────────────────────────────┘ │ └────────┘ └─────────┘ │
                                 └────────────────────────┘
```

### Chunking Process Workflow

```
┌─────────────┐     ┌─────────────┐     ┌──────────────────┐
│             │     │             │     │                  │
│  Document   │────▶│  Format     │────▶│ Select Chunking  │
│  Input      │     │  Detection  │     │ Strategy         │
│             │     │             │     │                  │
└─────────────┘     └─────────────┘     └──────────────────┘
                                                 │
                                                 ▼
┌─────────────┐     ┌─────────────┐     ┌──────────────────┐
│             │     │             │     │                  │
│  Finalize   │◀────│  Process    │◀────│ Detect           │
│  Chunks     │     │  Chunks     │     │ Boundaries       │
│             │     │             │     │                  │
└─────────────┘     └─────────────┘     └──────────────────┘
       │
       ▼
┌─────────────┐     ┌─────────────┐
│             │     │             │
│  Add        │────▶│  Return     │
│  Metadata   │     │  Results    │
│             │     │             │
└─────────────┘     └─────────────┘
```

### Chunking Strategy Selection Logic

```
┌───────────────────────┐
│                       │
│   Document Content    │
│                       │
└───────────────────────┘
           │
           ▼
┌───────────────────────┐     No     ┌───────────────────┐
│                       │────────────▶                   │
│  Format Detection     │            │ Default to        │
│  Successful?          │            │ Semantic Chunking │
│                       │            │                   │
└───────────────────────┘            └───────────────────┘
           │ Yes
           ▼
┌───────────────────────┐
│                       │
│  Identify Content     │
│  Format               │
│                       │
└───────────────────────┘
           │
           ▼
┌───────────────────────┐     Yes    ┌───────────────────┐
│                       │────────────▶                   │
│  Is it JSON?          │            │ Use JSON Chunking │
│                       │            │                   │
└───────────────────────┘            └───────────────────┘
           │ No
           ▼
┌───────────────────────┐     Yes    ┌───────────────────┐
│                       │────────────▶                   │
│  Is it Markdown?      │            │ Use Markdown      │
│                       │            │ Chunking          │
└───────────────────────┘            └───────────────────┘
           │ No
           ▼
┌───────────────────────┐     Yes    ┌───────────────────┐
│                       │────────────▶                   │
│  Is it Code?          │            │ Use Code-specific │
│  (React/Vue/Smalltalk)│            │ Chunking          │
└───────────────────────┘            └───────────────────┘
           │ No
           ▼
┌───────────────────────┐     Yes    ┌───────────────────┐
│                       │────────────▶                   │
│  Is document size     │            │ Use Fixed-Size    │
│  very large?          │            │ Chunking          │
└───────────────────────┘            └───────────────────┘
           │ No
           ▼
┌───────────────────────┐
│                       │
│  Use Semantic         │
│  Chunking             │
│                       │
└───────────────────────┘
```

### System Resource Management

```
┌─────────────────────────────────────────────────┐
│                                                 │
│               Resource Monitoring               │
│                                                 │
└─────────────────────────────────────────────────┘
                       │
           ┌───────────┼───────────┐
           │           │           │
           ▼           ▼           ▼
┌───────────────┐ ┌─────────┐ ┌───────────┐
│               │ │         │ │           │
│ CPU Usage     │ │ Memory  │ │ System    │
│ Monitoring    │ │ Monitor │ │ Load      │
│               │ │         │ │           │
└───────────────┘ └─────────┘ └───────────┘
           │           │           │
           └───────────┼───────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────┐
│                                                 │
│               Resource Adaptation               │
│                                                 │
└─────────────────────────────────────────────────┘
                       │
           ┌───────────┼───────────┐
           │           │           │
           ▼           ▼           ▼
┌───────────────┐ ┌─────────┐ ┌───────────┐
│               │ │         │ │           │
│ Dynamic       │ │ Memory  │ │ Circuit   │
│ Worker Scaling│ │ Safety  │ │ Breaker   │
│               │ │         │ │           │
└───────────────┘ └─────────┘ └───────────┘
           │           │           │
           └───────────┼───────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────┐
│                                                 │
│               Performance Metrics               │
│                                                 │
└─────────────────────────────────────────────────┘
```

---

## 📑 **14. Summaries & Quick Reference Cheatsheet**

### Key Concepts Summary

- **Enterprise Chunker**: A library for intelligently dividing large text documents into smaller pieces for LLM processing
- **Chunking Strategies**: Different methods for splitting text based on content type and structure
- **Format Detection**: Automatic identification of document format for optimal chunking
- **Token Estimation**: Predicts token counts to ensure chunks fit within LLM context windows
- **Memory Management**: Techniques to process large documents efficiently without memory issues
- **Parallel Processing**: Utilizing multiple cores/threads to accelerate chunking operations

### Quick Reference Cheatsheet

#### Basic Usage

```python
# Import
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy

# Initialize
chunker = EnterpriseChunker()

# Process a document
chunks = chunker.adaptive_chunk_text(
    text=document,
    max_tokens_per_chunk=1000,  # Adjust based on your LLM
    overlap_tokens=100,
    strategy=ChunkingStrategy.ADAPTIVE  # Auto-select best strategy
)
```

#### Chunking Strategies

```python
from enterprise_chunker.models.enums import ChunkingStrategy

# Available strategies
ChunkingStrategy.ADAPTIVE     # Auto-select best strategy
ChunkingStrategy.SEMANTIC     # Preserve meaning and natural boundaries
ChunkingStrategy.STRUCTURAL   # Format-aware (JSON, Markdown, code)
ChunkingStrategy.FIXED_SIZE   # Equal-sized chunks with minimal boundary detection
ChunkingStrategy.SENTENCE     # Split at sentence boundaries
```

#### Streaming Interface

```python
# Streaming from file
with open('large_file.txt', 'r') as file:
    for chunk in chunker.chunk_stream(file):
        # Process chunk
        pass

# Streaming from string
for chunk in chunker.chunk_stream(large_text):
    # Process chunk
    pass
```

#### Format Detection

```python
from enterprise_chunker.utils.format_detection import detect_content_format
from enterprise_chunker.models.enums import ContentFormat

# Detect format
format_type = detect_content_format(text)

# Check format type
if format_type == ContentFormat.JSON:
    print("Processing JSON document")
elif format_type == ContentFormat.MARKDOWN:
    print("Processing Markdown document")
elif format_type == ContentFormat.CODE:
    print("Processing code file")
```

#### Token Estimation

```python
from enterprise_chunker.utils.token_estimation import estimate_tokens
from enterprise_chunker.models.enums import TokenEstimationStrategy

# Estimate tokens with default balanced strategy
token_count = estimate_tokens("Your text goes here")

# Use precision strategy (more accurate but slower)
precise_count = estimate_tokens(
    "Your text goes here",
    strategy=TokenEstimationStrategy.PRECISION
)

# Use performance strategy (faster but less accurate)
fast_count = estimate_tokens(
    "Your text goes here",
    strategy=TokenEstimationStrategy.PERFORMANCE
)
```

#### Memory Optimization

```python
from enterprise_chunker.utils.memory_optimization import MemoryManager, MemoryMonitor

# Use memory-efficient context
memory_manager = MemoryManager(low_memory_mode=True)
with memory_manager.memory_efficient_context():
    # Your memory-intensive code here
    pass

# Monitor memory usage
memory_monitor = MemoryMonitor()
status = memory_monitor.check_memory()
print(f"Memory usage: {status['current_mb']:.2f} MB")
```

#### Format-Specific Chunking

```python
# JSON chunking
json_chunks = chunker.adaptive_chunk_text(
    text=json_text,
    strategy=ChunkingStrategy.STRUCTURAL
)

# Markdown chunking
md_chunks = chunker.adaptive_chunk_text(
    text=markdown_text,
    strategy=ChunkingStrategy.STRUCTURAL
)

# React/Vue component chunking
component_chunks = chunker.adaptive_chunk_text(
    text=component_code,
    strategy=ChunkingStrategy.STRUCTURAL
)
```

#### Parallel Processing

```python
from enterprise_chunker.orchestrator import create_auto_chunker

# Create auto-tuning chunker
chunker = create_auto_chunker(
    options=options,
    mode="performance"  # Other options: "balanced", "memory-safe"
)

# Process with parallelism
chunks = chunker.chunk(text, chunker_func)
```

#### Error Handling

```python
try:
    chunks = chunker.adaptive_chunk_text(text)
except Exception as e:
    print(f"Error during chunking: {type(e).__name__}: {str(e)}")
    # Fall back to simpler strategy
    chunks = chunker.adaptive_chunk_text(
        text=text,
        strategy=ChunkingStrategy.FIXED_SIZE
    )
```

---

## 📦 **15. Deployment & Distribution Guidelines**

### Packaging Enterprise Chunker in Your Projects

#### Python Package Dependencies

When including Enterprise Chunker in your own Python packages, ensure proper dependency specification:

```python
# setup.py
setup(
    name="your-package",
    # ... other setup parameters
    install_requires=[
        "enterprise-chunker>=1.0.0",
        # Your other dependencies
    ],
    extras_require={
        "performance": ["enterprise-chunker[all]"],
    }
)
```

#### Version Pinning

For production applications, pin the specific version to ensure stability:

```python
# requirements.txt
enterprise-chunker==1.0.0
```

Or with a range for minor updates:

```python
# requirements.txt
enterprise-chunker>=1.0.0,<1.1.0
```

### Deployment Environments

#### Local Development

For local development, a simple installation is sufficient:

```bash
pip install enterprise-chunker[dev]
```

#### Cloud Deployment

When deploying to cloud environments, consider the following:

**AWS Lambda:**
- Include Enterprise Chunker in your Lambda layer
- Set appropriate memory allocation (at least 256MB, 1GB+ recommended)
- Configure timeout settings based on your document sizes

**Docker Containers:**
- Include Enterprise Chunker in your Dockerfile:
  ```dockerfile
  FROM python:3.9-slim
  
  WORKDIR /app
  
  COPY requirements.txt .
  RUN pip install --no-cache-dir -r requirements.txt
  
  COPY . .
  
  CMD ["python", "your_application.py"]
  ```

- Set appropriate container resource limits:
  ```yaml
  # docker-compose.yml
  services:
    document-processor:
      build: .
      mem_limit: 2g
      cpus: 2.0
      environment:
        - CHUNKER_MAX_TOKENS_PER_CHUNK=1000
        - CHUNKER_OVERLAP_TOKENS=100
  ```

**Kubernetes:**
- Set resource requests and limits in your pod spec:
  ```yaml
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "4Gi"
      cpu: "2"
  ```

- Configure horizontal pod autoscaling for document processing services:
  ```yaml
  apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    name: document-processor
  spec:
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: document-processor
    minReplicas: 2
    maxReplicas: 10
    metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
  ```

### Common Deployment Problems & Solutions

#### Memory Issues

**Problem:** Out of memory errors when processing large documents

**Solution:**
1. Enable memory safety mode
2. Use streaming interfaces for large files
3. Increase container/instance memory allocation
4. Add swap space for burst processing

#### Slow Performance

**Problem:** Chunking is taking too long on large documents

**Solution:**
1. Enable parallel processing with appropriate worker count
2. Use performance mode in auto chunker
3. Adjust token estimation strategy to PERFORMANCE
4. Pre-process and cache results for frequently accessed documents

#### Container Crashes

**Problem:** Docker containers crash when processing large files

**Solution:**
1. Increase container memory limits
2. Enable memory safety mode
3. Add container health checks:
   ```yaml
   healthcheck:
     test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health')"]
     interval: 30s
     timeout: 10s
     retries: 3
   ```

#### Distributed Processing

**Problem:** Need to process large volumes of documents across multiple nodes

**Solution:**
1. Implement a queue-based architecture (RabbitMQ, Kafka, SQS)
2. Use chunker in worker nodes that pull jobs from the queue
3. Implement result aggregation service
4. Example worker code:
   ```python
   import pika
   from enterprise_chunker import EnterpriseChunker
   
   chunker = EnterpriseChunker()
   
   def process_document(document_data):
       chunks = chunker.adaptive_chunk_text(document_data["content"])
       # Store results...
   
   # Connect to RabbitMQ
   connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
   channel = connection.channel()
   channel.queue_declare(queue='document_queue')
   
   def callback(ch, method, properties, body):
       document_data = json.loads(body)
       process_document(document_data)
       ch.basic_ack(delivery_tag=method.delivery_tag)
   
   channel.basic_consume(queue='document_queue', on_message_callback=callback)
   channel.start_consuming()
   ```

---

## 🚧 **16. Maintenance, Updates & Long-term Considerations**

### Keeping Enterprise Chunker Updated

#### Version Management

Enterprise Chunker follows Semantic Versioning (SemVer), making it easier to understand update impacts:

- **Major version changes (1.0.0 → 2.0.0)**: May include breaking API changes
- **Minor version changes (1.0.0 → 1.1.0)**: New features without breaking changes
- **Patch version changes (1.0.0 → 1.0.1)**: Bug fixes and minor improvements

To check your current version:

```python
from enterprise_chunker import __version__
print(f"Enterprise Chunker version: {__version__}")
```

#### Update Process

To update to the latest version:

```bash
pip install --upgrade enterprise-chunker
```

To update to a specific version:

```bash
pip install --upgrade enterprise-chunker==1.1.0
```

For applications in production, create automated tests that verify your integration works with new versions before deploying updates.

### Dependency Management

Enterprise Chunker has both required and optional dependencies. To ensure all dependencies are properly maintained:

1. **Regular Dependency Audits**:
   ```bash
   pip install safety
   safety check -r requirements.txt
   ```

2. **Dependency Updates**:
   ```bash
   pip install pip-upgrader
   pip-upgrade requirements.txt
   ```

3. **Conflict Resolution**:
   If you encounter dependency conflicts, consider using a dependency resolver:
   ```bash
   pip install pip-tools
   pip-compile requirements.in
   pip-sync requirements.txt
   ```

### Long-term Maintenance Strategies

#### Monitoring & Logging

Implement comprehensive monitoring for long-running applications:

```python
import logging
from enterprise_chunker.orchestrator import create_auto_chunker

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("chunker.log"),
        logging.StreamHandler()
    ]
)

# Enable metrics server for monitoring
chunker = create_auto_chunker(
    options=options,
    enable_metrics_server=True,
    metrics_port=8000
)

# Connect to monitoring systems like Prometheus/Grafana
# http://localhost:8000/metrics
```

#### Performance Tuning Over Time

As you collect performance data, tune settings for optimal performance:

```python
# Load configuration from database or config service
from database import get_chunker_config

config = get_chunker_config()

# Apply dynamic configuration
chunker = create_auto_chunker(
    options=options,
    size_threshold=config['size_threshold'],
    complexity_threshold=config['complexity_threshold'],
    memory_safety=config['memory_safety']
)
```

#### Handling Deprecated Features

When features are deprecated, update your code following migration guides:

```python
# Old approach (deprecated)
# chunks = chunker.chunk_text(text)

# New approach
chunks = chunker.adaptive_chunk_text(text)
```

#### Maintenance Checklist

For long-term system health, perform regular maintenance:

1. **Weekly**:
   - Review error logs
   - Check memory usage patterns
   - Verify processing times remain consistent

2. **Monthly**:
   - Check for Enterprise Chunker updates
   - Run dependency security scans
   - Review and tune performance settings

3. **Quarterly**:
   - Benchmark chunking performance
   - Test with new document types/formats
   - Review resource allocation

### Scaling Strategies

As your document processing needs grow:

1. **Vertical Scaling**:
   - Increase memory and CPU resources
   - Adjust worker counts based on available resources
   ```python
   chunker = create_auto_chunker(
       options=options,
       worker_count_override=os.cpu_count() * 2
   )
   ```

2. **Horizontal Scaling**:
   - Implement document sharding across multiple instances
   - Use message queues to distribute processing
   - Implement stateless processing nodes

3. **Specialized Processing**:
   - Create dedicated instances for specific document formats
   - Route documents to specialized processors based on format
   ```python
   if format_type == ContentFormat.JSON:
       # Route to JSON-optimized processor
       json_processor.process(document)
   elif format_type == ContentFormat.MARKDOWN:
       # Route to Markdown-optimized processor
       markdown_processor.process(document)
   ```

---

## 🔮 **17. Further Resources & Next Steps**

### Official Documentation & Resources

- **Enterprise Chunker GitHub Repository**: For code examples, issues, and contributions
- **API Reference Documentation**: Comprehensive documentation of all classes and methods
- **User Forums**: Community discussion and support

### Learning Resources

#### Tutorials & Guides

- **[Beginner Guide](https://colab.research.google.com/github/enterprise-chunker/examples/blob/main/tutorials/beginner_guide.ipynb)**: "Getting Started with Enterprise Chunker"
- **[Intermediate Tutorial](https://colab.research.google.com/github/enterprise-chunker/examples/blob/main/tutorials/intermediate_guide.ipynb)**: "Optimizing Chunking Strategies for Different Document Types"
- **[Advanced Guide](https://colab.research.google.com/github/enterprise-chunker/examples/blob/main/tutorials/advanced_guide.ipynb)**: "Building High-Performance Document Processing Pipelines"

#### Video Courses

- **[Document Processing for LLM Applications](https://www.youtube.com/playlist?list=PLExample123)**: A video course covering Enterprise Chunker fundamentals
- **[Advanced RAG Systems with Enterprise Chunker](https://www.youtube.com/playlist?list=PLExample456)**: Comprehensive video tutorials on building RAG systems

### Community & Support

#### Community Channels

- **[GitHub Discussions](https://github.com/enterprise-chunker/discussions)**: Technical discussions and feature requests
- **[Discord Community](https://discord.gg/enterprise-chunker)**: Real-time chat and support
- **[Stack Overflow](https://stackoverflow.com/questions/tagged/enterprise-chunker)**: Questions tagged with `enterprise-chunker`

#### Commercial Support Options

- **[Enterprise Support Plans](https://www.enterprise-chunker.com/support)**: For organizations requiring SLAs and priority support
- **[Consulting Services](https://www.enterprise-chunker.com/consulting)**: Expert implementation and optimization

### Integration Ecosystem

#### LLM Frameworks

- **[LangChain Integration](https://python.langchain.com/docs/integrations/text_splitters/enterprise_chunker)**: Documentation on integrating with LangChain's text splitters
- **[LlamaIndex Integration](https://docs.llamaindex.ai/en/stable/examples/node_parsers/enterprise_chunker_node_parser)**: Guides for using Enterprise Chunker with LlamaIndex

#### Vector Databases

- **[Pinecone Example](https://docs.pinecone.io/docs/examples/enterprise-chunker)**: Example projects for document chunking and retrieval
- **[Weaviate Guide](https://weaviate.io/developers/weaviate/tutorials/enterprise-chunker)**: Integration tutorials for semantic search
- **[Milvus Documentation](https://milvus.io/docs/enterprise-chunker-integration.md)**: Guide to optimal chunking for Milvus indexing

### Next Steps in Your Learning Journey

#### Beginner Next Steps

1. Try different chunking strategies on your documents
2. Experiment with different overlap settings
3. Integrate with a simple LLM API

#### Intermediate Next Steps

1. Implement memory-efficient processing for large documents
2. Create format-specific processing pipelines
3. Add monitoring and performance metrics

#### Advanced Next Steps

1. Build a distributed document processing architecture
2. Implement custom chunking strategies for specialized formats
3. Contribute to the Enterprise Chunker project

### Contributing to Enterprise Chunker

The project welcomes contributions:

1. **Bug Reports**: Help improve stability by reporting issues
2. **Feature Requests**: Suggest new features and improvements
3. **Documentation**: Improve guides and examples
4. **Code Contributions**: Submit pull requests for new features or bug fixes

To contribute:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

### Expanding Your Knowledge

To deepen your understanding of the concepts behind Enterprise Chunker:

1. **Research Papers**:
   - "Efficient Document Segmentation for Large Language Models"
   - "Semantic Preservation in Text Chunking"

2. **Related Technologies**:
   - Explore text embedding techniques for retrieval systems
   - Learn about document processing pipelines and extraction systems
   - Study vector search algorithms and similarity metrics

3. **Advanced Use Cases**:
   - Building multi-modal chunking systems (text + images)
   - Creating hierarchical document representations
   - Implementing cross-document reference tracking

---

## API Reference Documentation

<!-- Auto-generated API documentation using pdoc3 -->

### EnterpriseChunker

```python
class EnterpriseChunker:
    """Enterprise-grade text chunking utility for LLM processing"""
    
    def __init__(self, options: Optional[Dict[str, Any]] = None):
        """
        Initialize the chunker with configuration options
        
        Args:
            options: Optional configuration dictionary
        """
    
    def adaptive_chunk_text(
        self, 
        text: str, 
        max_tokens_per_chunk: Optional[int] = None,
        overlap_tokens: Optional[int] = None,
        strategy: Optional[Union[str, ChunkingStrategy]] = None
    ) -> List[str]:
        """
        Main entry point: Adaptively chunk text based on content format
        
        Args:
            text: Text content to chunk
            max_tokens_per_chunk: Maximum tokens per chunk (overrides class settings)
            overlap_tokens: Number of tokens to overlap between chunks
            strategy: Chunking strategy to use
            
        Returns:
            List of text chunks optimized for processing
        """
    
    def chunk_stream(
        self, 
        stream: Union[str, io.TextIOBase], 
        **kwargs
    ) -> Generator[str, None, None]:
        """
        Process a text stream by dynamically chunking it as it's read
        
        Args:
            stream: Text stream to process
            **kwargs: Additional options for chunking
            
        Yields:
            Text chunks sequentially
        """
    
    def with_max_tokens(self, max_tokens: int) -> 'EnterpriseChunker':
        """
        Fluent API for setting max tokens per chunk
        
        Args:
            max_tokens: Maximum tokens per chunk
            
        Returns:
            Self for chaining
        """
    
    def with_overlap(self, overlap_tokens: int) -> 'EnterpriseChunker':
        """
        Fluent API for setting overlap tokens
        
        Args:
            overlap_tokens: Number of tokens to overlap
            
        Returns:
            Self for chaining
        """
    
    def with_strategy(self, strategy: Union[str, ChunkingStrategy]) -> 'EnterpriseChunker':
        """
        Fluent API for setting chunking strategy
        
        Args:
            strategy: Chunking strategy to use
            
        Returns:
            Self for chaining
        """
    
    def chunk(self, text: str) -> List[str]:
        """
        Chunk text with current configuration
        
        Args:
            text: Text to chunk
            
        Returns:
            List of text chunks
        """
```

### ChunkingOptions

```python
class ChunkingOptions:
    """Configuration options for chunking"""
    
    def __init__(self, **kwargs):
        """
        Initialize chunking options from provided kwargs
        
        Args:
            **kwargs: Configuration parameters
        """
    
    def get_effective_max_tokens(self) -> int:
        """
        Calculate effective maximum tokens, accounting for reserved tokens and safety margin
        
        Returns:
            Effective maximum tokens that should be used for chunking
        """
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert options to dictionary
        
        Returns:
            Dictionary representation of options
        """
```

### Format-Specific Chunking Strategies

```python
class JsonChunkingStrategy(BaseChunkingStrategy):
    """
    Strategy for chunking JSON content with structural preservation.
    """
    
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        This method is overridden but not used for JSON chunking
        because we parse the JSON and handle it differently.
        """
    
    def chunk(self, text: str, options: ChunkingOptions) -> ChunkingResult:
        """
        Override the chunking process for JSON with structure-aware processing.
        """

class MarkdownChunkingStrategy(BaseChunkingStrategy):
    """
    Strategy for chunking Markdown content with header/section awareness
    """
    
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        Detect Markdown boundaries like headers, lists, and code blocks
        """

class ReactVueChunkingStrategy(BaseChunkingStrategy):
    """
    Strategy for chunking React and Vue component files with structure awareness.
    """
    
    def chunk(self, text: str, options: ChunkingOptions) -> ChunkingResult:
        """
        Override to detect Vue or React before chunking.
        """
    
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        Detect boundaries in React or Vue components based on component type.
        """

class SmalltalkChunkingStrategy(BaseChunkingStrategy):
    """
    Strategy for chunking Smalltalk code with method and class awareness.
    """
    
    def chunk(self, text: str, options: ChunkingOptions) -> ChunkingResult:
        """
        Override to detect Smalltalk dialect before chunking.
        """
    
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        Detect method and class boundaries in Smalltalk code.
        """
```

---

## Conclusion

Enterprise Chunker is a powerful tool that solves the critical challenge of breaking down large documents for LLM processing. By intelligently preserving document structure and meaning, it enables LLMs to work with documents of any size while maintaining context and coherence.

Whether you're building a simple Q&A system or a complex enterprise document processing pipeline, the techniques and best practices covered in this guide will help you get the most out of Enterprise Chunker and create more effective, efficient, and reliable LLM applications.

Happy chunking!

<script>
// Simple search functionality
document.addEventListener('DOMContentLoaded', function() {
    const searchInput = document.getElementById('search-input');
    const searchResults = document.getElementById('search-results');
    const headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
    
    searchInput.addEventListener('input', function() {
        const query = this.value.toLowerCase();
        
        if (query.length < 2) {
            searchResults.innerHTML = '';
            return;
        }
        
        const results = [];
        headings.forEach(heading => {
            const text = heading.textContent.toLowerCase();
            if (text.includes(query)) {
                results.push({
                    text: heading.textContent,
                    id: heading.id || heading.textContent.toLowerCase().replace(/[^\w]+/g, '-'),
                    level: parseInt(heading.tagName.substring(1))
                });
            }
        });
        
        const content = document.querySelector('body');
        const contentText = content.textContent.toLowerCase();
        let match;
        const regex = new RegExp(`[^.!?]*${query}[^.!?]*[.!?]`, 'gi');
        
        while ((match = regex.exec(contentText)) !== null && results.length < 10) {
            const context = match[0].trim();
            if (context.length > 20) {
                results.push({
                    text: '...' + context + '...',
                    context: true
                });
            }
        }
        
        if (results.length > 0) {
            searchResults.innerHTML = results.map(result => {
                if (result.context) {
                    return `<div class="search-result-context">${result.text}</div>`;
                } else {
                    const indent = '&nbsp;'.repeat((result.level - 1) * 2);
                    return `<div class="search-result"><a href="#${result.id}">${indent}${result.text}</a></div>`;
                }
            }).join('');
        } else {
            searchResults.innerHTML = '<div class="no-results">No results found</div>';
        }
    });
});
</script>

<style>
.search-container {
    position: sticky;
    top: 0;
    background: white;
    padding: 10px;
    border-bottom: 1px solid #ddd;
    z-index: 1000;
}

#search-input {
    width: 100%;
    padding: 8px;
    font-size: 16px;
    border: 1px solid #ddd;
    border-radius: 4px;
}

#search-results {
    position: absolute;
    width: 100%;
    max-height: 400px;
    overflow-y: auto;
    background: white;
    border: 1px solid #ddd;
    border-top: none;
    border-radius: 0 0 4px 4px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    display: none;
}

#search-input:focus + #search-results, #search-results:hover {
    display: block;
}

.search-result, .search-result-context, .no-results {
    padding: 8px 12px;
    border-bottom: 1px solid #eee;
}

.search-result:hover {
    background-color: #f5f5f5;
}

.search-result a {
    color: #333;
    text-decoration: none;
    display: block;
}

.search-result-context {
    font-size: 14px;
    color: #666;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
}
</style>
</file>

<file path="exceptions.py">
"""
Exception classes for Enterprise Chunker.

This module defines the comprehensive exception hierarchy used throughout the Enterprise Chunker library,
providing consistent error handling and informative error messages for all possible failure modes.

Exceptions are organized in a logical hierarchy that allows catching specific error types
while also enabling broad exception handling when needed. Each exception class captures
contextual information relevant to diagnosing and resolving the specific error condition.

The exception hierarchy follows this structure:
- EnterpriseChunkerError (Base class)
  ├── ChunkingError (Core chunking failures)
  │   ├── BoundaryDetectionError
  │   ├── FormatParsingError
  │   │   ├── JsonParsingError
  │   │   ├── MarkdownParsingError
  │   │   ├── XmlParsingError
  │   │   └── ... (Other formats)
  │   └── RecoveryFailureError
  ├── FormatDetectionError
  ├── StreamProcessingError
  ├── ParallelProcessingError
  ├── MemorySafetyError
  ├── ResourceExhaustionError
  ├── CircuitBreakerError
  ├── TimeoutExceededError
  ├── TokenEstimationError
  ├── ValidationError
  ├── ConfigurationError
  ├── PatternLoadError
  ├── PluginError
  ├── ExternalLibraryError
  ├── FileAccessError
  └── UnsupportedOperationError
  ├── SecurityViolationError

Typical usage:
    try:
        result = chunker.chunk(text)
    except BoundaryDetectionError as e:
        # Handle boundary-specific errors
        logger.warning(f"Boundary detection failed: {e}")
    except ChunkingError as e:
        # Handle any chunking error
        logger.error(f"Chunking failed: {e}")
    except FormatDetectionError as e:
        # Handle format detection errors
        logger.error(f"Format detection failed: {e}")
    except EnterpriseChunkerError as e:
        # Handle any library error
        logger.error(f"Enterprise Chunker error: {e}")
"""

import logging
from typing import Optional, Dict, Any, List, Union, Tuple

# Configure module logger
logger = logging.getLogger(__name__)


class EnterpriseChunkerError(Exception):
    """
    Base exception class for all Enterprise Chunker errors.
    
    This serves as the root of the exception hierarchy, allowing users to catch
    all library-specific exceptions with a single except clause.
    
    Attributes:
        message: Human-readable error description
        operation_id: Optional identifier for the operation that failed
        details: Optional dictionary with additional error context
    """
    
    def __init__(
        self, 
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ):
        """
        Initialize the exception with error information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
        """
        self.message = message
        self.operation_id = operation_id
        self.details = details or {}
        
        # Format the message with operation_id if available
        formatted_message = message
        if operation_id:
            formatted_message = f"[{operation_id}] {message}"
            
        super().__init__(formatted_message)


class ChunkingError(EnterpriseChunkerError):
    """
    Exception raised when the chunking process fails.
    
    This is a general exception for errors that occur during the core chunking process,
    serving as a parent class for more specific chunking-related exceptions.
    
    Attributes:
        strategy: Optional name of the chunking strategy that was used
        text_length: Optional length of the text being processed
    """
    
    def __init__(
        self, 
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        strategy: Optional[str] = None,
        text_length: Optional[int] = None,
        fallback_attempted: bool = False
    ):
        """
        Initialize chunking error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            strategy: Optional name of the chunking strategy used
            text_length: Optional length of the text being processed
            fallback_attempted: Whether a fallback strategy was attempted
        """
        self.strategy = strategy
        self.text_length = text_length
        self.fallback_attempted = fallback_attempted
        
        # Add strategy and text_length to details if provided
        error_details = details or {}
        if strategy:
            error_details["strategy"] = strategy
        if text_length is not None:
            error_details["text_length"] = text_length
        error_details["fallback_attempted"] = fallback_attempted
        
        super().__init__(message, operation_id, error_details)


class BoundaryDetectionError(ChunkingError):
    """
    Exception raised when boundary detection fails.
    
    This exception is used specifically for errors during the boundary detection phase
    of the chunking process, which identifies where to split the text.
    
    Attributes:
        boundary_type: Optional type of boundary being detected
        detected_boundaries: Optional count of boundaries successfully detected before failure
        format_type: Optional content format being processed
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        strategy: Optional[str] = None,
        text_length: Optional[int] = None,
        boundary_type: Optional[str] = None,
        detected_boundaries: Optional[int] = None,
        format_type: Optional[str] = None
    ):
        """
        Initialize boundary detection error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            strategy: Optional name of the chunking strategy used
            text_length: Optional length of the text being processed
            boundary_type: Optional type of boundary being detected
            detected_boundaries: Optional count of boundaries detected before failure
            format_type: Optional content format being processed
        """
        self.boundary_type = boundary_type
        self.detected_boundaries = detected_boundaries
        self.format_type = format_type
        
        # Add boundary information to details
        error_details = details or {}
        if boundary_type:
            error_details["boundary_type"] = boundary_type
        if detected_boundaries is not None:
            error_details["detected_boundaries"] = detected_boundaries
        if format_type:
            error_details["format_type"] = format_type
            
        super().__init__(
            message, 
            operation_id, 
            error_details, 
            strategy, 
            text_length
        )


class FormatDetectionError(EnterpriseChunkerError):
    """
    Exception raised when content format detection fails.
    
    This exception occurs when the library cannot determine the format
    of the content being processed, which affects strategy selection.
    
    Attributes:
        sample_size: Size of the sample used for format detection
        detected_formats: Dictionary of format candidates with confidence scores
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        sample_size: Optional[int] = None,
        detected_formats: Optional[Dict[str, float]] = None
    ):
        """
        Initialize format detection error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            sample_size: Size of the sample used for format detection
            detected_formats: Dictionary of format candidates with confidence scores
        """
        self.sample_size = sample_size
        self.detected_formats = detected_formats
        
        # Add format detection info to details
        error_details = details or {}
        if sample_size is not None:
            error_details["sample_size"] = sample_size
        if detected_formats:
            error_details["detected_formats"] = detected_formats
            
        super().__init__(message, operation_id, error_details)


class ParallelProcessingError(ChunkingError):
    """
    Exception raised when parallel processing fails.
    
    This exception occurs during multi-threaded or multi-process chunking
    operations, typically in the orchestrator layer.
    
    Attributes:
        worker_count: Number of workers that were being used
        completed_tasks: Number of tasks that were completed before the error
        worker_errors: List of worker-specific error messages
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        worker_count: Optional[int] = None,
        completed_tasks: Optional[int] = None,
        worker_errors: Optional[List[str]] = None,
        use_processes: bool = False
    ):
        """
        Initialize parallel processing error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            worker_count: Number of workers being used
            completed_tasks: Number of tasks completed before error
            worker_errors: List of worker-specific error messages
            use_processes: Whether using processes (True) or threads (False)
        """
        self.worker_count = worker_count
        self.completed_tasks = completed_tasks
        self.worker_errors = worker_errors
        self.use_processes = use_processes
        
        # Add parallel processing info to details
        error_details = details or {}
        if worker_count is not None:
            error_details["worker_count"] = worker_count
        if completed_tasks is not None:
            error_details["completed_tasks"] = completed_tasks
        if worker_errors:
            error_details["worker_errors"] = worker_errors
        error_details["parallel_mode"] = "processes" if use_processes else "threads"
            
        super().__init__(message, operation_id, error_details)


class MemorySafetyError(EnterpriseChunkerError):
    """
    Exception raised when memory safety limits would be exceeded.
    
    This exception occurs when processing would require more memory
    than is safely available, preventing out-of-memory conditions.
    
    Attributes:
        required_memory: Estimated memory required for the operation (MB)
        available_memory: Estimated memory available (MB)
        operation_type: Type of operation that would exceed memory limits
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        required_memory: Optional[float] = None,
        available_memory: Optional[float] = None,
        operation_type: Optional[str] = None
    ):
        """
        Initialize memory safety error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            required_memory: Estimated memory required for operation (MB)
            available_memory: Estimated memory available (MB)
            operation_type: Type of operation that would exceed memory limits
        """
        self.required_memory = required_memory
        self.available_memory = available_memory
        self.operation_type = operation_type
        
        # Add memory information to details
        error_details = details or {}
        if required_memory is not None:
            error_details["required_memory_mb"] = required_memory
        if available_memory is not None:
            error_details["available_memory_mb"] = available_memory
        if operation_type:
            error_details["operation_type"] = operation_type
            
        super().__init__(message, operation_id, error_details)


class CircuitBreakerError(EnterpriseChunkerError):
    """
    Exception raised when the circuit breaker is open.
    
    This exception prevents cascading failures by stopping operations
    when the system is in a degraded state.
    
    Attributes:
        health_status: Current health status of the system
        reset_time: When the circuit breaker will reset (seconds since epoch)
        error_rate: Recent error rate that triggered the circuit breaker
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        health_status: Optional[str] = None,
        reset_time: Optional[float] = None,
        error_rate: Optional[float] = None
    ):
        """
        Initialize circuit breaker error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            health_status: Current health status of the system
            reset_time: When the circuit breaker will reset (seconds since epoch)
            error_rate: Recent error rate that triggered the circuit breaker
        """
        self.health_status = health_status
        self.reset_time = reset_time
        self.error_rate = error_rate
        
        # Add circuit breaker info to details
        error_details = details or {}
        if health_status:
            error_details["health_status"] = health_status
        if reset_time is not None:
            error_details["reset_time"] = reset_time
        if error_rate is not None:
            error_details["error_rate"] = error_rate
            
        super().__init__(message, operation_id, error_details)


class TimeoutExceededError(EnterpriseChunkerError):
    """
    Exception raised when an operation times out.
    
    This exception occurs when processing takes longer than the
    configured timeout, preventing indefinite blocking.
    
    Attributes:
        timeout: Timeout duration that was exceeded (seconds)
        elapsed: Actual time elapsed before timeout (seconds)
        operation_type: Type of operation that timed out
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        timeout: Optional[float] = None,
        elapsed: Optional[float] = None,
        operation_type: Optional[str] = None
    ):
        """
        Initialize timeout error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            timeout: Timeout duration that was exceeded (seconds)
            elapsed: Actual time elapsed before timeout (seconds)
            operation_type: Type of operation that timed out
        """
        self.timeout = timeout
        self.elapsed = elapsed
        self.operation_type = operation_type
        
        # Add timeout info to details
        error_details = details or {}
        if timeout is not None:
            error_details["timeout_seconds"] = timeout
        if elapsed is not None:
            error_details["elapsed_seconds"] = elapsed
        if operation_type:
            error_details["operation_type"] = operation_type
            
        super().__init__(message, operation_id, error_details)


class TokenEstimationError(EnterpriseChunkerError):
    """
    Exception raised when token estimation fails.
    
    This exception occurs during the token counting/estimation process,
    which is critical for determining chunk sizes.
    
    Attributes:
        estimation_strategy: Token estimation strategy being used
        sample_length: Length of the text sample being estimated
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        estimation_strategy: Optional[str] = None,
        sample_length: Optional[int] = None
    ):
        """
        Initialize token estimation error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            estimation_strategy: Token estimation strategy being used
            sample_length: Length of the text sample being estimated
        """
        self.estimation_strategy = estimation_strategy
        self.sample_length = sample_length
        
        # Add estimation info to details
        error_details = details or {}
        if estimation_strategy:
            error_details["estimation_strategy"] = estimation_strategy
        if sample_length is not None:
            error_details["sample_length"] = sample_length
            
        super().__init__(message, operation_id, error_details)


class ValidationError(EnterpriseChunkerError):
    """
    Exception raised when input validation fails.
    
    This exception occurs when input parameters do not meet required
    constraints, such as type, range, or format.
    
    Attributes:
        parameter: Name of the parameter that failed validation
        expected: Description of the expected value or constraint
        received: The actual value received
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        parameter: Optional[str] = None,
        expected: Optional[str] = None,
        received: Optional[Any] = None
    ):
        """
        Initialize validation error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            parameter: Name of the parameter that failed validation
            expected: Description of the expected value or constraint
            received: The actual value received
        """
        self.parameter = parameter
        self.expected = expected
        self.received = received
        
        # Add validation info to details
        error_details = details or {}
        if parameter:
            error_details["parameter"] = parameter
        if expected:
            error_details["expected"] = expected
        if received is not None:
            # Convert received value to string for better error messages
            error_details["received"] = str(received)
            
        super().__init__(message, operation_id, error_details)


class ConfigurationError(EnterpriseChunkerError):
    """
    Exception raised when there is an issue with configuration.
    
    This exception occurs when configuration parameters are invalid,
    missing, or conflicting, preventing proper operation.
    
    Attributes:
        config_source: Source of the configuration (e.g., file, env vars)
        config_keys: List of configuration keys involved in the error
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        config_source: Optional[str] = None,
        config_keys: Optional[List[str]] = None
    ):
        """
        Initialize configuration error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            config_source: Source of the configuration
            config_keys: List of configuration keys involved in the error
        """
        self.config_source = config_source
        self.config_keys = config_keys
        
        # Add configuration info to details
        error_details = details or {}
        if config_source:
            error_details["config_source"] = config_source
        if config_keys:
            error_details["config_keys"] = config_keys
            
        super().__init__(message, operation_id, error_details)


class PatternLoadError(EnterpriseChunkerError):
    """
    Exception raised when regex pattern compilation fails.
    
    This exception occurs when a regular expression pattern cannot be
    compiled, typically due to syntax errors.
    
    Attributes:
        pattern_name: Name or identifier of the pattern
        pattern_source: The regular expression pattern that failed to compile
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        pattern_name: Optional[str] = None,
        pattern_source: Optional[str] = None
    ):
        """
        Initialize pattern load error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            pattern_name: Name or identifier of the pattern
            pattern_source: The regex pattern that failed to compile
        """
        self.pattern_name = pattern_name
        self.pattern_source = pattern_source
        
        # Add pattern info to details
        error_details = details or {}
        if pattern_name:
            error_details["pattern_name"] = pattern_name
        if pattern_source:
            error_details["pattern_source"] = pattern_source
            
        super().__init__(message, operation_id, error_details)


class PluginError(EnterpriseChunkerError):
    """
    Exception raised when there is an issue with a plugin.
    
    This exception occurs when loading, initializing, or using a plugin
    fails, allowing graceful fallback to built-in functionality.
    
    Attributes:
        plugin_name: Name of the plugin
        plugin_version: Version of the plugin
        entry_point: Entry point for the plugin
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        plugin_name: Optional[str] = None,
        plugin_version: Optional[str] = None,
        entry_point: Optional[str] = None
    ):
        """
        Initialize plugin error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            plugin_name: Name of the plugin
            plugin_version: Version of the plugin
            entry_point: Entry point for the plugin
        """
        self.plugin_name = plugin_name
        self.plugin_version = plugin_version
        self.entry_point = entry_point
        
        # Add plugin info to details
        error_details = details or {}
        if plugin_name:
            error_details["plugin_name"] = plugin_name
        if plugin_version:
            error_details["plugin_version"] = plugin_version
        if entry_point:
            error_details["entry_point"] = entry_point
            
        super().__init__(message, operation_id, error_details)


class StreamProcessingError(EnterpriseChunkerError):
    """
    Exception raised when streaming operations fail.
    
    This exception occurs during stream-based processing of large files
    or content, which is used to optimize memory usage.
    
    Attributes:
        stream_type: Type of stream being processed (file, string, etc.)
        bytes_processed: Number of bytes processed before the error
        buffer_size: Size of the stream buffer being used
        stream_position: Position in the stream when the error occurred
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        stream_type: Optional[str] = None,
        bytes_processed: Optional[int] = None,
        buffer_size: Optional[int] = None,
        stream_position: Optional[int] = None
    ):
        """
        Initialize stream processing error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            stream_type: Type of stream being processed
            bytes_processed: Number of bytes processed before the error
            buffer_size: Size of the stream buffer being used
            stream_position: Position in the stream when the error occurred
        """
        self.stream_type = stream_type
        self.bytes_processed = bytes_processed
        self.buffer_size = buffer_size
        self.stream_position = stream_position
        
        # Add stream info to details
        error_details = details or {}
        if stream_type:
            error_details["stream_type"] = stream_type
        if bytes_processed is not None:
            error_details["bytes_processed"] = bytes_processed
        if buffer_size is not None:
            error_details["buffer_size"] = buffer_size
        if stream_position is not None:
            error_details["stream_position"] = stream_position
            
        super().__init__(message, operation_id, error_details)


class FormatParsingError(ChunkingError):
    """
    Base exception for format-specific parsing errors.
    
    This exception occurs when parsing fails for a specific content format,
    serving as a parent class for more specific format parsing exceptions.
    
    Attributes:
        format_type: The content format being parsed
        parser_name: Name of the parser being used
        content_sample: Sample of the content that failed to parse
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        strategy: Optional[str] = None,
        text_length: Optional[int] = None,
        format_type: Optional[str] = None,
        parser_name: Optional[str] = None,
        content_sample: Optional[str] = None,
        sample_position: Optional[int] = None
    ):
        """
        Initialize format parsing error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            strategy: Optional name of the chunking strategy used
            text_length: Optional length of the text being processed
            format_type: The content format being parsed
            parser_name: Name of the parser being used
            content_sample: Sample of the content that failed to parse
            sample_position: Position of the sample in the original content
        """
        self.format_type = format_type
        self.parser_name = parser_name
        self.content_sample = content_sample
        self.sample_position = sample_position
        
        # Add parsing info to details
        error_details = details or {}
        if format_type:
            error_details["format_type"] = format_type
        if parser_name:
            error_details["parser_name"] = parser_name
        if content_sample:
            # Include only a reasonable sample size to avoid huge error messages
            max_sample_size = 100
            if len(content_sample) > max_sample_size:
                error_details["content_sample"] = f"{content_sample[:max_sample_size]}..."
            else:
                error_details["content_sample"] = content_sample
        if sample_position is not None:
            error_details["sample_position"] = sample_position
            
        super().__init__(message, operation_id, error_details, strategy, text_length)


class JsonParsingError(FormatParsingError):
    """
    Exception raised when JSON parsing fails.
    
    This exception occurs during parsing of JSON content in the
    JsonChunkingStrategy or other JSON-handling code.
    
    Attributes:
        json_path: JSON path to the element where parsing failed
        is_array: Whether the JSON content is an array
        is_object: Whether the JSON content is an object
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        strategy: Optional[str] = None,
        text_length: Optional[int] = None,
        content_sample: Optional[str] = None,
        json_path: Optional[str] = None,
        is_array: bool = False,
        is_object: bool = False,
        error_position: Optional[int] = None
    ):
        """
        Initialize JSON parsing error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            strategy: Optional name of the chunking strategy used
            text_length: Optional length of the text being processed
            content_sample: Sample of the content that failed to parse
            json_path: JSON path to the element where parsing failed
            is_array: Whether the JSON content is an array
            is_object: Whether the JSON content is an object
            error_position: Character position where the error occurred
        """
        self.json_path = json_path
        self.is_array = is_array
        self.is_object = is_object
        self.error_position = error_position
        
        # Add JSON-specific info to details
        error_details = details or {}
        if json_path:
            error_details["json_path"] = json_path
        error_details["is_array"] = is_array
        error_details["is_object"] = is_object
        if error_position is not None:
            error_details["error_position"] = error_position
            
        super().__init__(
            message, 
            operation_id, 
            error_details, 
            strategy, 
            text_length, 
            "JSON", 
            "json_parser", 
            content_sample
        )


class MarkdownParsingError(FormatParsingError):
    """
    Exception raised when Markdown parsing fails.
    
    This exception occurs during parsing of Markdown content in
    the MarkdownChunkingStrategy.
    
    Attributes:
        element_type: Type of Markdown element where parsing failed
        line_number: Line number where the error occurred
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        strategy: Optional[str] = None,
        text_length: Optional[int] = None,
        content_sample: Optional[str] = None,
        element_type: Optional[str] = None,
        line_number: Optional[int] = None
    ):
        """
        Initialize Markdown parsing error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            strategy: Optional name of the chunking strategy used
            text_length: Optional length of the text being processed
            content_sample: Sample of the content that failed to parse
            element_type: Type of Markdown element where parsing failed
            line_number: Line number where the error occurred
        """
        self.element_type = element_type
        self.line_number = line_number
        
        # Add Markdown-specific info to details
        error_details = details or {}
        if element_type:
            error_details["element_type"] = element_type
        if line_number is not None:
            error_details["line_number"] = line_number
            
        super().__init__(
            message, 
            operation_id, 
            error_details, 
            strategy, 
            text_length, 
            "Markdown", 
            "markdown_parser", 
            content_sample
        )


class XmlParsingError(FormatParsingError):
    """
    Exception raised when XML parsing fails.
    
    This exception occurs during parsing of XML content in
    the XmlChunkingStrategy.
    
    Attributes:
        element_path: XPath to the element where parsing failed
        line_number: Line number where the error occurred
        column_number: Column number where the error occurred
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        strategy: Optional[str] = None,
        text_length: Optional[int] = None,
        content_sample: Optional[str] = None,
        element_path: Optional[str] = None,
        line_number: Optional[int] = None,
        column_number: Optional[int] = None,
        is_malformed: bool = False
    ):
        """
        Initialize XML parsing error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            strategy: Optional name of the chunking strategy used
            text_length: Optional length of the text being processed
            content_sample: Sample of the content that failed to parse
            element_path: XPath to the element where parsing failed
            line_number: Line number where the error occurred
            column_number: Column number where the error occurred
            is_malformed: Whether the XML is structurally invalid
        """
        self.element_path = element_path
        self.line_number = line_number
        self.column_number = column_number
        self.is_malformed = is_malformed
        
        # Add XML-specific info to details
        error_details = details or {}
        if element_path:
            error_details["element_path"] = element_path
        if line_number is not None:
            error_details["line_number"] = line_number
        if column_number is not None:
            error_details["column_number"] = column_number
        error_details["is_malformed"] = is_malformed
            
        super().__init__(
            message, 
            operation_id, 
            error_details, 
            strategy, 
            text_length, 
            "XML", 
            "xml_parser", 
            content_sample
        )


class CodeParsingError(FormatParsingError):
    """
    Exception raised when code parsing fails.
    
    This exception occurs during parsing of code in code-specific
    chunking strategies like ReactVueChunkingStrategy.
    
    Attributes:
        language: Programming language of the code
        line_number: Line number where the error occurred
        feature_type: Type of code feature where parsing failed
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        strategy: Optional[str] = None,
        text_length: Optional[int] = None,
        content_sample: Optional[str] = None,
        language: Optional[str] = None,
        line_number: Optional[int] = None,
        feature_type: Optional[str] = None
    ):
        """
        Initialize code parsing error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            strategy: Optional name of the chunking strategy used
            text_length: Optional length of the text being processed
            content_sample: Sample of the content that failed to parse
            language: Programming language of the code
            line_number: Line number where the error occurred
            feature_type: Type of code feature where parsing failed
        """
        self.language = language
        self.line_number = line_number
        self.feature_type = feature_type
        
        # Add code-specific info to details
        error_details = details or {}
        if language:
            error_details["language"] = language
        if line_number is not None:
            error_details["line_number"] = line_number
        if feature_type:
            error_details["feature_type"] = feature_type
            
        super().__init__(
            message, 
            operation_id, 
            error_details, 
            strategy, 
            text_length, 
            "CODE", 
            "code_parser", 
            content_sample
        )


class RecoveryFailureError(ChunkingError):
    """
    Exception raised when recovery from a previous error fails.
    
    This exception occurs when a fallback strategy also fails after
    a primary chunking method encounters an error.
    
    Attributes:
        original_error: The original error that triggered recovery
        recovery_strategy: The strategy used for recovery
        recovery_attempts: Number of recovery attempts made
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        strategy: Optional[str] = None,
        text_length: Optional[int] = None,
        original_error: Optional[Exception] = None,
        recovery_strategy: Optional[str] = None,
        recovery_attempts: int = 0
    ):
        """
        Initialize recovery failure error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            strategy: Optional name of the chunking strategy used
            text_length: Optional length of the text being processed
            original_error: The original error that triggered recovery
            recovery_strategy: The strategy used for recovery
            recovery_attempts: Number of recovery attempts made
        """
        self.original_error = original_error
        self.recovery_strategy = recovery_strategy
        self.recovery_attempts = recovery_attempts
        
        # Add recovery info to details
        error_details = details or {}
        if original_error:
            error_details["original_error_type"] = type(original_error).__name__
            error_details["original_error_message"] = str(original_error)
        if recovery_strategy:
            error_details["recovery_strategy"] = recovery_strategy
        error_details["recovery_attempts"] = recovery_attempts
            
        super().__init__(message, operation_id, error_details, strategy, text_length)


class FileAccessError(EnterpriseChunkerError):
    """
    Exception raised when file access operations fail.
    
    This exception occurs when reading from or writing to files,
    which is commonly used for processing large documents.
    
    Attributes:
        file_path: Path to the file that couldn't be accessed
        operation: Type of file operation that failed
        is_temporary: Whether the file is a temporary file
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        file_path: Optional[str] = None,
        operation: Optional[str] = None,
        is_temporary: bool = False,
        os_error_code: Optional[int] = None
    ):
        """
        Initialize file access error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            file_path: Path to the file that couldn't be accessed
            operation: Type of file operation that failed (read, write, etc.)
            is_temporary: Whether the file is a temporary file
            os_error_code: Operating system error code, if available
        """
        self.file_path = file_path
        self.operation = operation
        self.is_temporary = is_temporary
        self.os_error_code = os_error_code
        
        # Add file info to details
        error_details = details or {}
        if file_path:
            error_details["file_path"] = file_path
        if operation:
            error_details["operation"] = operation
        error_details["is_temporary"] = is_temporary
        if os_error_code is not None:
            error_details["os_error_code"] = os_error_code
            
        super().__init__(message, operation_id, error_details)


class ResourceExhaustionError(EnterpriseChunkerError):
    """
    Exception raised when system resources are exhausted.
    
    This exception is a broader version of MemorySafetyError that covers
    other resource types like file handles, network connections, etc.
    
    Attributes:
        resource_type: Type of resource that was exhausted
        current_usage: Current usage of the resource
        maximum_allowed: Maximum allowed usage of the resource
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        resource_type: Optional[str] = None,
        current_usage: Optional[Union[int, float]] = None,
        maximum_allowed: Optional[Union[int, float]] = None,
        unit: Optional[str] = None
    ):
        """
        Initialize resource exhaustion error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            resource_type: Type of resource that was exhausted
            current_usage: Current usage of the resource
            maximum_allowed: Maximum allowed usage of the resource
            unit: Unit of measurement for the resource
        """
        self.resource_type = resource_type
        self.current_usage = current_usage
        self.maximum_allowed = maximum_allowed
        self.unit = unit
        
        # Add resource info to details
        error_details = details or {}
        if resource_type:
            error_details["resource_type"] = resource_type
        if current_usage is not None:
            error_details["current_usage"] = current_usage
        if maximum_allowed is not None:
            error_details["maximum_allowed"] = maximum_allowed
        if unit:
            error_details["unit"] = unit
            
        super().__init__(message, operation_id, error_details)


class ExternalLibraryError(EnterpriseChunkerError):
    """
    Exception raised when integration with an external library fails.
    
    This exception occurs when using optional dependencies like tiktoken,
    defusedxml, or other external libraries that might not be available.
    
    Attributes:
        library_name: Name of the external library
        library_version: Version of the external library if available
        feature: Feature that was using the external library
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        library_name: Optional[str] = None,
        library_version: Optional[str] = None,
        feature: Optional[str] = None,
        is_missing: bool = False
    ):
        """
        Initialize external library error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            library_name: Name of the external library
            library_version: Version of the external library if available
            feature: Feature that was using the external library
            is_missing: Whether the library is missing (not installed)
        """
        self.library_name = library_name
        self.library_version = library_version
        self.feature = feature
        self.is_missing = is_missing
        
        # Add library info to details
        error_details = details or {}
        if library_name:
            error_details["library_name"] = library_name
        if library_version:
            error_details["library_version"] = library_version
        if feature:
            error_details["feature"] = feature
        error_details["is_missing"] = is_missing
            
        super().__init__(message, operation_id, error_details)


class SecurityViolationError(EnterpriseChunkerError):
    """
    Exception raised when a security violation is detected.
    
    This exception occurs when processing potentially malicious content,
    such as XML external entity attacks or decompression bombs.
    
    Attributes:
        violation_type: Type of security violation detected
        content_format: Format of the content with the violation
        mitigation: Action taken to mitigate the security risk
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        violation_type: Optional[str] = None,
        content_format: Optional[str] = None,
        mitigation: Optional[str] = None,
        severity: Optional[str] = None
    ):
        """
        Initialize security violation error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            violation_type: Type of security violation detected
            content_format: Format of the content with the violation
            mitigation: Action taken to mitigate the security risk
            severity: Severity level of the security violation
        """
        self.violation_type = violation_type
        self.content_format = content_format
        self.mitigation = mitigation
        self.severity = severity
        
        # Add security info to details
        error_details = details or {}
        if violation_type:
            error_details["violation_type"] = violation_type
        if content_format:
            error_details["content_format"] = content_format
        if mitigation:
            error_details["mitigation"] = mitigation
        if severity:
            error_details["severity"] = severity
            
        super().__init__(message, operation_id, error_details)


class UnsupportedOperationError(EnterpriseChunkerError):
    """
    Exception raised when an unsupported operation is attempted.
    
    This exception occurs when a feature is used that is not supported
    in the current configuration or environment.
    
    Attributes:
        operation: Name of the operation that is not supported
        reason: Reason why the operation is not supported
        alternatives: List of alternative operations that are supported
    """
    
    def __init__(
        self,
        message: str,
        operation_id: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        operation: Optional[str] = None,
        reason: Optional[str] = None,
        alternatives: Optional[List[str]] = None
    ):
        """
        Initialize unsupported operation error with context information.
        
        Args:
            message: Human-readable error description
            operation_id: Optional identifier for the operation that failed
            details: Optional dictionary with additional error context
            operation: Name of the operation that is not supported
            reason: Reason why the operation is not supported
            alternatives: List of alternative operations that are supported
        """
        self.operation = operation
        self.reason = reason
        self.alternatives = alternatives
        
        # Add operation info to details
        error_details = details or {}
        if operation:
            error_details["operation"] = operation
        if reason:
            error_details["reason"] = reason
        if alternatives:
            error_details["alternatives"] = alternatives
            
        super().__init__(message, operation_id, error_details)


# Helper functions for consistent error creation and logging

def raise_chunking_error(
    message: str,
    operation_id: Optional[str] = None,
    strategy: Optional[str] = None,
    text_length: Optional[int] = None,
    details: Optional[Dict[str, Any]] = None,
    log_level: int = logging.ERROR,
    log_stacktrace: bool = True
) -> None:
    """
    Helper function to create, log, and raise a ChunkingError.
    
    Args:
        message: Human-readable error description
        operation_id: Optional identifier for the operation that failed
        strategy: Optional name of the chunking strategy used
        text_length: Optional length of the text being processed
        details: Optional dictionary with additional error context
        log_level: Logging level to use (default: ERROR)
        log_stacktrace: Whether to include stack trace in log (default: True)
        
    Raises:
        ChunkingError: Always raised with the provided information
    """
    # Create the error
    error = ChunkingError(
        message=message,
        operation_id=operation_id,
        strategy=strategy,
        text_length=text_length,
        details=details
    )
    
    # Log the error
    if operation_id:
        log_message = f"[{operation_id}] {message}"
    else:
        log_message = message
        
    if log_stacktrace:
        logger.log(log_level, log_message, exc_info=True)
    else:
        logger.log(log_level, log_message)
    
    # Raise the error
    raise error


def raise_boundary_detection_error(
    message: str,
    operation_id: Optional[str] = None,
    strategy: Optional[str] = None,
    text_length: Optional[int] = None,
    boundary_type: Optional[str] = None,
    detected_boundaries: Optional[int] = None,
    format_type: Optional[str] = None,
    details: Optional[Dict[str, Any]] = None,
    log_level: int = logging.ERROR,
    log_stacktrace: bool = True
) -> None:
    """
    Helper function to create, log, and raise a BoundaryDetectionError.
    
    Args:
        message: Human-readable error description
        operation_id: Optional identifier for the operation that failed
        strategy: Optional name of the chunking strategy used
        text_length: Optional length of the text being processed
        boundary_type: Optional type of boundary being detected
        detected_boundaries: Optional count of boundaries detected before failure
        format_type: Optional content format being processed
        details: Optional dictionary with additional error context
        log_level: Logging level to use (default: ERROR)
        log_stacktrace: Whether to include stack trace in log (default: True)
        
    Raises:
        BoundaryDetectionError: Always raised with the provided information
    """
    # Create the error
    error = BoundaryDetectionError(
        message=message,
        operation_id=operation_id,
        strategy=strategy,
        text_length=text_length,
        boundary_type=boundary_type,
        detected_boundaries=detected_boundaries,
        format_type=format_type,
        details=details
    )
    
    # Log the error
    if operation_id:
        log_message = f"[{operation_id}] {message}"
    else:
        log_message = message
        
    if log_stacktrace:
        logger.log(log_level, log_message, exc_info=True)
    else:
        logger.log(log_level, log_message)
    
    # Raise the error
    raise error


def raise_format_parsing_error(
    message: str,
    format_type: str,
    operation_id: Optional[str] = None,
    strategy: Optional[str] = None,
    text_length: Optional[int] = None,
    content_sample: Optional[str] = None,
    parser_name: Optional[str] = None,
    details: Optional[Dict[str, Any]] = None,
    log_level: int = logging.ERROR,
    log_stacktrace: bool = True
) -> None:
    """
    Helper function to create, log, and raise a FormatParsingError.
    
    Args:
        message: Human-readable error description
        format_type: Content format being parsed
        operation_id: Optional identifier for the operation that failed
        strategy: Optional name of the chunking strategy used
        text_length: Optional length of the text being processed
        content_sample: Sample of the content that failed to parse
        parser_name: Name of the parser being used
        details: Optional dictionary with additional error context
        log_level: Logging level to use (default: ERROR)
        log_stacktrace: Whether to include stack trace in log (default: True)
        
    Raises:
        FormatParsingError: Always raised with the provided information
    """
    # Create the error
    error = FormatParsingError(
        message=message,
        operation_id=operation_id,
        strategy=strategy,
        text_length=text_length,
        format_type=format_type,
        parser_name=parser_name,
        content_sample=content_sample,
        details=details
    )
    
    # Log the error
    if operation_id:
        log_message = f"[{operation_id}] {message}"
    else:
        log_message = message
        
    if log_stacktrace:
        logger.log(log_level, log_message, exc_info=True)
    else:
        logger.log(log_level, log_message)
    
    # Raise the error
    raise error


def raise_stream_processing_error(
    message: str,
    operation_id: Optional[str] = None,
    stream_type: Optional[str] = None,
    bytes_processed: Optional[int] = None,
    buffer_size: Optional[int] = None,
    stream_position: Optional[int] = None,
    details: Optional[Dict[str, Any]] = None,
    log_level: int = logging.ERROR,
    log_stacktrace: bool = True
) -> None:
    """
    Helper function to create, log, and raise a StreamProcessingError.
    
    Args:
        message: Human-readable error description
        operation_id: Optional identifier for the operation that failed
        stream_type: Type of stream being processed
        bytes_processed: Number of bytes processed before the error
        buffer_size: Size of the stream buffer being used
        stream_position: Position in the stream when the error occurred
        details: Optional dictionary with additional error context
        log_level: Logging level to use (default: ERROR)
        log_stacktrace: Whether to include stack trace in log (default: True)
        
    Raises:
        StreamProcessingError: Always raised with the provided information
    """
    # Create the error
    error = StreamProcessingError(
        message=message,
        operation_id=operation_id,
        stream_type=stream_type,
        bytes_processed=bytes_processed,
        buffer_size=buffer_size,
        stream_position=stream_position,
        details=details
    )
    
    # Log the error
    if operation_id:
        log_message = f"[{operation_id}] {message}"
    else:
        log_message = message
        
    if log_stacktrace:
        logger.log(log_level, log_message, exc_info=True)
    else:
        logger.log(log_level, log_message)
    
    # Raise the error
    raise error


def raise_file_access_error(
    message: str,
    operation_id: Optional[str] = None,
    file_path: Optional[str] = None,
    operation: Optional[str] = None,
    is_temporary: bool = False,
    os_error_code: Optional[int] = None,
    details: Optional[Dict[str, Any]] = None,
    log_level: int = logging.ERROR,
    log_stacktrace: bool = True
) -> None:
    """
    Helper function to create, log, and raise a FileAccessError.
    
    Args:
        message: Human-readable error description
        operation_id: Optional identifier for the operation that failed
        file_path: Path to the file that couldn't be accessed
        operation: Type of file operation that failed
        is_temporary: Whether the file is a temporary file
        os_error_code: Operating system error code
        details: Optional dictionary with additional error context
        log_level: Logging level to use (default: ERROR)
        log_stacktrace: Whether to include stack trace in log (default: True)
        
    Raises:
        FileAccessError: Always raised with the provided information
    """
    # Create the error
    error = FileAccessError(
        message=message,
        operation_id=operation_id,
        file_path=file_path,
        operation=operation,
        is_temporary=is_temporary,
        os_error_code=os_error_code,
        details=details
    )
    
    # Log the error
    if operation_id:
        log_message = f"[{operation_id}] {message}"
    else:
        log_message = message
        
    if log_stacktrace:
        logger.log(log_level, log_message, exc_info=True)
    else:
        logger.log(log_level, log_message)
    
    # Raise the error
    raise error


def raise_resource_exhaustion_error(
    message: str,
    operation_id: Optional[str] = None,
    resource_type: Optional[str] = None,
    current_usage: Optional[Union[int, float]] = None,
    maximum_allowed: Optional[Union[int, float]] = None,
    unit: Optional[str] = None,
    details: Optional[Dict[str, Any]] = None,
    log_level: int = logging.ERROR,
    log_stacktrace: bool = True
) -> None:
    """
    Helper function to create, log, and raise a ResourceExhaustionError.
    
    Args:
        message: Human-readable error description
        operation_id: Optional identifier for the operation that failed
        resource_type: Type of resource that was exhausted
        current_usage: Current usage of the resource
        maximum_allowed: Maximum allowed usage of the resource
        unit: Unit of measurement for the resource
        details: Optional dictionary with additional error context
        log_level: Logging level to use (default: ERROR)
        log_stacktrace: Whether to include stack trace in log (default: True)
        
    Raises:
        ResourceExhaustionError: Always raised with the provided information
    """
    # Create the error
    error = ResourceExhaustionError(
        message=message,
        operation_id=operation_id,
        resource_type=resource_type,
        current_usage=current_usage,
        maximum_allowed=maximum_allowed,
        unit=unit,
        details=details
    )
    
    # Log the error
    if operation_id:
        log_message = f"[{operation_id}] {message}"
    else:
        log_message = message
        
    if log_stacktrace:
        logger.log(log_level, log_message, exc_info=True)
    else:
        logger.log(log_level, log_message)
    
    # Raise the error
    raise error


def raise_security_violation_error(
    message: str,
    operation_id: Optional[str] = None,
    violation_type: Optional[str] = None,
    content_format: Optional[str] = None,
    mitigation: Optional[str] = None,
    severity: Optional[str] = None,
    details: Optional[Dict[str, Any]] = None,
    log_level: int = logging.ERROR,
    log_stacktrace: bool = True
) -> None:
    """
    Helper function to create, log, and raise a SecurityViolationError.
    
    Args:
        message: Human-readable error description
        operation_id: Optional identifier for the operation that failed
        violation_type: Type of security violation detected
        content_format: Format of the content with the violation
        mitigation: Action taken to mitigate the security risk
        severity: Severity level of the security violation
        details: Optional dictionary with additional error context
        log_level: Logging level to use (default: ERROR)
        log_stacktrace: Whether to include stack trace in log (default: True)
        
    Raises:
        SecurityViolationError: Always raised with the provided information
    """
    # Create the error
    error = SecurityViolationError(
        message=message,
        operation_id=operation_id,
        violation_type=violation_type,
        content_format=content_format,
        mitigation=mitigation,
        severity=severity,
        details=details
    )
    
    # Log the error
    if operation_id:
        log_message = f"[{operation_id}] {message}"
    else:
        log_message = message
        
    if log_stacktrace:
        logger.log(log_level, log_message, exc_info=True)
    else:
        logger.log(log_level, log_message)
    
    # Raise the error
    raise error
</file>

<file path="models/__init__.py">
"""
enterprise_chunker: Data Models Package
=======================================

This package contains data models used throughout the enterprise chunker application.
Models define the structure for chunking strategies, metadata, and content features.

Usage:
    from enterprise_chunker.models import ChunkingStrategy, ChunkMetadata
    
    strategy = ChunkingStrategy.SEMANTIC
    metadata = ChunkMetadata(id="chunk-123", text="Sample text")
"""

from __future__ import annotations

# Version information
__version__ = "0.1.0"

# Import models for easier access with type annotations
from enterprise_chunker.models.enums import (
    ChunkingStrategy,
    TokenEstimationStrategy,
    ContentFormat,
)
from enterprise_chunker.models.chunk_metadata import ChunkMetadata, ChunkingResult
from enterprise_chunker.models.content_features import ContentFeatures
from enterprise_chunker.models.user import User

# List available models for introspection and auto-import capabilities
__all__ = [
    "ChunkingStrategy",
    "TokenEstimationStrategy",
    "ContentFormat",
    "ChunkMetadata",
    "ChunkingResult",
    "ContentFeatures",
    "User"
]
</file>

<file path="models/chunk_metadata.py">
"""
Metadata models for chunks and chunking results
"""

import copy
from dataclasses import dataclass, field
from typing import List, Tuple, Optional
from enterprise_chunker.models.enums import ContentFormat, TokenEstimationStrategy, ChunkingStrategy


@dataclass(frozen=True)
class ChunkMetadata:
    """Metadata about a single chunk"""
    index: int
    total_chunks: int
    format: ContentFormat
    token_count: int
    char_count: int
    has_overlap: bool = False
    overlap_from: int = -1
    content_slice: Tuple[int, int] = (-1, -1)  # Original content slice (start, end)
    preserved_context: str = ""


@dataclass
class ChunkingResult:
    """Result of a chunking operation with metrics and metadata"""
    chunks: List[str]
    chunk_metadata: List[ChunkMetadata]
    original_length: int
    detected_format: ContentFormat
    token_estimation_strategy: TokenEstimationStrategy
    chunking_strategy: ChunkingStrategy
    processing_time: float
    total_token_count: int
    operation_id: str


class MetadataBuilder:
    """Builder for chunk metadata"""
    
    def __init__(self):
        self._reset_state()
        
    def _reset_state(self):
        """Reset the internal state to default values"""
        self.metadata = ChunkMetadata(
            index=0,
            total_chunks=0,
            format=ContentFormat.TEXT,
            token_count=0,
            char_count=0
        )
        
    def with_index(self, index: int):
        """Set the chunk index"""
        # Since ChunkMetadata is frozen, we need to create a new instance with updated fields
        self.metadata = ChunkMetadata(
            index=index,
            total_chunks=self.metadata.total_chunks,
            format=self.metadata.format,
            token_count=self.metadata.token_count,
            char_count=self.metadata.char_count,
            has_overlap=self.metadata.has_overlap,
            overlap_from=self.metadata.overlap_from,
            content_slice=self.metadata.content_slice,
            preserved_context=self.metadata.preserved_context
        )
        return self
        
    def with_total_chunks(self, total: int):
        """Set the total number of chunks"""
        self.metadata = ChunkMetadata(
            index=self.metadata.index,
            total_chunks=total,
            format=self.metadata.format,
            token_count=self.metadata.token_count,
            char_count=self.metadata.char_count,
            has_overlap=self.metadata.has_overlap,
            overlap_from=self.metadata.overlap_from,
            content_slice=self.metadata.content_slice,
            preserved_context=self.metadata.preserved_context
        )
        return self
        
    def with_format(self, format_type: ContentFormat):
        """Set the content format"""
        self.metadata = ChunkMetadata(
            index=self.metadata.index,
            total_chunks=self.metadata.total_chunks,
            format=format_type,
            token_count=self.metadata.token_count,
            char_count=self.metadata.char_count,
            has_overlap=self.metadata.has_overlap,
            overlap_from=self.metadata.overlap_from,
            content_slice=self.metadata.content_slice,
            preserved_context=self.metadata.preserved_context
        )
        return self
        
    def with_token_count(self, token_count: int):
        """Set the token count"""
        self.metadata = ChunkMetadata(
            index=self.metadata.index,
            total_chunks=self.metadata.total_chunks,
            format=self.metadata.format,
            token_count=token_count,
            char_count=self.metadata.char_count,
            has_overlap=self.metadata.has_overlap,
            overlap_from=self.metadata.overlap_from,
            content_slice=self.metadata.content_slice,
            preserved_context=self.metadata.preserved_context
        )
        return self
        
    def with_char_count(self, char_count: int):
        """Set the character count"""
        self.metadata = ChunkMetadata(
            index=self.metadata.index,
            total_chunks=self.metadata.total_chunks,
            format=self.metadata.format,
            token_count=self.metadata.token_count,
            char_count=char_count,
            has_overlap=self.metadata.has_overlap,
            overlap_from=self.metadata.overlap_from,
            content_slice=self.metadata.content_slice,
            preserved_context=self.metadata.preserved_context
        )
        return self
        
    def with_overlap(self, has_overlap: bool, overlap_from: int = -1):
        """Set overlap information"""
        self.metadata = ChunkMetadata(
            index=self.metadata.index,
            total_chunks=self.metadata.total_chunks,
            format=self.metadata.format,
            token_count=self.metadata.token_count,
            char_count=self.metadata.char_count,
            has_overlap=has_overlap,
            overlap_from=overlap_from,
            content_slice=self.metadata.content_slice,
            preserved_context=self.metadata.preserved_context
        )
        return self
        
    def with_content_slice(self, start: int, end: int):
        """Set the content slice in the original text"""
        self.metadata = ChunkMetadata(
            index=self.metadata.index,
            total_chunks=self.metadata.total_chunks,
            format=self.metadata.format,
            token_count=self.metadata.token_count,
            char_count=self.metadata.char_count,
            has_overlap=self.metadata.has_overlap,
            overlap_from=self.metadata.overlap_from,
            content_slice=(start, end),
            preserved_context=self.metadata.preserved_context
        )
        return self
        
    def with_preserved_context(self, context: str):
        """Set preserved context information"""
        self.metadata = ChunkMetadata(
            index=self.metadata.index,
            total_chunks=self.metadata.total_chunks,
            format=self.metadata.format,
            token_count=self.metadata.token_count,
            char_count=self.metadata.char_count,
            has_overlap=self.metadata.has_overlap,
            overlap_from=self.metadata.overlap_from,
            content_slice=self.metadata.content_slice,
            preserved_context=context
        )
        return self
        
    def build(self) -> ChunkMetadata:
        """Build and return the metadata object, then reset internal state"""
        # Create a deep copy of the metadata
        result = copy.deepcopy(self.metadata)
        
        # Reset the internal state to prevent accidental reuse
        self._reset_state()
        
        return result
</file>

<file path="models/content_features.py">
"""
Content feature models for text analysis
"""

from dataclasses import dataclass


@dataclass
class ContentFeatures:
    """Features extracted from text for analysis"""
    length: int
    word_count: int
    whitespace_ratio: float
    symbol_density: float
    has_cjk: bool
    has_emoji: bool
    avg_word_length: float
    sentences_count: int = 0
    has_code_blocks: bool = False
    has_list_items: bool = False
    has_tables: bool = False
    has_headings: bool = False
</file>

<file path="models/enums.py">
"""
Enum definitions for the EnterpriseChunker
"""

from enum import Enum


class ChunkingStrategy(Enum):
    """Strategies for chunking text content"""
    SEMANTIC = "semantic"  # Preserve semantic boundaries (paragraphs, sections)
    STRUCTURAL = "structural"  # Preserve structural elements (XML/HTML tags, code blocks)
    FIXED_SIZE = "fixed_size"  # Simple fixed-size chunks with overlap
    SENTENCE = "sentence"  # Split by sentences
    ADAPTIVE = "adaptive"  # Dynamically choose best strategy based on content


class TokenEstimationStrategy(Enum):
    """Strategies for token estimation"""
    PRECISION = "precision"  # More accurate but slower
    PERFORMANCE = "performance"  # Faster but less accurate
    BALANCED = "balanced"  # Compromise between accuracy and speed


class ContentFormat(Enum):
    """Types of content formats recognized by the chunker"""
    JSON = "json"
    XML = "xml"
    MARKDOWN = "markdown"
    CODE = "code"
    LOGS = "logs"
    CSV = "csv"
    TEXT = "text"
    UNKNOWN = "unknown"
</file>

<file path="models/user.py">
"""
User model for authentication and tracking.

This module defines the User dataclass for managing user authentication,
authorization roles, and tracking within the enterprise chunker system.
"""

from dataclasses import dataclass, field
from typing import Optional, List, Set


@dataclass(frozen=False)
class User:
    """
    User model for the system.
    
    Attributes:
        id: Unique identifier for the user
        username: User's login name
        email: User's email address (optional)
        roles: List of role identifiers the user has in the system
        is_active: Whether the user account is active
    """
    id: str
    username: str
    email: Optional[str] = None
    roles: List[str] = field(default_factory=list)
    is_active: bool = True
    
    def has_role(self, role: str) -> bool:
        """
        Check if user has a specific role.
        
        Args:
            role: The role identifier to check
            
        Returns:
            bool: True if the user has the role, False otherwise
        """
        return role in self.roles
    
    def add_role(self, role: str) -> None:
        """
        Add a role to the user if it doesn't already exist.
        
        Args:
            role: The role identifier to add
        """
        if role not in self.roles:
            self.roles.append(role)
    
    def remove_role(self, role: str) -> None:
        """
        Remove a role from the user if it exists.
        
        Args:
            role: The role identifier to remove
        """
        if role in self.roles:
            self.roles.remove(role)
    
    @property
    def display_name(self) -> str:
        """
        Get a user-friendly display name.
        
        Returns:
            str: Username if no email exists, otherwise the username portion of the email
        """
        if self.email:
            return self.email.split('@')[0]
        return self.username
    
    def __str__(self) -> str:
        """String representation of the user."""
        return f"User(id={self.id}, username={self.username})"
</file>

<file path="orchestrator.py">
"""
Orchestration layer for intelligent parallel chunking strategies.

This module provides an intelligent orchestration layer that automatically selects
between different chunking implementations based on content characteristics,
system resources, and performance metrics.
"""

import os
import time
import logging
import platform
import threading
import gc
import traceback
import math  # Added import for math module used in _estimate_complexity
from typing import (
    List, Dict, Any, Optional, Generator, Callable, Union, TypeVar,
    Tuple, Deque, NamedTuple
)
from threading import RLock, Event, Condition
from dataclasses import dataclass, field
from contextlib import contextmanager
from concurrent.futures import TimeoutError, CancelledError
from collections import deque
import statistics

# Import core chunking implementations
from enterprise_chunker.utils.parallel_processing import ParallelChunker as SimpleParallelChunker
from enterprise_chunker.models.enums import ChunkingStrategy
from enterprise_chunker.config import ChunkingOptions

# Import utilities from the existing modules
from enterprise_chunker.utils.memory_optimization import MemoryManager, MemoryMonitor
from enterprise_chunker.utils.optimized_streaming import StreamingBuffer
from enterprise_chunker.utils.token_estimation import estimate_tokens, TokenEstimationStrategy
from enterprise_chunker.utils.format_detection import detect_content_format, ContentFormat

# Optional imports
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    psutil = None
    HAS_PSUTIL = False
    logging.warning("psutil not installed; resource monitoring will be limited")

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False
    logging.warning("numpy not installed; advanced analytics will be limited")

try:
    from prometheus_client import start_http_server, Gauge, Counter, Summary
    HAS_PROMETHEUS = True
except ImportError:
    HAS_PROMETHEUS = False

# Configure logging
logger = logging.getLogger(__name__)

# Type variables and protocols
T = TypeVar('T')
ChunkerFunc = Callable[[str], List[str]]

# Constants
DEFAULT_TIMEOUT = 300.0  # 5 minutes
MAX_RETRIES = 3
MEMORY_SAFETY_MARGIN = 0.15  # 15% safety margin for memory usage
BACKPRESSURE_THRESHOLD = 1000
HEALTH_CHECK_INTERVAL = 10.0  # seconds
DEFAULT_BATCH_SIZE = 5
MAX_BATCH_SIZE = 20
DYNAMIC_SCALING_THRESHOLD = 0.75
PROCESS_MEMORY_LIMIT = 2 * (1024 ** 3)  # 2GB per process
THREAD_STACK_SIZE = 4 * (1024 ** 2)  # 4MB

# Priority levels for QoS
PRIORITY_LEVELS = {
    'high': {'timeout': 60, 'retries': 5, 'batch_size_multiplier': 0.5},
    'normal': {'timeout': DEFAULT_TIMEOUT, 'retries': MAX_RETRIES, 'batch_size_multiplier': 1.0},
    'background': {'timeout': DEFAULT_TIMEOUT * 2, 'retries': 1, 'batch_size_multiplier': 1.5}
}

# Initialize Prometheus metrics if available
if HAS_PROMETHEUS:
    CHUNKER_METRICS = {
        'throughput': Gauge('chunker_throughput', 'Chunks processed per second'),
        'memory_usage': Gauge('chunker_memory', 'Memory usage percentage'),
        'worker_util': Gauge('chunker_workers', 'Worker utilization percentage'),
        'processing_time': Summary('chunker_processing_time', 'Time to process text'),
        'error_count': Counter('chunker_errors', 'Number of processing errors'),
        'retry_count': Counter('chunker_retries', 'Number of retry attempts'),
        'batch_size': Gauge('chunker_batch_size', 'Current batch size')
    }


# Custom exceptions
class ChunkerError(Exception):
    """Base exception for chunker errors"""
    pass

class MemorySafetyError(ChunkerError):
    """Exception raised when memory safety limits would be exceeded"""
    pass

class CircuitBreakerError(ChunkerError):
    """Exception raised when circuit breaker is open"""
    pass

class TimeoutExceededError(ChunkerError):
    """Exception raised when timeout is exceeded"""
    pass


@dataclass
class HealthCheckResult:
    """Result of a system health check"""
    success: bool
    duration: float = 0.0
    error: Optional[str] = None
    memory_usage: float = 0.0
    cpu_usage: float = 0.0


@dataclass
class ResourceInfo:
    """System resource information with container awareness"""
    cpu_count: int 
    logical_cores: int
    system_load: float = 0.0
    memory_total: int = 0
    memory_available: int = 0
    memory_percent: float = 0.0
    swap_percent: float = 0.0
    io_wait: float = 0.0
    disk_usage: float = 0.0
    
    @staticmethod
    def safe_cpu_count() -> int:
        """Robust CPU count with container awareness"""
        try:
            # Check cgroup constraints for containerized environments (Linux)
            if platform.system() == "Linux":
                # Check for cgroups v2
                if os.path.exists("/sys/fs/cgroup/cpu.max"):
                    try:
                        with open("/sys/fs/cgroup/cpu.max") as f:
                            content = f.read().strip()
                            if content != "max":
                                quota, period = map(int, content.split())
                                if quota > 0 and period > 0:
                                    return max(1, quota // period)
                    except (IOError, ValueError):
                        pass
                
                # Check for cgroups v1
                if os.path.exists("/sys/fs/cgroup/cpu/cpu.cfs_quota_us"):
                    try:
                        with open("/sys/fs/cgroup/cpu/cpu.cfs_quota_us") as f:
                            quota = int(f.read())
                        if quota != -1:
                            with open("/sys/fs/cgroup/cpu/cpu.cfs_period_us") as f:
                                period = int(f.read())
                            return max(1, quota // period)
                    except (IOError, ValueError):
                        pass
            
            # Fallback to standard detection
            return os.cpu_count() or 4
        except Exception as e:
            logger.warning(f"Error detecting CPU count: {e}")
            return 4
    
    @classmethod
    def detect(cls) -> 'ResourceInfo':
        """Detect system resources with container awareness"""
        # Enhanced CPU detection
        cpu_count = cls.safe_cpu_count()
        logical_cores = cpu_count  # Default if psutil not available
        
        memory_total = 0
        memory_available = 0
        memory_percent = 0.0
        swap_percent = 0.0
        io_wait = 0.0
        system_load = 0.0
        disk_usage = 0.0
        
        # Enhanced detection with psutil if available
        if HAS_PSUTIL:
            try:
                logical_cores = psutil.cpu_count(logical=True) or logical_cores
                
                # Get memory info
                vm = psutil.virtual_memory()
                memory_total = vm.total
                memory_available = vm.available
                memory_percent = vm.percent
                
                # Check for swap
                try:
                    swap = psutil.swap_memory()
                    swap_percent = swap.percent
                except Exception:
                    pass
                
                # Get system load
                try:
                    if hasattr(psutil, 'getloadavg'):
                        load = psutil.getloadavg()
                        system_load = load[0] / logical_cores
                    elif platform.system() != 'Windows':
                        # Fallback for Unix-like systems
                        try:
                            with open('/proc/loadavg', 'r') as f:
                                system_load = float(f.read().split()[0]) / logical_cores
                        except (IOError, ValueError):
                            pass
                except Exception:
                    pass
                
                # Get IO wait time from CPU stats if available
                try:
                    cpu_times = psutil.cpu_times_percent()
                    if hasattr(cpu_times, 'iowait'):
                        io_wait = cpu_times.iowait
                except Exception:
                    pass
                
                # Get disk usage for the current disk
                try:
                    disk = psutil.disk_usage('/')
                    disk_usage = disk.percent
                except Exception:
                    pass
                
            except Exception as e:
                logger.warning(f"Error detecting system resources: {e}")
        
        # Container memory limits (Linux)
        if memory_total == 0 and platform.system() == "Linux":
            try:
                # Check cgroups v2
                if os.path.exists("/sys/fs/cgroup/memory.max"):
                    with open("/sys/fs/cgroup/memory.max") as f:
                        content = f.read().strip()
                        if content != "max":
                            memory_total = int(content)
                
                # Check cgroups v1
                elif os.path.exists("/sys/fs/cgroup/memory/memory.limit_in_bytes"):
                    with open("/sys/fs/cgroup/memory/memory.limit_in_bytes") as f:
                        memory_total = int(f.read())
            except Exception:
                pass
            
        return cls(
            cpu_count=cpu_count,
            logical_cores=logical_cores,
            system_load=system_load,
            memory_total=memory_total,
            memory_available=memory_available,
            memory_percent=memory_percent,
            swap_percent=swap_percent,
            io_wait=io_wait,
            disk_usage=disk_usage
        )


@dataclass
class PerformanceMetrics:
    """Performance tracking metrics with prediction capabilities"""
    decisions: Dict[str, int] = field(default_factory=lambda: {'simple': 0, 'advanced': 0})
    avg_processing_time: float = 0.0
    last_strategy: Optional[str] = None
    strategy_switches: int = 0
    total_chunks_processed: int = 0
    total_bytes_processed: int = 0
    start_time: float = field(default_factory=time.monotonic)
    last_error_time: Optional[float] = None
    error_count: int = 0
    retry_count: int = 0
    
    # Timing statistics
    processing_times: Deque[float] = field(default_factory=lambda: deque(maxlen=100))
    throughput_history: Deque[float] = field(default_factory=lambda: deque(maxlen=20))
    
    # Batch statistics 
    batch_sizes: Deque[int] = field(default_factory=lambda: deque(maxlen=50))
    worker_utilization: Deque[float] = field(default_factory=lambda: deque(maxlen=20))
    
    # Memory usage tracking
    memory_usage_samples: Deque[float] = field(default_factory=lambda: deque(maxlen=20))
    
    # Processing rate by workload size for predictive scaling
    workload_history: Deque[Tuple[int, float, float]] = field(
        default_factory=lambda: deque(maxlen=50)) # (size, time, throughput)
    
    def add_processing_time(self, elapsed_time: float, bytes_processed: int, chunks_produced: int) -> None:
        """Add processing time statistics"""
        self.processing_times.append(elapsed_time)
        self.total_chunks_processed += chunks_produced
        self.total_bytes_processed += bytes_processed
        
        # Calculate throughput (chunks/second)
        if elapsed_time > 0:
            throughput = chunks_produced / elapsed_time
            self.throughput_history.append(throughput)
            
            # Update workload history for predictive scaling
            self.workload_history.append((bytes_processed, elapsed_time, throughput))
            
        # Update average processing time with exponential smoothing
        self.avg_processing_time = (
            0.9 * self.avg_processing_time + 0.1 * elapsed_time
            if self.avg_processing_time > 0 else elapsed_time
        )
        
        # Update Prometheus metrics if available
        if HAS_PROMETHEUS:
            CHUNKER_METRICS['throughput'].set(throughput if elapsed_time > 0 else 0)
            CHUNKER_METRICS['processing_time'].observe(elapsed_time)
    
    def record_decision(self, strategy: str, previous_strategy: Optional[str]) -> None:
        """Record a strategy decision"""
        self.decisions[strategy] += 1
        self.last_strategy = strategy
        
        if previous_strategy and previous_strategy != strategy:
            self.strategy_switches += 1
    
    def record_error(self) -> None:
        """Record an error occurrence"""
        self.error_count += 1
        self.last_error_time = time.monotonic()
        
        # Update Prometheus metrics if available
        if HAS_PROMETHEUS:
            CHUNKER_METRICS['error_count'].inc()
    
    def record_retry(self) -> None:
        """Record a retry attempt"""
        self.retry_count += 1
        
        # Update Prometheus metrics if available
        if HAS_PROMETHEUS:
            CHUNKER_METRICS['retry_count'].inc()
    
    def record_batch_size(self, size: int) -> None:
        """Record a batch size used"""
        self.batch_sizes.append(size)
        
        # Update Prometheus metrics if available
        if HAS_PROMETHEUS:
            CHUNKER_METRICS['batch_size'].set(size)
    
    def record_worker_utilization(self, utilization: float) -> None:
        """Record worker utilization"""
        self.worker_utilization.append(utilization)
        
        # Update Prometheus metrics if available
        if HAS_PROMETHEUS:
            CHUNKER_METRICS['worker_util'].set(utilization * 100)  # Convert to percentage
    
    def record_memory_usage(self, usage: float) -> None:
        """Record memory usage percentage"""
        self.memory_usage_samples.append(usage)
        
        # Update Prometheus metrics if available
        if HAS_PROMETHEUS:
            CHUNKER_METRICS['memory_usage'].set(usage)
    
    def predict_throughput(self, planned_workload: int) -> float:
        """
        Predict processing throughput for a given workload size
        using historical data and regression analysis.
        
        Args:
            planned_workload: Size of planned workload in bytes
            
        Returns:
            Predicted throughput (chunks/second)
        """
        if not self.workload_history:
            return 0.0
            
        # Use linear regression with NumPy if available
        if HAS_NUMPY and len(self.workload_history) >= 3:
            try:
                # Extract features and target values
                sizes = np.array([entry[0] for entry in self.workload_history])
                throughputs = np.array([entry[2] for entry in self.workload_history])
                
                # Simple linear regression
                slope, intercept = np.polyfit(sizes, throughputs, 1)
                
                # Predict throughput for planned workload
                predicted = slope * planned_workload + intercept
                
                # Ensure prediction is reasonable
                min_throughput = min(throughputs)
                max_throughput = max(throughputs)
                
                return max(min_throughput * 0.8, min(max_throughput * 1.2, predicted))
            except Exception as e:
                logger.debug(f"Error in throughput prediction: {e}")
                # Fall back to simple average
                pass
        
        # Simple average if prediction fails or NumPy not available
        return statistics.mean([entry[2] for entry in self.workload_history])
    
    def get_avg_throughput(self) -> float:
        """Get average throughput (chunks/second)"""
        if not self.throughput_history:
            return 0.0
        return sum(self.throughput_history) / len(self.throughput_history)
    
    def get_avg_batch_size(self) -> float:
        """Get average batch size"""
        if not self.batch_sizes:
            return 0.0
        return sum(self.batch_sizes) / len(self.batch_sizes)
    
    def get_uptime(self) -> float:
        """Get uptime in seconds"""
        return time.monotonic() - self.start_time
    
    def get_error_rate(self) -> float:
        """Get error rate (errors per minute)"""
        uptime_minutes = self.get_uptime() / 60
        if uptime_minutes > 0:
            return self.error_count / uptime_minutes
        return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert metrics to dictionary"""
        result = {
            'decisions': self.decisions.copy(),
            'avg_processing_time': self.avg_processing_time,
            'last_strategy': self.last_strategy,
            'strategy_switches': self.strategy_switches,
            'total_chunks_processed': self.total_chunks_processed,
            'total_bytes_processed': self.total_bytes_processed,
            'uptime_seconds': self.get_uptime(),
            'error_count': self.error_count,
            'retry_count': self.retry_count,
            'avg_throughput': self.get_avg_throughput(),
            'avg_batch_size': self.get_avg_batch_size(),
        }
        
        # Add statistics if we have enough data
        if len(self.processing_times) >= 5:
            result.update({
                'min_processing_time': min(self.processing_times),
                'max_processing_time': max(self.processing_times),
                'p90_processing_time': np.percentile(list(self.processing_times), 90) if HAS_NUMPY else sorted(self.processing_times)[int(len(self.processing_times) * 0.9)],
            })
            
        if self.throughput_history:
            result['throughput_trend'] = list(self.throughput_history)
            
        # Add memory metrics if available
        if self.memory_usage_samples:
            result['avg_memory_usage'] = statistics.mean(self.memory_usage_samples)
            result['max_memory_usage'] = max(self.memory_usage_samples)
            
        return result


class DynamicConfig:
    """Dynamic configuration with remote update capabilities"""
    
    def __init__(self, initial_config: Optional[Dict[str, Any]] = None):
        """Initialize with optional initial configuration"""
        self._config = initial_config or {}
        self._lock = threading.RLock()
        self._updated_at = time.monotonic()
        self._update_interval = 300.0  # Default 5 minutes
        self._auto_update = False
        self._update_url = None
        self._update_thread = None
        
    def get(self, key: str, default: Any = None) -> Any:
        """Get a configuration value with default fallback"""
        with self._lock:
            return self._config.get(key, default)
    
    def set(self, key: str, value: Any) -> None:
        """Set a configuration value"""
        with self._lock:
            self._config[key] = value
            self._updated_at = time.monotonic()
    
    def update(self, config: Dict[str, Any]) -> None:
        """Update multiple configuration values"""
        with self._lock:
            self._config.update(config)
            self._updated_at = time.monotonic()
    
    def update_from_url(self, url: str) -> bool:
        """
        Update configuration from a remote URL.
        
        Args:
            url: URL to fetch configuration from
            
        Returns:
            Success status
        """
        try:
            import requests
            response = requests.get(url, timeout=10.0)
            response.raise_for_status()
            config = response.json()
            
            with self._lock:
                self._config.update(config)
                self._updated_at = time.monotonic()
                
            logger.info(f"Updated configuration from {url}")
            return True
        except ImportError:
            logger.error("Requests library not installed - cannot update from URL")
            return False
        except Exception as e:
            logger.error(f"Failed to update configuration from {url}: {e}")
            return False
    
    def start_auto_update(self, url: str, interval: float = 300.0) -> bool:
        """
        Start auto-updating configuration from a URL.
        
        Args:
            url: URL to fetch configuration from
            interval: Update interval in seconds
            
        Returns:
            Success status
        """
        try:
            import requests
        except ImportError:
            logger.error("Requests library not installed - cannot auto-update from URL")
            return False
            
        if self._auto_update and self._update_thread and self._update_thread.is_alive():
            logger.warning("Auto-update already running")
            return False
            
        self._update_url = url
        self._update_interval = interval
        self._auto_update = True
        
        def update_loop():
            while self._auto_update:
                try:
                    self.update_from_url(self._update_url)
                except Exception as e:
                    logger.error(f"Error in config update loop: {e}")
                
                # Sleep for interval or until auto_update is disabled
                for _ in range(int(self._update_interval / 0.1)):
                    if not self._auto_update:
                        break
                    time.sleep(0.1)
        
        self._update_thread = threading.Thread(
            target=update_loop,
            daemon=True,
            name="config-updater"
        )
        self._update_thread.start()
        
        logger.info(f"Started auto-update from {url} every {interval}s")
        return True
    
    def stop_auto_update(self) -> None:
        """Stop auto-updating configuration"""
        self._auto_update = False
        if self._update_thread and self._update_thread.is_alive():
            self._update_thread.join(timeout=1.0)
            logger.info("Stopped configuration auto-update")
    
    def to_dict(self) -> Dict[str, Any]:
        """Get a copy of the entire configuration"""
        with self._lock:
            return self._config.copy()
    
    def __str__(self) -> str:
        """String representation with update timestamp"""
        from datetime import datetime
        update_time = datetime.fromtimestamp(self._updated_at).strftime('%Y-%m-%d %H:%M:%S')
        return f"DynamicConfig(entries={len(self._config)}, updated={update_time})"


# Global configuration instance
CONFIG = DynamicConfig({
    'processing_timeout': DEFAULT_TIMEOUT,
    'max_retries': MAX_RETRIES,
    'memory_safety': True,
    'dynamic_batch_sizing': True,
    'health_check_interval': HEALTH_CHECK_INTERVAL,
})


class SmartParallelChunker:
    """
    Enterprise-grade adaptive parallel chunking system that automatically selects 
    between simple and advanced chunking strategies based on workload characteristics,
    system resources, and real-time performance monitoring.
    
    Features:
    - Self-tuning workload complexity estimation
    - Dynamic batch sizing and worker scaling
    - System resource monitoring and adaptation
    - Comprehensive error handling and recovery
    - Circuit breaker pattern for fault tolerance
    - Performance metrics and health monitoring
    - Graceful degradation under pressure
    - Memory safety management
    - Dynamic configuration updates
    - Quality of Service (QoS) prioritization
    - Prometheus metrics integration
    - Predictive resource scaling
    - Container awareness
    """
    
    def __init__(
        self,
        options: ChunkingOptions,
        size_threshold: int = 100_000,  # 100KB
        complexity_threshold: float = 0.5,
        sample_size: int = 1000,  # 1KB sample for complexity estimation
        force_strategy: Optional[str] = None,  # 'simple', 'advanced', or None for auto
        timeout: float = DEFAULT_TIMEOUT,
        max_retries: int = MAX_RETRIES,
        memory_safety: bool = True,
        adaptive_batch_sizing: bool = True,
        health_check_enabled: bool = True,
        worker_count_override: Optional[int] = None,
        resource_monitor_interval: float = 5.0,
        config: Optional[DynamicConfig] = None,
    ):
        """
        Initialize the enterprise-grade smart chunker system.
        
        Args:
            options: Chunking configuration options
            size_threshold: Text size threshold for automatic selection (bytes)
            complexity_threshold: Processing complexity threshold for selection
            sample_size: Size of sample text for complexity estimation
            force_strategy: Force a specific strategy ('simple', 'advanced', or None for auto)
            timeout: Operation timeout in seconds
            max_retries: Maximum number of retry attempts for failures
            memory_safety: Enable memory safety monitoring and protection
            adaptive_batch_sizing: Enable dynamic batch size adjustments
            health_check_enabled: Enable background health monitoring
            worker_count_override: Override auto-detected worker counts
            resource_monitor_interval: Resource monitoring interval in seconds
            config: Optional dynamic configuration instance
        """
        self.options = options
        self.size_threshold = size_threshold
        self.complexity_threshold = complexity_threshold
        self.sample_size = sample_size
        self.force_strategy = force_strategy
        self.timeout = timeout
        self.max_retries = max_retries
        self.memory_safety = memory_safety
        self.adaptive_batch_sizing = adaptive_batch_sizing
        self.health_check_enabled = health_check_enabled
        self.worker_count_override = worker_count_override
        self.resource_monitor_interval = resource_monitor_interval
        self.config = config or CONFIG
        
        # Initialize system resource information
        self.resources = ResourceInfo.detect()
        
        # Performance metrics and state
        self.metrics = PerformanceMetrics()
        self.health_status = "INITIALIZING"
        self.circuit_breaker_status = "CLOSED"  # CLOSED, HALF_OPEN, OPEN
        self.circuit_breaker_reset_time = None
        self.last_health_check = time.monotonic()
        
        # Thread safety
        self._lock = RLock()
        self._shutdown_event = Event()
        # Replace Event with Condition for resource updates to avoid missed signals
        self._resource_update_condition = Condition()
        
        # QoS settings
        self._current_priority = 'normal'
        self._priority_context_stack = []
        
        # Initialize memory management
        self._memory_manager = MemoryManager(low_memory_mode=memory_safety)
        self._memory_monitor = MemoryMonitor()
        
        # Initialize streaming buffer
        self._streaming_buffer = StreamingBuffer(
            buffer_size=options.stream_buffer_size if hasattr(options, 'stream_buffer_size') else 100000,
            overlap_size=options.overlap_tokens * 4 if hasattr(options, 'overlap_tokens') else 5000
        )
        
        # Initialize chunkers with optimized settings
        self.simple_chunker = self._create_simple_chunker()
        self.advanced_chunker = self._create_advanced_chunker()
        
        # Start background tasks if enabled
        if self.health_check_enabled:
            self._start_background_monitoring()
        
        # Auto-tune based on initial system state
        self._auto_tune_thresholds()
        
        # Initialize memory safety tracking
        self._last_memory_check = time.monotonic()
        self._memory_estimate_cache = {}
        
        # Start metrics server if Prometheus is available
        if HAS_PROMETHEUS and CONFIG.get('start_metrics_server', False):
            self._start_metrics_server()
        
        logger.info(
            f"SmartParallelChunker initialized on {platform.system()} with "
            f"{self.resources.logical_cores} logical cores, "
            f"size_threshold={size_threshold}, "
            f"complexity_threshold={complexity_threshold}, "
            f"memory_safety={memory_safety}"
        )
    
    def _create_simple_chunker(self) -> SimpleParallelChunker:
        """Create an optimized simple chunker instance"""
        # Determine optimal worker count for simple case
        if self.worker_count_override is not None:
            workers = self.worker_count_override
        else:
            # For simple chunker, thread count can be higher (I/O bound work)
            workers = min(
                self.resources.logical_cores * 4,  # 4x logical cores
                32,  # Cap at reasonable maximum
                int((self.resources.memory_available or 8*1024*1024*1024) // THREAD_STACK_SIZE)  # Memory-aware scaling
            )
        
        logger.debug(f"Creating simple chunker with {workers} workers")
        
        # Create optimized simple chunker
        return SimpleParallelChunker(
            options=self.options,
            max_workers=workers,
            use_processes=False  # Always use threads for simple case
        )
    
    def _create_advanced_chunker(self):
        """
        Create an optimized advanced chunker instance
        
        Dynamically imports the advanced chunker to avoid circular imports
        """
        # We'll pass our detected resources to advanced chunker
        # Advanced implementation will use its own logic for optimal workers
        logger.debug("Creating advanced chunker with auto-configuration")
        
        # Check if the advanced chunker is available
        try:
            # Try to import the advanced chunker
            from enterprise_chunker.utils.advanced_parallel import ParallelChunker as AdvancedParallelChunker
            
            # Create optimized advanced chunker with our custom settings
            return AdvancedParallelChunker(options=self.options)
        except ImportError:
            # Use debug level instead of warning for this common case
            logger.debug("Advanced chunker not available, falling back to simple chunker")
            
            # Return a simple chunker with different settings for "advanced" mode
            return SimpleParallelChunker(
                options=self.options,
                max_workers=self.worker_count_override or max(1, (self.resources.logical_cores or 4) * 2),
                use_processes=True  # Use processes for "advanced" mode
            )
    
    def _start_background_monitoring(self) -> None:
        """Start background health and resource monitoring"""
        # Resource monitoring thread
        self._resource_monitor = threading.Thread(
            target=self._resource_monitoring_loop,
            daemon=True,
            name="resource-monitor"
        )
        self._resource_monitor.start()
        
        logger.debug("Background monitoring started")
    
    def _start_metrics_server(self, port: int = 8000) -> None:
        """Start Prometheus metrics server if available"""
        if not HAS_PROMETHEUS:
            logger.warning("Prometheus client not available - metrics server not started")
            return
            
        try:
            metrics_port = CONFIG.get('metrics_port', port)
            start_http_server(metrics_port)
            logger.info(f"Metrics server started on port {metrics_port}")
        except Exception as e:
            logger.error(f"Failed to start metrics server: {e}")
    
    def _resource_monitoring_loop(self) -> None:
        """Background resource monitoring loop"""
        while not self._shutdown_event.is_set():
            try:
                # Update resource information
                self.resources = ResourceInfo.detect()
                
                # Check system health
                self._check_system_health()
                
                # Update memory usage metric
                if HAS_PSUTIL:
                    try:
                        memory_usage = self._monitor_memory_usage()
                        self.metrics.record_memory_usage(memory_usage)
                    except Exception as e:
                        logger.debug(f"Error monitoring memory: {e}")
                
                # Check circuit breaker if in HALF_OPEN state
                if self.circuit_breaker_status == "HALF_OPEN":
                    self._circuit_breaker_check()
                
                # Signal resource update to any waiting threads using Condition
                with self._resource_update_condition:
                    self._resource_update_condition.notify_all()
                
                # Wait for next check interval
                self._shutdown_event.wait(self.resource_monitor_interval)
            except Exception as e:
                logger.error(f"Error in resource monitoring: {e}")
                # Don't crash the monitor thread, just wait and retry
                time.sleep(max(1.0, self.resource_monitor_interval / 2))
    
    def _circuit_breaker_check(self) -> None:
        """
        Advanced circuit breaker state management with health check.
        Tests system recovery with a synthetic workload before fully closing
        the circuit breaker.
        """
        # Don't run health check too frequently
        now = time.monotonic()
        if self.last_health_check and now - self.last_health_check < 5.0:
            return
            
        self.last_health_check = now
        
        # Execute a controlled health check task
        test_result = self._run_health_check_task()
        
        if test_result.success:
            # System seems healthy, close the circuit
            logger.info(f"Circuit breaker health check passed in {test_result.duration:.2f}s - resetting to CLOSED")
            self.circuit_breaker_status = "CLOSED"
        else:
            # System still has issues, extend outage
            logger.warning(f"Circuit breaker health check failed: {test_result.error} - keeping HALF_OPEN")
            self._trip_circuit_breaker()
    
    def _run_health_check_task(self) -> HealthCheckResult:
        """
        Execute a controlled health check with a synthetic workload.
        
        Returns:
            HealthCheckResult: Results of the health check
        """
        # Create a simple synthetic workload
        test_text = "x " * 1000  # 2KB synthetic text
        
        start_time = time.monotonic()
        memory_usage = self._monitor_memory_usage()
        cpu_usage = 0.0
        
        if HAS_PSUTIL:
            try:
                # Get CPU usage for this process
                process = psutil.Process(os.getpid())
                cpu_usage = process.cpu_percent(interval=0.1)
            except Exception:
                pass
        
        try:
            # Use a short timeout for the test
            with self._timeout_context(5.0):
                # Try to process with simple chunker
                result = self.simple_chunker.chunk_segments([test_text], lambda x: [x])
                duration = time.monotonic() - start_time
                
                # Check if result is valid
                if result and len(result) > 0:
                    return HealthCheckResult(
                        success=True,
                        duration=duration,
                        memory_usage=memory_usage,
                        cpu_usage=cpu_usage
                    )
                else:
                    return HealthCheckResult(
                        success=False,
                        duration=duration,
                        error="Empty result from test chunking",
                        memory_usage=memory_usage,
                        cpu_usage=cpu_usage
                    )
        except Exception as e:
            duration = time.monotonic() - start_time
            return HealthCheckResult(
                success=False,
                duration=duration,
                error=str(e),
                memory_usage=memory_usage,
                cpu_usage=cpu_usage
            )
    
    def _check_system_health(self) -> None:
        """Enhanced system health check with multiple indicators"""
        # Skip if psutil not available
        if not HAS_PSUTIL:
            self.health_status = "UNKNOWN"
            return
        
        try:
            # Check CPU load
            high_load = self.resources.system_load > 0.9
            
            # Check memory pressure
            high_memory = self.resources.memory_percent > 90
            high_swap = self.resources.swap_percent > 80
            
            # Check disk pressure
            high_disk = self.resources.disk_usage > 95
            high_io = self.resources.io_wait > 30
            
            # Determine overall health
            if high_memory and high_load and (high_swap or high_io):
                self.health_status = "CRITICAL"
            elif high_memory or (high_load and high_swap):
                self.health_status = "WARNING"
            elif high_disk or high_io:
                self.health_status = "DEGRADED"
            else:
                self.health_status = "HEALTHY"
                
            # Reset circuit breaker if in HALF_OPEN and health is good
            if (self.circuit_breaker_status == "HALF_OPEN" and 
                self.health_status in ("HEALTHY", "DEGRADED")):
                self.circuit_breaker_status = "CLOSED"
                logger.info("Circuit breaker reset to CLOSED state")
                
            # Log health state changes
            if hasattr(self, '_last_health_status') and self._last_health_status != self.health_status:
                logger.info(f"System health changed: {self._last_health_status} -> {self.health_status}")
                
            self._last_health_status = self.health_status
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            self.health_status = "UNKNOWN"
    
    def _auto_tune_thresholds(self) -> None:
        """Auto-tune thresholds based on system capabilities"""
        # Adjust size threshold based on available memory
        if self.resources.memory_total and self.memory_safety:
            # Higher memory = higher threshold (process more in simple chunker)
            mem_gb = self.resources.memory_total / (1024**3)
            if mem_gb > 32:  # High memory system
                self.size_threshold = max(self.size_threshold, 250_000)
            elif mem_gb < 4:  # Low memory system
                self.size_threshold = min(self.size_threshold, 50_000)
        
        # Adjust complexity threshold based on CPU power
        if self.resources.cpu_count > 8:  # High core count
            # Be more aggressive with advanced chunker on powerful systems
            self.complexity_threshold = max(0.3, self.complexity_threshold - 0.1)
        elif self.resources.cpu_count <= 2:  # Low core count
            # Be more conservative with advanced chunker on weak systems
            self.complexity_threshold = min(0.8, self.complexity_threshold + 0.2)
            
        logger.debug(
            f"Auto-tuned thresholds: size_threshold={self.size_threshold}, "
            f"complexity_threshold={self.complexity_threshold}"
        )
    
    def _monitor_memory_usage(self) -> float:
        """
        Enhanced memory usage monitoring.
        
        Returns:
            Memory usage percentage (0-100)
        """
        if not HAS_PSUTIL:
            return 0.0
            
        try:
            # Get current process memory usage
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            
            # Get system memory information
            system_memory = psutil.virtual_memory()
            
            # Calculate percentage of total system memory used by this process
            process_percent = (memory_info.rss / system_memory.total) * 100
            
            return process_percent
        except Exception as e:
            logger.warning(f"Error monitoring memory: {e}")
            return 0.0
    
    @contextmanager
    def _timeout_context(self, seconds: float):
        """
        Context manager that raises TimeoutError if execution takes longer than specified time.
        Works on all platforms, including Windows.
        
        Args:
            seconds: Timeout in seconds
        """
        # For zero or negative timeout, don't apply timeout
        if seconds <= 0:
            yield
            return
            
        # Use signal-based implementation on Unix systems
        if platform.system() != 'Windows':
            def _timeout_handler(signum, frame):
                raise TimeoutError(f"Operation timed out after {seconds} seconds")
                
            import signal
            original_handler = signal.getsignal(signal.SIGALRM)
            try:
                signal.signal(signal.SIGALRM, _timeout_handler)
                signal.setitimer(signal.ITIMER_REAL, seconds)
                yield
            finally:
                signal.setitimer(signal.ITIMER_REAL, 0)
                signal.signal(signal.SIGALRM, original_handler)
        else:
            # Thread-based timeout for Windows
            timeout_event = threading.Event()
            result = {"timeout": False}
            
            def _timeout_thread():
                time.sleep(seconds)
                if not timeout_event.is_set():
                    result["timeout"] = True
                    
            timer = threading.Thread(target=_timeout_thread, daemon=True)
            timer.start()
            
            try:
                yield
            finally:
                timeout_event.set()
                # Always join the timer thread to avoid orphan threads
                timer.join(timeout=0.1)
                if result["timeout"]:
                    raise TimeoutError(f"Operation timed out after {seconds} seconds")
    
    def _is_circuit_breaker_open(self) -> bool:
        """Check if circuit breaker is open"""
        if self.circuit_breaker_status == "OPEN":
            # Check if it's time to try again
            if self.circuit_breaker_reset_time and time.monotonic() >= self.circuit_breaker_reset_time:
                self.circuit_breaker_status = "HALF_OPEN"
                logger.info("Circuit breaker changed to HALF_OPEN state")
                return False
            return True
            
        return False
    
    def _trip_circuit_breaker(self) -> None:
        """Trip the circuit breaker due to system issues"""
        self.circuit_breaker_status = "OPEN"
        # Set reset time - exponential backoff based on error count
        backoff = min(60, 2 ** min(self.metrics.error_count, 6))  # Cap at 60 seconds
        self.circuit_breaker_reset_time = time.monotonic() + backoff
        logger.warning(f"Circuit breaker OPEN, will retry in {backoff}s")
    
    def _calculate_optimal_batch_size(self) -> int:
        """
        Calculate optimal batch size based on performance history and system state.
        
        Returns:
            Optimal batch size
        """
        if not self.adaptive_batch_sizing or not self.metrics.batch_sizes:
            # Apply QoS priority multiplier to default batch size
            return self._apply_priority_multiplier(DEFAULT_BATCH_SIZE)
            
        # Get recent batch sizes and their performance
        recent_sizes = list(self.metrics.batch_sizes)
        recent_throughput = list(self.metrics.throughput_history)
        
        # If we don't have enough history, use default
        if len(recent_throughput) < 2:
            return self._apply_priority_multiplier(DEFAULT_BATCH_SIZE)
            
        # Start with current average
        current_avg = self.metrics.get_avg_batch_size()
        
        # Check if throughput is improving or declining
        if len(recent_throughput) >= 3:
            # Calculate trend
            trend = recent_throughput[-1] - statistics.mean(recent_throughput[:-1])
            
            # If improving, try larger batch
            if trend > 0:
                new_size = min(int(current_avg * 1.2) + 1, MAX_BATCH_SIZE)
            # If declining, try smaller batch
            elif trend < -DYNAMIC_SCALING_THRESHOLD:
                new_size = max(int(current_avg * 0.8), 1)
            else:
                new_size = max(1, min(MAX_BATCH_SIZE, int(current_avg)))
        else:
            # Default case - stick with current size with small adjustments
            new_size = max(1, min(MAX_BATCH_SIZE, int(current_avg)))
            
        # Apply QoS priority multiplier
        return self._apply_priority_multiplier(new_size)
    
    def _apply_priority_multiplier(self, size: int) -> int:
        """Apply priority multiplier to batch size based on current QoS priority"""
        priority_info = PRIORITY_LEVELS.get(self._current_priority, PRIORITY_LEVELS['normal'])
        multiplier = priority_info.get('batch_size_multiplier', 1.0)
        
        return max(1, int(size * multiplier))
    
    @contextmanager
    def _priority_context(self, priority: str):
        """
        Context manager for temporary priority change.
        
        Args:
            priority: Priority level ('high', 'normal', 'background')
        """
        if priority not in PRIORITY_LEVELS:
            raise ValueError(f"Invalid priority: {priority}")
            
        previous_priority = self._current_priority
        self._priority_context_stack.append(previous_priority)
        self._current_priority = priority
        
        try:
            yield
        finally:
            # Restore previous priority
            if self._priority_context_stack:
                self._current_priority = self._priority_context_stack.pop()
            else:
                self._current_priority = 'normal'
                
    def _select_strategy(self, text: str, chunker_func: ChunkerFunc) -> str:
        """
        Advanced strategy selection based on multiple factors including:
        - Workload characteristics
        - System resources
        - Performance history
        - Circuit breaker status
        - Memory safety predictions
        
        Args:
            text: Text to chunk
            chunker_func: Function that chunks text
            
        Returns:
            Strategy name ('simple' or 'advanced')
        """
        # Check circuit breaker first
        if self._is_circuit_breaker_open():
            # When circuit breaker is open, use simple strategy
            logger.warning("Circuit breaker OPEN - forcing simple strategy")
            return 'simple'
        
        # Check if strategy is forced
        if self.force_strategy:
            return self.force_strategy
        
        # Check system health
        if self.health_status in ("CRITICAL", "WARNING") and self.memory_safety:
            # Use simple strategy under system pressure
            logger.warning(f"System health {self.health_status} - using simple strategy")
            return 'simple'
            
        # 1. Size-based selection (small texts use simple strategy)
        if len(text) < self.size_threshold:
            return 'simple'
            
        # 2. For larger texts, estimate complexity
        complexity = self._estimate_complexity(text, chunker_func)
        logger.debug(f"Estimated complexity: {complexity:.4f}")
        
        # 3. Check memory safety (for very large texts)
        if self.memory_safety and len(text) > 10 * self.size_threshold:
            memory_usage = self._monitor_memory_usage()
            # If memory usage is high, use simple strategy to be safe
            if memory_usage > 80:
                logger.warning(f"High memory usage ({memory_usage:.1f}%) - using simple strategy")
                return 'simple'
                
        # 4. Check historical performance
        if (self.metrics.last_strategy == 'advanced' and 
            self.metrics.error_count > 3 and 
            self.metrics.get_error_rate() > 0.5):
            # If advanced strategy has been failing, switch to simple
            logger.warning("High error rate with advanced strategy - switching to simple")
            return 'simple'
            
        # 5. Standard complexity-based selection
        if complexity < self.complexity_threshold:
            return 'simple'
        else:
            return 'advanced'
    
    def _estimate_complexity(self, text: str, chunker_func: ChunkerFunc) -> float:
        """
        Estimate processing complexity of text using a combination of
        token count, structure analysis, and sample processing.
        
        Args:
            text: Text to chunk
            chunker_func: Function that chunks text
            
        Returns:
            Complexity score (higher = more complex)
        """
        if not text:
            return 0.0
            
        # Use the token estimator for a quick assessment of content density
        tokens = estimate_tokens(text[:min(len(text), 5000)], TokenEstimationStrategy.BALANCED)
        tokens_per_char = tokens / min(len(text), 5000)
        
        # Detect content format for more intelligent complexity estimation
        content_format = detect_content_format(text[:min(len(text), 2500)])
        
        # Take a sample of text for performance estimation
        sample_size = min(self.sample_size, len(text))
        sample = text[:sample_size]
        
        # Time the chunking operation on the sample
        try:
            start_time = time.monotonic()
            
            # Guard against empty sample - Fix: Ensure sample is not empty before calling chunker_func
            if sample:
                chunks = chunker_func(sample)
            else:
                chunks = []
                
            processing_time = time.monotonic() - start_time
            
            # Normalize by sample size and estimate for full text
            normalized_time = processing_time / max(1, len(sample))  # Avoid division by zero
            estimated_full_time = normalized_time * len(text)
            
            # Apply scaling factor based on content format and size
            scaling_factor = 1.0
            
            # Adjust for different content types using enum values instead of string literals
            if content_format == ContentFormat.CODE:
                scaling_factor = 1.3  # Code tends to be more complex
            elif content_format == ContentFormat.JSON:
                scaling_factor = 1.2  # JSON has nested structure
            elif content_format == ContentFormat.MARKDOWN:
                scaling_factor = 1.1  # Markdown has formatting
                
            # Adjust for document size (larger texts often have non-linear complexity)
            size_factor = min(2.0, 1.0 + 0.2 * (math.log2(max(1, len(text) / max(1, sample_size)))))
            
            return estimated_full_time * scaling_factor * size_factor
        except Exception as e:
            logger.warning(f"Error during complexity estimation: {e}")
            # Default to threshold if estimation fails
            return self.complexity_threshold
    
    def chunk(self, text: str, chunker_func: ChunkerFunc) -> List[str]:
        """
        Enterprise-grade parallel chunking with automatic strategy selection,
        error recovery, and performance optimization.
        
        Features:
        - Self-selecting strategy based on workload and system state
        - Automatic retry with fallback on errors
        - Circuit breaker pattern for system protection
        - Comprehensive performance metrics
        - Memory safety enforcement
        - Predictive resource usage
        
        Args:
            text: Text to chunk
            chunker_func: Function that chunks text
            
        Returns:
            List of processed chunks
        """
        # Empty input check
        if not text:
            return []
        
        # Detect dynamic config updates
        timeout = self.config.get('processing_timeout', self.timeout)
        max_retries = self.config.get('max_retries', self.max_retries)
        
        # Predict resource usage
        text_size = len(text)
        if text_size > self.size_threshold * 5:
            # For large texts, predict throughput
            predicted_throughput = self.metrics.predict_throughput(text_size)
            if predicted_throughput > 0:
                estimated_time = text_size / (predicted_throughput * 1000)  # Rough estimate
                logger.info(f"Predicted processing time: {estimated_time:.1f}s for {text_size:,} chars")
            
            # Use memory manager for large texts
            with self._memory_manager.memory_efficient_context():
                return self._chunk_with_memory_safety(text, chunker_func, timeout, max_retries)
        else:
            # Process normally for smaller texts
            return self._chunk_with_memory_safety(text, chunker_func, timeout, max_retries)
    
    def _chunk_with_memory_safety(
        self, 
        text: str, 
        chunker_func: ChunkerFunc, 
        timeout: float, 
        max_retries: int
    ) -> List[str]:
        """Inner implementation of chunk with memory safety controls"""
        # Track full processing time
        start_time = time.monotonic()
        previous_strategy = self.metrics.last_strategy
        retry_count = 0
        chunks = []
        
        # Keep overall processing under timeout
        try:
            with self._timeout_context(timeout):
                try:
                    # Select strategy
                    strategy = self._select_strategy(text, chunker_func)
                    logger.info(f"Selected '{strategy}' strategy for text of length {len(text):,}")
                    
                    # Record decision
                    self.metrics.record_decision(strategy, previous_strategy)
                    
                    # Process with selected strategy
                    if strategy == 'simple':
                        chunks = self._process_with_simple(text, chunker_func)
                    else:
                        chunks = self._process_with_advanced(text, chunker_func)
                    
                except Exception as e:
                    # First error - record and retry
                    self.metrics.record_error()
                    err_name = type(e).__name__
                    logger.warning(f"Error ({err_name}) processing with {strategy} strategy: {e}")
                    
                    # Circuit breaker check
                    if self.health_status in ("CRITICAL", "WARNING"):
                        self._trip_circuit_breaker()
                
                    # Retry with simple strategy if we were using advanced
                    if strategy == 'advanced' and retry_count < max_retries:
                        retry_count += 1
                        self.metrics.record_retry()
                        logger.info(f"Retrying with simple strategy (attempt {retry_count})")
                        
                        try:
                            # Fallback to simple strategy
                            chunks = self._process_with_simple(text, chunker_func)
                        except Exception as retry_e:
                            # Record retry failure
                            logger.error(f"Retry failed: {type(retry_e).__name__}: {retry_e}")
                            # Re-raise original error
                            raise e
                    else:
                        # No more retries or already using simple - re-raise
                        raise
        except TimeoutError:
            # Global timeout - this is serious
            logger.error(f"Global timeout ({timeout}s) exceeded while processing text of length {len(text):,}")
            self.metrics.record_error()
            self._trip_circuit_breaker()
            
            # Try one last desperate attempt with simple strategy on a portion of the text
            if len(text) > self.size_threshold:
                logger.warning("Attempting emergency processing of first section only")
                try:
                    # Process just the first section as a fallback
                    first_section = text[:min(len(text), self.size_threshold)]
                    chunks = self._process_with_simple(first_section, chunker_func)
                    logger.warning(f"Emergency processing returned {len(chunks)} chunks from first {len(first_section):,} chars")
                except Exception as e:
                    logger.error(f"Emergency processing failed: {e}")
                    chunks = []
            else:
                chunks = []
        except Exception as e:
            # Unhandled exception
            logger.error(f"Unhandled exception: {type(e).__name__}: {e}")
            self.metrics.record_error()
            self._trip_circuit_breaker()
            
            # Try to get a stack trace
            logger.error(f"Stack trace: {traceback.format_exc()}")
            chunks = []
        finally:
            # Calculate processing time and update metrics
            processing_time = time.monotonic() - start_time
            self.metrics.add_processing_time(
                elapsed_time=processing_time,
                bytes_processed=len(text),
                chunks_produced=len(chunks)
            )
            
            # Log results
            logger.info(
                f"Processed {len(text):,} chars in {processing_time:.2f}s "
                f"producing {len(chunks)} chunks (strategy: {self.metrics.last_strategy})"
            )
            
            # Memory safety cleanup for large texts
            if len(text) > self.size_threshold * 5 and self.memory_safety:
                self._memory_manager.reduce_memory_usage(force=True)
                
        return chunks
    
    def _process_with_simple(self, text: str, chunker_func: ChunkerFunc) -> List[str]:
        """
        Process text with the simple chunker - enhanced with dynamic batch sizing
        and optimized segmentation.
        
        Args:
            text: Text to chunk
            chunker_func: Function that chunks text
            
        Returns:
            List of processed chunks
        """
        # Calculate batch size
        batch_size = self._calculate_optimal_batch_size()
        self.metrics.record_batch_size(batch_size)
        
        # For simple chunker, break into optimally sized segments
        segments = self._split_into_segments(text)
        
        # Monitor resource utilization during processing
        start_mem = self._monitor_memory_usage() if self.memory_safety else 0
        
        # Process segments in batches
        results = []
        for i in range(0, len(segments), batch_size):
            batch = segments[i:i+batch_size]
            
            # Check for shutdown signal
            if self._shutdown_event.is_set():
                logger.warning("Shutdown signaled - interrupting processing")
                break
                
            # Process batch
            batch_results = self.simple_chunker.chunk_segments(batch, chunker_func)
            results.extend(batch_results)
            
            # Check memory pressure and adjust batch size if needed
            if self.memory_safety and i < len(segments) - batch_size:
                current_mem = self._monitor_memory_usage()
                
                # If memory usage grew significantly, reduce batch size
                if current_mem > start_mem + 10 and batch_size > 1:  # 10% increase
                    new_batch_size = max(1, batch_size // 2)
                    logger.warning(
                        f"Memory pressure detected ({current_mem:.1f}%) - "
                        f"reducing batch size from {batch_size} to {new_batch_size}"
                    )
                    batch_size = new_batch_size
                    self.metrics.record_batch_size(batch_size)
        
        return results
    
    def _process_with_advanced(self, text: str, chunker_func: ChunkerFunc) -> List[str]:
        """
        Process text with the advanced chunker - enhanced with fallback mechanisms,
        health monitoring, and circuit breaker protection.
        
        Args:
            text: Text to chunk
            chunker_func: Function that chunks text
            
        Returns:
            List of processed chunks
        """
        try:
            # Set a reasonable timeout for the advanced processing
            with self._timeout_context(self.timeout * 0.8):
                # Check system health before proceeding
                if self.health_status == "CRITICAL" and self.memory_safety:
                    logger.warning("System health CRITICAL - falling back to simple chunker with emergency settings")
                    # Emergency simple processing with conservative settings
                    segments = self._split_conservatively(text)
                    results = []
                    for segment in segments:
                        # Process one segment at a time
                        segment_result = self.simple_chunker.chunk_segments([segment], chunker_func)
                        results.extend(segment_result)
                        
                        # Aggressive GC after each segment in critical state
                        gc.collect()
                    
                    return results
                
                # Normal advanced processing
                try:
                    # Check if advanced chunker has chunk_in_parallel method
                    if hasattr(self.advanced_chunker, 'chunk_in_parallel'):
                        results = self.advanced_chunker.chunk_in_parallel(text, chunker_func)
                    else:
                        # Fallback to standard interface
                        segments = self._split_into_segments(text)
                        results = self.advanced_chunker.chunk_segments(segments, chunker_func)
                    
                    # Validate results as a sanity check
                    if not results and len(text) > 1000:
                        logger.warning("Advanced chunker returned empty results for non-empty text")
                        raise RuntimeError("Advanced chunker returned empty results")
                        
                    return results
                except AttributeError as e:
                    logger.error(f"Advanced chunker interface error: {e}, falling back to simple")
                    segments = self._split_into_segments(text)
                    return self.simple_chunker.chunk_segments(segments, chunker_func)
                
        except (TimeoutError, Exception) as e:
            logger.error(f"Advanced chunker failed: {type(e).__name__}: {e}")
            # Do not trip circuit breaker here - the main chunk method will handle errors
            raise
    
    def _split_into_segments(self, text: str) -> List[str]:
        """
        Split text into segments for parallel processing.
        
        Args:
            text: Text to split
            
        Returns:
            List of text segments
        """
        # Empty or very small text
        if len(text) < 1000:
            return [text]
        
        # Use the existing text splitting from StreamingBuffer
        segments = []
        for segment in self._streaming_buffer.stream_string(text):
            segments.append(segment)
            
        return segments
    
    def _split_conservatively(self, text: str) -> List[str]:
        """
        Split text very conservatively for emergency processing.
        
        Args:
            text: Text to split
            
        Returns:
            List of small text segments
        """
        # Use much smaller segments for conservative processing
        max_segment_size = min(self.size_threshold // 4, 25000)  # 25KB or less
        
        segments = []
        current_pos = 0
        text_len = len(text)
        
        while current_pos < text_len:
            # Small segment size for safety
            end_pos = min(current_pos + max_segment_size, text_len)
            
            # Look for natural boundaries
            if end_pos < text_len:
                # Check for paragraph break nearby
                para_pos = text.find('\n\n', end_pos - 100, end_pos + 100)
                if para_pos != -1:
                    end_pos = para_pos + 2
                else:
                    # Line break
                    line_pos = text.find('\n', end_pos - 50, end_pos + 50)
                    if line_pos != -1:
                        end_pos = line_pos + 1
                    else:
                        # Sentence or word boundary
                        sent_pos = max(
                            text.rfind('. ', end_pos - 100, end_pos),
                            text.rfind('! ', end_pos - 100, end_pos),
                            text.rfind('? ', end_pos - 100, end_pos)
                        )
                        if sent_pos != -1:
                            end_pos = sent_pos + 2
                        else:
                            # Word boundary
                            space_pos = text.rfind(' ', end_pos - 50, end_pos)
                            if space_pos != -1:
                                end_pos = space_pos + 1
            
            # Add segment
            segments.append(text[current_pos:end_pos])
            current_pos = end_pos
            
        return segments
    
    def stream_chunks(
        self,
        segment_gen: Generator[str, None, None],
        chunker_func: ChunkerFunc
    ) -> Generator[str, None, None]:
        """
        Stream chunks with efficient memory usage and automatic strategy selection.
        
        This method uses the StreamingBuffer for memory-efficient processing of
        streamed content, with automatic adaptation based on workload characteristics.
        
        Args:
            segment_gen: Generator yielding text segments
            chunker_func: Function that chunks text
            
        Yields:
            Processed chunks
        """
        # Initialize tracking
        start_time = time.monotonic()
        processed_segments = 0
        processed_bytes = 0
        produced_chunks = 0
        strategy = None
        buffer = []
        buffer_size = 0
        max_buffer_size = 500_000  # 500KB max buffer
        
        # First, buffer a small amount to estimate complexity
        try:
            # Fill buffer for initial complexity estimation
            for segment in segment_gen:
                buffer.append(segment)
                buffer_size += len(segment)
                
                if buffer_size >= max_buffer_size:
                    break
        except Exception as e:
            logger.error(f"Error during initial buffering: {e}")
        
        try:
            # Determine initial strategy if we have buffered data
            if buffer:
                # Combine samples for better estimation
                sample_text = "".join(buffer[:min(5, len(buffer))])
                
                # Estimate complexity and select strategy
                complexity = self._estimate_complexity(sample_text, chunker_func)
                strategy = 'advanced' if complexity >= self.complexity_threshold else 'simple'
                
                logger.info(f"Selected '{strategy}' strategy for streaming (complexity: {complexity:.4f})")
                self.metrics.record_decision(strategy, None)
            else:
                # Default to simple for empty generator
                strategy = 'simple'
            
            # Process buffered segments
            if buffer:
                for segment in buffer:
                    # Update counters
                    processed_segments += 1
                    processed_bytes += len(segment)
                    
                    # Process with selected strategy
                    if strategy == 'simple':
                        # Simple direct processing
                        for chunk in chunker_func(segment):
                            produced_chunks += 1
                            yield chunk
                    else:
                        # Advanced processing via chunker
                        try:
                            for chunk in self.advanced_chunker.chunk_segments([segment], chunker_func):
                                produced_chunks += 1
                                yield chunk
                        except Exception as e:
                            logger.error(f"Error in advanced streaming: {e}, falling back to simple")
                            # Fallback to simple
                            for chunk in chunker_func(segment):
                                produced_chunks += 1
                                yield chunk
            
            # Process remaining stream
            for segment in segment_gen:
                # Update counters
                processed_segments += 1
                processed_bytes += len(segment)
                
                # Process with selected strategy
                if strategy == 'simple':
                    # Simple direct processing
                    for chunk in chunker_func(segment):
                        produced_chunks += 1
                        yield chunk
                else:
                    # Advanced processing via chunker
                    try:
                        for chunk in self.advanced_chunker.chunk_segments([segment], chunker_func):
                            produced_chunks += 1
                            yield chunk
                    except Exception as e:
                        logger.error(f"Error in advanced streaming: {e}, falling back to simple")
                        # Fallback to simple
                        for chunk in chunker_func(segment):
                            produced_chunks += 1
                            yield chunk
                
        except Exception as e:
            logger.error(f"Error during streaming: {e}")
            # We're already yielding, so we can't do much more than log the error
        finally:
            # Calculate processing time and update metrics
            processing_time = time.monotonic() - start_time
            self.metrics.add_processing_time(
                elapsed_time=processing_time,
                bytes_processed=processed_bytes,
                chunks_produced=produced_chunks
            )
            
            # Log results
            logger.info(
                f"Streamed {processed_segments} segments ({processed_bytes:,} bytes) in {processing_time:.2f}s "
                f"producing {produced_chunks} chunks (strategy: {strategy})"
            )
    
    def chunk_with_priority(self, text: str, chunker_func: ChunkerFunc, priority: str = 'normal') -> List[str]:
        """
        Priority-aware processing with QoS controls.
        
        Args:
            text: Text to chunk
            chunker_func: Function that chunks text
            priority: Priority level ('high', 'normal', 'background')
            
        Returns:
            List of processed chunks
        """
        # Validate priority
        if priority not in PRIORITY_LEVELS:
            logger.warning(f"Invalid priority '{priority}' - using 'normal'")
            priority = 'normal'
            
        # Get priority settings
        priority_settings = PRIORITY_LEVELS[priority]
        
        # Set up context with priority settings
        with self._priority_context(priority):
            # Use priority-specific timeout
            custom_timeout = priority_settings.get('timeout', DEFAULT_TIMEOUT)
            original_timeout = self.timeout
            self.timeout = custom_timeout
            
            # Use priority-specific retries
            custom_retries = priority_settings.get('retries', MAX_RETRIES)
            original_retries = self.max_retries
            self.max_retries = custom_retries
            
            try:
                # Process with priority settings
                logger.info(f"Processing with {priority} priority (timeout={custom_timeout}s, retries={custom_retries})")
                return self.chunk(text, chunker_func)
            finally:
                # Restore original settings
                self.timeout = original_timeout
                self.max_retries = original_retries
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        Get comprehensive chunker performance metrics and system state.
        
        Returns:
            Dictionary of metrics and diagnostics
        """
        # Get base metrics
        metrics = self.metrics.to_dict()
        
        # Add system resource info
        metrics['system'] = {
            'health_status': self.health_status,
            'circuit_breaker': self.circuit_breaker_status,
            'cpu_count': self.resources.cpu_count,
            'logical_cores': self.resources.logical_cores,
            'memory_percent': self.resources.memory_percent,
            'system_load': self.resources.system_load,
        }
        
        # Add configuration
        metrics['config'] = {
            'size_threshold': self.size_threshold,
            'complexity_threshold': self.complexity_threshold,
            'memory_safety': self.memory_safety,
            'adaptive_batch_sizing': self.adaptive_batch_sizing,
            'timeout': self.timeout,
            'current_priority': self._current_priority,
        }
        
        return metrics
    
    def reset_metrics(self) -> None:
        """Reset performance metrics."""
        self.metrics = PerformanceMetrics()
        logger.info("Performance metrics reset")
    
    def shutdown(self) -> None:
        """
        Gracefully shut down the chunker and release resources.
        """
        logger.info("Shutting down SmartParallelChunker...")
        self._shutdown_event.set()
        
        # Wait for background threads to stop
        if hasattr(self, '_resource_monitor') and self._resource_monitor.is_alive():
            self._resource_monitor.join(timeout=2.0)
            
        # Stop config auto-update if running
        if hasattr(self.config, 'stop_auto_update'):
            self.config.stop_auto_update()
            
        # Release resources
        gc.collect()
        logger.info("Shutdown complete")

    def __enter__(self):
        """Context manager entry"""
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with graceful shutdown"""
        self.shutdown()
        return False  # Don't suppress exceptions


def create_auto_chunker(
    options: ChunkingOptions,
    mode: str = "auto",
    memory_safety: bool = True,
    timeout: float = DEFAULT_TIMEOUT,
    config: Optional[DynamicConfig] = None,
    enable_metrics_server: bool = False,
    metrics_port: int = 8000,
) -> SmartParallelChunker:
    """
    Factory function to create a pre-configured chunker instance
    with optimized settings for different workloads.
    
    Args:
        options: Chunking configuration options
        mode: Operational mode ('auto', 'performance', 'balanced', 'memory-safe')
        memory_safety: Enable memory safety features
        timeout: Operation timeout in seconds
        config: Optional dynamic configuration instance
        enable_metrics_server: Enable Prometheus metrics server
        metrics_port: Port for metrics server
        
    Returns:
        Configured SmartParallelChunker instance
    """
    # Create config instance if not provided
    if config is None:
        config = DynamicConfig({
            'processing_timeout': timeout,
            'max_retries': MAX_RETRIES,
            'memory_safety': memory_safety,
            'dynamic_batch_sizing': True,
            'enable_ml_segmentation': False,  # Disabled by default
            'start_metrics_server': enable_metrics_server,
            'metrics_port': metrics_port,
        })
    elif enable_metrics_server and HAS_PROMETHEUS:
        # Update existing config
        config.set('start_metrics_server', True)
        config.set('metrics_port', metrics_port)
        
    # Detect system resources
    resources = ResourceInfo.detect()
    
    # Configure based on mode
    if mode == "performance":
        # Performance-optimized settings
        chunker = SmartParallelChunker(
            options=options,
            size_threshold=200_000,  # Higher threshold for simple chunker (200KB)
            complexity_threshold=0.3,  # More aggressive use of advanced chunker
            memory_safety=False,  # Disable memory safety checks for performance
            adaptive_batch_sizing=True,
            health_check_enabled=True,
            timeout=timeout,
            resource_monitor_interval=10.0,  # Less frequent monitoring
            config=config,
        )
    elif mode == "memory-safe":
        # Memory-conservative settings
        chunker = SmartParallelChunker(
            options=options,
            size_threshold=50_000,  # Lower threshold for simple chunker (50KB)
            complexity_threshold=0.7,  # Less aggressive use of advanced chunker
            memory_safety=True,
            adaptive_batch_sizing=True,
            health_check_enabled=True,
            timeout=timeout,
            resource_monitor_interval=2.0,  # More frequent monitoring
            config=config,
        )
    elif mode == "balanced":
        # Balanced settings
        chunker = SmartParallelChunker(
            options=options,
            size_threshold=100_000,  # Medium threshold (100KB)
            complexity_threshold=0.5,  # Balanced use of advanced chunker
            memory_safety=memory_safety,
            adaptive_batch_sizing=True,
            health_check_enabled=True,
            timeout=timeout,
            resource_monitor_interval=5.0,
            config=config,
        )
    else:  # "auto" or any other value
        # Auto-configured based on system resources
        cpu_count = resources.cpu_count or os.cpu_count() or 4
        memory_gb = (resources.memory_total or 8 * 1024**3) / 1024**3
        
        # Determine settings based on system capabilities
        if cpu_count >= 8 and memory_gb >= 16:
            # High-end system - favor performance
            chunker = SmartParallelChunker(
                options=options,
                size_threshold=150_000,  # 150KB
                complexity_threshold=0.4,
                memory_safety=memory_safety,
                adaptive_batch_sizing=True,
                health_check_enabled=True,
                timeout=timeout,
                config=config,
            )
        elif cpu_count <= 2 or memory_gb <= 4:
            # Low-end system - conservative settings
            chunker = SmartParallelChunker(
                options=options,
                size_threshold=30_000,  # 30KB
                complexity_threshold=0.7,
                memory_safety=True,
                adaptive_batch_sizing=True,
                health_check_enabled=True,
                timeout=timeout,
                config=config,
            )
        else:
            # Medium system - balanced settings
            chunker = SmartParallelChunker(
                options=options,
                size_threshold=100_000,  # 100KB
                complexity_threshold=0.5,
                memory_safety=memory_safety,
                adaptive_batch_sizing=True,
                health_check_enabled=True,
                timeout=timeout,
                config=config,
            )
            
    return chunker


def start_monitoring_server(port: int = 8000) -> bool:
    """
    Start Prometheus metrics server for external monitoring.
    
    Args:
        port: HTTP port for metrics server
        
    Returns:
        Success status
    """
    if not HAS_PROMETHEUS:
        logger.warning("Prometheus client not available - cannot start metrics server")
        return False
        
    try:
        start_http_server(port)
        logger.info(f"Metrics server started on port {port}")
        return True
    except Exception as e:
        logger.error(f"Failed to start metrics server: {e}")
        return False
</file>

<file path="patterns/__init__.py">
"""
Package initialization for pattern modules
"""

# Import patterns for easier access
from enterprise_chunker.patterns.regex_patterns import RegexPatterns

# List available pattern modules for introspection
__all__ = [
    "RegexPatterns",
]
</file>

<file path="patterns/regex_patterns.py">
"""
Centralized regex pattern management for the EnterpriseChunker.

This module provides optimized and cached regular expression patterns for various
content formats, supporting efficient text parsing, chunking, and analysis operations.

Usage:
    from enterprise_chunker.utils.regex_patterns import RegexPatterns
    
    # Get format-specific patterns
    json_patterns = RegexPatterns.get_format_patterns(ContentFormat.JSON)
    
    # Use a specific pattern
    matches = json_patterns['property'].findall(content)
"""

import re
import logging
from enum import Enum
from functools import lru_cache
from typing import Dict, Any, List, Optional, Union, ClassVar
from enterprise_chunker.models.enums import ContentFormat

# Configure module logger
logger = logging.getLogger(__name__)


class PatternLoadError(Exception):
    """Exception raised when a regex pattern cannot be compiled or loaded."""
    pass


class RegexPatterns:
    """
    Central repository for all regex patterns used in chunking.
    
    This class provides optimized and cached access to regular expression patterns
    for various content formats and detection purposes. All patterns are compiled
    once and cached for performance.
    """
    
    @classmethod
    def get_format_patterns(cls, format_type: ContentFormat) -> Dict[str, re.Pattern]:
        """
        Get patterns specific to a content format.
        
        Args:
            format_type: Content format type enum value
            
        Returns:
            Dictionary of compiled regex patterns for the specified format
            
        Raises:
            ValueError: If the format_type is not recognized
        """
        if not isinstance(format_type, ContentFormat):
            logger.warning(f"Invalid format type provided: {format_type}")
            format_type = ContentFormat.TEXT
            
        # Generate patterns based on format type
        format_patterns = {
            ContentFormat.JSON: cls._json_patterns(),
            ContentFormat.XML: cls._xml_patterns(),
            ContentFormat.MARKDOWN: cls._markdown_patterns(),
            ContentFormat.CODE: cls._code_patterns(),
            ContentFormat.LOGS: cls._logs_patterns(),
            ContentFormat.CSV: cls._csv_patterns(),
            ContentFormat.TEXT: cls._text_patterns(),
        }
        
        # Get patterns for the specified format, fallback to TEXT if not found
        patterns = format_patterns.get(format_type)
        if patterns is None:
            logger.warning(f"Format type not found: {format_type}, using TEXT patterns instead")
            patterns = cls._text_patterns()
            
        return patterns
    
    @classmethod
    @lru_cache(maxsize=None)
    def get_language_detection_patterns(cls) -> Dict[str, re.Pattern]:
        """
        Get patterns for programming language detection.
        
        Returns:
            Dictionary of compiled regex patterns for language detection
        """
        try:
            patterns = {
                'python': re.compile(r'^(?:def|class|import|from|if\s+__name__\s*==|@|\s{4})', re.MULTILINE),
                'javascript': re.compile(r'^(?:function|const|let|var|import|export|=>)', re.MULTILINE),
                'java': re.compile(r'^(?:public|private|protected|class|interface|enum|package)', re.MULTILINE),
                'csharp': re.compile(r'^(?:namespace|using|public|private|protected|class|interface)', re.MULTILINE),
                'html': re.compile(r'^(?:<html|<head|<body|<div|<!DOCTYPE)', re.MULTILINE),
                'css': re.compile(r'^(?:\S+\s*{|@media|@import|@keyframes)', re.MULTILINE),
                'typescript': re.compile(r'^(?:interface|type|namespace|enum|declare|as\s+const)', re.MULTILINE),
                'rust': re.compile(r'^(?:fn|struct|enum|impl|mod|use|pub|let\s+mut)', re.MULTILINE),
                'go': re.compile(r'^(?:package|import|func|type|struct|interface|var|const)', re.MULTILINE),
                'php': re.compile(r'^(?:<\?php|\$\w+|function|class|namespace)', re.MULTILINE),
                'ruby': re.compile(r'^(?:require|class|def|module|if\s+__FILE__\s*==)', re.MULTILINE),
                'swift': re.compile(r'^(?:import|class|struct|enum|extension|func|let|var)', re.MULTILINE),
            }
            return patterns
        except re.error as e:
            logger.error(f"Failed to compile language detection patterns: {e}")
            raise PatternLoadError(f"Language detection pattern compilation failed: {e}")
    
    @classmethod
    @lru_cache(maxsize=None)
    def get_token_estimation_patterns(cls) -> Dict[str, re.Pattern]:
        """
        Get patterns for token estimation.
        
        Returns:
            Dictionary of compiled regex patterns for token estimation
        """
        try:
            patterns = {
                # Fixed: Converted to raw string for clarity
                'latin': re.compile(r'[a-zA-Z0-9\s.,?!;:()\[\]{}\'"`<>/\\|~!@#$%^&*_+=]'),
                'cjk': re.compile(r'[\u4e00-\u9fff\u3000-\u303f\u3040-\u309f\u30a0-\u30ff\uff00-\uff9f\u3400-\u4dbf]'),
                'emoji': re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF]', re.UNICODE),
                'whitespace': re.compile(r'\s+'),
                'punctuation': re.compile(r'[.,;:!?()[\]{}\'"`\-_+=<>|\\/@#$%^&*~]'),
                'numbers': re.compile(r'\b\d+(?:\.\d+)?\b'),
                'special_tokens': re.compile(r'<[^>]+>'),
            }
            return patterns
        except re.error as e:
            logger.error(f"Failed to compile token estimation patterns: {e}")
            raise PatternLoadError(f"Token estimation pattern compilation failed: {e}")
    
    @classmethod
    @lru_cache(maxsize=None)
    def get_format_detection_patterns(cls) -> Dict[str, re.Pattern]:
        """
        Get patterns for content format detection.
        
        Returns:
            Dictionary of compiled regex patterns for format detection
        """
        try:
            patterns = {
                'json': re.compile(r'^\s*[\[{]'),
                'xml': re.compile(r'^\s*<(?:\?xml|!DOCTYPE|[a-zA-Z])'),
                'markdown': re.compile(r'^(?:#+\s|\*\s|\d+\.\s|>\s|=+|-+)', re.MULTILINE),
                'source_code': re.compile(r'^(?:function|class|import|export|const|let|var|if|for|while)\b', re.MULTILINE),
                'yaml': re.compile(r'^(?:---|\w+:\s+)', re.MULTILINE),
                'csv': re.compile(r'^(?:[^,\n\r]+(?:,[^,\n\r]+)+)(?:\r?\n(?:[^,\n\r]+(?:,[^,\n\r]+)+))+$'),
                'log': re.compile(r'^(?:\[\d|\d{4}-\d{2}-\d{2}|ERROR|INFO|DEBUG|WARN|FATAL)', re.MULTILINE | re.IGNORECASE),
            }
            return patterns
        except re.error as e:
            logger.error(f"Failed to compile format detection patterns: {e}")
            raise PatternLoadError(f"Format detection pattern compilation failed: {e}")
    
    @classmethod
    @lru_cache(maxsize=None)
    def get_sentence_boundaries(cls) -> re.Pattern:
        """
        Get pattern for sentence boundaries.
        
        Returns:
            Compiled regex pattern for sentence boundaries
        """
        try:
            pattern = re.compile(r'(?<=[.!?])\s+(?=[A-Z])')
            return pattern
        except re.error as e:
            logger.error(f"Failed to compile sentence boundary pattern: {e}")
            raise PatternLoadError(f"Sentence boundary pattern compilation failed: {e}")
    
    @classmethod
    def get_pattern_by_name(cls, format_type: ContentFormat, pattern_name: str) -> Optional[re.Pattern]:
        """
        Get a specific pattern by name from a format.
        
        Args:
            format_type: Content format type
            pattern_name: Name of the pattern to retrieve
            
        Returns:
            The compiled regex pattern or None if not found
        """
        patterns = cls.get_format_patterns(format_type)
        pattern = patterns.get(pattern_name)
        
        if pattern is None:
            logger.warning(f"Pattern '{pattern_name}' not found for format {format_type}")
            
        return pattern
    
    @classmethod
    def find_matches(cls, text: str, format_type: ContentFormat, pattern_name: str) -> List[str]:
        """
        Find all matches for a specific pattern in text.
        
        Args:
            text: The text to search in
            format_type: Content format type
            pattern_name: Name of the pattern to use
            
        Returns:
            List of matched strings
        """
        if not text:
            return []
            
        pattern = cls.get_pattern_by_name(format_type, pattern_name)
        if not pattern:
            return []
            
        try:
            return pattern.findall(text)
        except re.error as e:
            logger.error(f"Error matching pattern '{pattern_name}': {e}")
            return []
    
    @staticmethod
    @lru_cache(maxsize=None)
    def _json_patterns() -> Dict[str, re.Pattern]:
        """
        JSON-specific patterns.
        
        Returns:
            Dictionary of compiled regex patterns for JSON format
        """
        try:
            return {
                'object_start': re.compile(r'^\s*\{'),
                'array_start': re.compile(r'^\s*\['),
                'property': re.compile(r'"([^"\\]|\\.)*":\s*'),
                'string': re.compile(r'"(?:[^"\\]|\\.)*"'),
                'number': re.compile(r'-?\d+(?:\.\d+)?(?:[eE][+-]?\d+)?'),
                'boolean': re.compile(r'\b(?:true|false|null)\b'),
                'json_lines': re.compile(r'^\s*\{.*\}\s*$', re.MULTILINE),
                'nested_object': re.compile(r'\{(?:[^{}]|\{(?:[^{}]|\{[^{}]*\})*\})*\}'),
                'nested_array': re.compile(r'\[(?:[^\[\]]|\[(?:[^\[\]]|\[[^\[\]]*\])*\])*\]'),
            }
        except re.error as e:
            logger.error(f"Failed to compile JSON patterns: {e}")
            raise PatternLoadError(f"JSON pattern compilation failed: {e}")
        
    @staticmethod
    @lru_cache(maxsize=None)
    def _xml_patterns() -> Dict[str, re.Pattern]:
        """
        XML-specific patterns.
        
        Returns:
            Dictionary of compiled regex patterns for XML format
        """
        try:
            return {
                'tag': re.compile(r'</?[a-zA-Z][^>]*>'),
                'opening_tag': re.compile(r'<([a-zA-Z][a-zA-Z0-9]*)[^>]*(?<!/)>'),
                'closing_tag': re.compile(r'</([a-zA-Z][a-zA-Z0-9]*)>'),
                'self_closing_tag': re.compile(r'<([a-zA-Z][a-zA-Z0-9]*)[^>]*/>', re.DOTALL),
                'declaration': re.compile(r'<\?xml[^>]*\?>'),
                'doctype': re.compile(r'<!DOCTYPE[^>]*>'),
                'comment': re.compile(r'<!--.*?-->', re.DOTALL),
                'attribute': re.compile(r'\s([a-zA-Z][a-zA-Z0-9]*)=["\'](.*?)["\']'),
                'cdata': re.compile(r'<!\[CDATA\[(.*?)\]\]>', re.DOTALL),
                'processing_instruction': re.compile(r'<\?([a-zA-Z][a-zA-Z0-9]*)[^>]*\?>'),
                'namespace': re.compile(r'xmlns(?::([a-zA-Z][a-zA-Z0-9]*))?=["\'](.*?)["\']'),
            }
        except re.error as e:
            logger.error(f"Failed to compile XML patterns: {e}")
            raise PatternLoadError(f"XML pattern compilation failed: {e}")
        
    @staticmethod
    @lru_cache(maxsize=None)
    def _markdown_patterns() -> Dict[str, re.Pattern]:
        """
        Markdown-specific patterns.
        
        Returns:
            Dictionary of compiled regex patterns for Markdown format
        """
        try:
            return {
                'headers': re.compile(r'^(#{1,6})\s+(.+)', re.MULTILINE),
                'alt_headers': re.compile(r'^([^\n]+)\n([=\-]{3,})', re.MULTILINE),
                'list_items': re.compile(r'^[\s]*[-*+]\s+.+$', re.MULTILINE),
                'numbered_list': re.compile(r'^[\s]*\d+\.\s+.+$', re.MULTILINE),
                'code_blocks': re.compile(r'^```[\s\S]*?```$', re.MULTILINE),
                'inline_code': re.compile(r'`[^`]+`'),
                'links': re.compile(r'\[.+?\]\(.+?\)'),
                'images': re.compile(r'!\[.*?\]\(.*?\)'),
                'blockquotes': re.compile(r'^>\s+.*?', re.MULTILINE),
                'emphasis': re.compile(r'(\*\*|__).+?(\*\*|__)'),
                'horizontal_rule': re.compile(r'^---', re.MULTILINE),
                'tables': re.compile(r'^\|.+\|$[\r\n]+^\|[-:| ]+\|$', re.MULTILINE),
                'task_list': re.compile(r'^[\s]*[-*+]\s+\[[ xX]\]\s+.+$', re.MULTILINE),
                'footnote_ref': re.compile(r'\[\^.+?\]'),
                'footnote_def': re.compile(r'^\[\^.+?\]:', re.MULTILINE),
                'strikethrough': re.compile(r'~~.+?~~'),
                'html_tags': re.compile(r'<([a-zA-Z][a-zA-Z0-9]*)[^>]*>.*?</\1>', re.DOTALL),
            }
        except re.error as e:
            logger.error(f"Failed to compile Markdown patterns: {e}")
            raise PatternLoadError(f"Markdown pattern compilation failed: {e}")
        
    @staticmethod
    @lru_cache(maxsize=None)
    def _code_patterns() -> Dict[str, re.Pattern]:
        """
        Code-specific patterns.
        
        Returns:
            Dictionary of compiled regex patterns for Code format
        """
        try:
            return {
                'function_def': re.compile(r'^(?:function|class|def|public|private|protected|static|async)\s+\w+\s*\([^)]*\)\s*{?', re.MULTILINE),
                'control_flow': re.compile(r'^(?:if|for|while|switch|try|catch|finally)\s*\([^)]*\)\s*{?', re.MULTILINE),
                'variable_decl': re.compile(r'^(?:const|let|var)\s+\w+\s*=.+?;?', re.MULTILINE),
                'import': re.compile(r'^(?:import|export|require|module\.exports)\s+.+?;?', re.MULTILINE),
                'comments_single': re.compile(r'^\/\/.*', re.MULTILINE),
                'comments_multi': re.compile(r'^\/\*[\s\S]*?\*\/', re.MULTILINE),
                'empty_lines': re.compile(r'^\s*$', re.MULTILINE),
                'class_def': re.compile(r'^(?:class|interface|enum|struct)\s+\w+', re.MULTILINE),
                'decorator': re.compile(r'^@\w+', re.MULTILINE),
                'method_def': re.compile(r'^\s+(?:public|private|protected|static|async)?\s*\w+\s*\([^)]*\)\s*{?', re.MULTILINE),
                'namespace': re.compile(r'^(?:namespace|package|module)\s+[\w.]+', re.MULTILINE),
                'lambda': re.compile(r'=>|->'),
                'string_literal': re.compile(r'([\'"])(?:(?=(\\?))\2.)*?\1'),
                'brackets': re.compile(r'[{}()\[\]]'),
            }
        except re.error as e:
            logger.error(f"Failed to compile Code patterns: {e}")
            raise PatternLoadError(f"Code pattern compilation failed: {e}")
        
    @staticmethod
    @lru_cache(maxsize=None)
    def _logs_patterns() -> Dict[str, re.Pattern]:
        """
        Log-specific patterns.
        
        Returns:
            Dictionary of compiled regex patterns for Logs format
        """
        try:
            return {
                'iso_timestamp': re.compile(r'^\[\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?\]', re.MULTILINE),
                'iso_timestamp_no_bracket': re.compile(r'^\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?', re.MULTILINE),
                'us_date': re.compile(r'^\d{1,2}\/\d{1,2}\/\d{2,4}\s+\d{1,2}:\d{2}(?::\d{2})?(?:\s+[AP]M)?', re.MULTILINE),
                'log_level': re.compile(r'^(?:ERROR|WARN(?:ING)?|INFO|DEBUG|TRACE|FATAL|CRITICAL|NOTICE|SEVERE)(?:\s+|\:)', re.MULTILINE | re.IGNORECASE),
                'mobile_log': re.compile(r'^(?:E|W|I|D|T|F|C|N|S)\/[\w.]+\(\s*\d+\):', re.MULTILINE),
                'exception': re.compile(r'^(?:Exception|Error|Traceback|Caused by|at)\s+[\w.$]+(?:[:]\s|[:]\s+\w+|\s+[\w.(]+\()', re.MULTILINE),
                'stack_trace_java': re.compile(r'^\s+at\s+[\w.$]+(?:\.[\w.$]+)+\([^)]*\)$', re.MULTILINE),
                'stack_trace_python': re.compile(r'^\s+File ".*", line \d+', re.MULTILINE),
                'process_id': re.compile(r'^Process ID:?\s+\d+', re.MULTILINE),
                'thread_id': re.compile(r'^Thread(?: ID)?:?\s+\d+', re.MULTILINE),
                'uuid': re.compile(r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}'),
                'ip_address': re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b'),
                'request_id': re.compile(r'(?:request[-_]id|correlation[-_]id|trace[-_]id)[:=]\s*[\w-]+', re.IGNORECASE),
                'user_agent': re.compile(r'user[-_]agent[:=]\s*[^\n\r]+', re.IGNORECASE),
                'status_code': re.compile(r'(?:status|code|response)[:=]\s*\d{3}', re.IGNORECASE),
                'memory_usage': re.compile(r'(?:memory|heap|ram)[:=]\s*\d+(?:\.\d+)?\s*(?:KB|MB|GB|B)', re.IGNORECASE),
            }
        except re.error as e:
            logger.error(f"Failed to compile Logs patterns: {e}")
            raise PatternLoadError(f"Logs pattern compilation failed: {e}")
        
    @staticmethod
    @lru_cache(maxsize=None)
    def _csv_patterns() -> Dict[str, re.Pattern]:
        """
        CSV-specific patterns.
        
        Returns:
            Dictionary of compiled regex patterns for CSV format
        """
        try:
            return {
                'quoted_field': re.compile(r'"(?:[^"]|"")*"'),
                'unquoted_field': re.compile(r'[^,;\t|"\n\r]+'),
                'delimiter_comma': re.compile(r','),
                'delimiter_semicolon': re.compile(r';'),
                'delimiter_tab': re.compile(r'\t'),
                'delimiter_pipe': re.compile(r'\|'),
                'header_row': re.compile(r'^[^,\n\r]+(?:,[^,\n\r]+)*$'),
                'empty_field': re.compile(r',,|^,|,$'),
                'quoted_with_comma': re.compile(r'"[^"]*,[^"]*"'),
                'row': re.compile(r'^.*$', re.MULTILINE),
                'field_count': re.compile(r'^(?:[^,\n\r]+(?:,[^,\n\r]+)*)\r?\n', re.MULTILINE),
                'number_field': re.compile(r'\b\d+(?:\.\d+)?\b'),
                'date_field': re.compile(r'\b\d{1,4}[-/]\d{1,2}[-/]\d{1,4}\b'),
            }
        except re.error as e:
            logger.error(f"Failed to compile CSV patterns: {e}")
            raise PatternLoadError(f"CSV pattern compilation failed: {e}")
        
    @staticmethod
    @lru_cache(maxsize=None)
    def _text_patterns() -> Dict[str, re.Pattern]:
        """
        General text patterns.
        
        Returns:
            Dictionary of compiled regex patterns for Text format
        """
        try:
            return {
                'paragraph': re.compile(r'\n\s*\n'),
                'line_break': re.compile(r'\n'),
                'sentence': re.compile(r'(?<=[.!?])\s+(?=[A-Z])'),
                'url': re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'),
                'email': re.compile(r'[\w.+-]+@[\w-]+\.[\w.-]+'),
                'phone_number': re.compile(r'\b(?:\+\d{1,2}\s)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b'),
                'date': re.compile(r'\b\d{1,4}[-/]\d{1,2}[-/]\d{1,4}\b'),
                'time': re.compile(r'\b\d{1,2}:\d{2}(?::\d{2})?(?:\s*[aApP][mM])?\b'),
                'currency': re.compile(r'[$€£¥]\s*\d+(?:\.\d{2})?|\d+(?:\.\d{2})?\s*[$€£¥]'),
                'hashtag': re.compile(r'#[a-zA-Z0-9_]+'),
                'mention': re.compile(r'@[a-zA-Z0-9_]+'),
                'acronym': re.compile(r'\b[A-Z]{2,}\b'),
                'word': re.compile(r'\b[a-zA-Z]+\b'),
                'number': re.compile(r'\b\d+(?:\.\d+)?\b'),
                'percentage': re.compile(r'\b\d+(?:\.\d+)?%\b'),
                'citation': re.compile(r'\[\d+\]'),
            }
        except re.error as e:
            logger.error(f"Failed to compile Text patterns: {e}")
            raise PatternLoadError(f"Text pattern compilation failed: {e}")

    @classmethod
    def precompile_all_patterns(cls) -> None:
        """
        Precompile all patterns to populate the cache.
        This can be called at application startup to ensure all patterns
        are loaded and validated before they are needed.
        
        Raises:
            PatternLoadError: If any pattern fails to compile
        """
        try:
            start_time = time.time()
            
            # Load all pattern sets to populate caches
            cls.get_sentence_boundaries()
            cls.get_format_detection_patterns()
            cls.get_language_detection_patterns()
            cls.get_token_estimation_patterns()
            
            # Load all format-specific patterns
            pattern_counts = {}
            for format_type in ContentFormat:
                format_patterns = cls.get_format_patterns(format_type)
                pattern_counts[format_type.name] = len(format_patterns)
                
            # Debug-time sanity check for pattern counts
            logger.info(f"Successfully precompiled all regex patterns in {time.time() - start_time:.3f}s")
            logger.debug(f"Pattern count by format: {pattern_counts}")
            
        except Exception as e:
            logger.error(f"Failed to precompile patterns: {e}")
            raise PatternLoadError(f"Pattern precompilation failed: {e}")
</file>

<file path="readme.md">
# EnterpriseChunker

An advanced text chunking utility for LLM processing with intelligent content-aware strategies.

## Features

- **Multi-strategy token estimation** with adaptive contextual awareness
- **Format-specific intelligent chunking** for JSON, XML, code, logs, markdown and more
- **Structural boundary preservation** with nested context tracking
- **Semantic coherence maintenance** between chunks
- **Memory-efficient processing** for large files
- **Streaming support** with generator-based processing
- **Smart overlap** with context-aware boundaries
- **Performance optimization** with caching
- **Comprehensive error recovery** with fallback strategies

## Installation

```bash
# From PyPI
pip install enterprise-chunker

# From source
git clone https://github.com/your-org/enterprise-chunker.git
cd enterprise-chunker
pip install -e .
```

## Quick Start

```python
from enterprise_chunker import EnterpriseChunker, ChunkingStrategy

# Initialize chunker with default settings
chunker = EnterpriseChunker()

# Simple chunking with automatic format detection
chunks = chunker.chunk(text)

# Advanced configuration
chunks = chunker.adaptive_chunk_text(
    text,
    max_tokens_per_chunk=1000,
    overlap_tokens=50,
    strategy=ChunkingStrategy.SEMANTIC
)

# Using the fluent API
chunks = chunker.with_max_tokens(1000).with_overlap(50).chunk(text)

# Using a context manager
with chunker.semantic_context(max_tokens=1000, overlap=50):
    chunks = chunker.chunk(text)

# Processing a file stream
with open('large_file.txt', 'r') as f:
    for chunk in chunker.chunk_stream(f, max_tokens_per_chunk=1000):
        process_chunk(chunk)
```

## Configuration Options

EnterpriseChunker provides many configuration options:

| Option | Default | Description |
|--------|---------|-------------|
| `max_tokens_per_chunk` | 4000 | Maximum tokens per chunk |
| `overlap_tokens` | 200 | Number of tokens to overlap between chunks |
| `chunking_strategy` | ADAPTIVE | Strategy for chunking (ADAPTIVE, SEMANTIC, STRUCTURAL, FIXED_SIZE, SENTENCE) |
| `token_strategy` | BALANCED | Strategy for token estimation (BALANCED, PRECISION, PERFORMANCE) |
| `preserve_structure` | True | Preserve document structure when chunking |
| `add_metadata_comments` | True | Add metadata comments to chunks |
| `respect_sentences` | True | Try to respect sentence boundaries |

## Chunking Strategies

EnterpriseChunker supports multiple chunking strategies:

- **ADAPTIVE**: Dynamically choose best strategy based on content
- **SEMANTIC**: Preserve semantic boundaries (paragraphs, sections)
- **STRUCTURAL**: Preserve structural elements (JSON, XML, Markdown, etc.)
- **FIXED_SIZE**: Simple fixed-size chunks with overlap
- **SENTENCE**: Split by sentences

## Environment Variables

EnterpriseChunker can be configured with environment variables:

```bash
# Set maximum tokens per chunk
export CHUNKER_MAX_TOKENS_PER_CHUNK=2000

# Set chunking strategy
export CHUNKER_CHUNKING_STRATEGY=semantic

# Set token estimation strategy
export CHUNKER_TOKEN_STRATEGY=precision
```

## Format-Specific Chunking

EnterpriseChunker provides specialized chunking for different formats:

### JSON Chunking

Maintains valid JSON structure in each chunk with metadata:

```python
json_chunks = chunker.adaptive_chunk_text(json_text, strategy=ChunkingStrategy.STRUCTURAL)
```

Each chunk is a valid JSON object with metadata:

```json
{
  "_chunk_info": {
    "index": 0,
    "type": "json_array",
    "total": 3,
    "has_overlap": false
  },
  "data": [...]
}
```

### Markdown Chunking

Preserves headings and section structure:

```python
markdown_chunks = chunker.adaptive_chunk_text(markdown_text, strategy=ChunkingStrategy.STRUCTURAL)
```

Chunk boundaries are placed at headings, and context is preserved across chunks:

```markdown
<!-- Context from previous chunk -->
# Previous Section

<!-- Current content -->
## Current Subsection
Content text...
```

## Performance Considerations

- Use `TokenEstimationStrategy.PERFORMANCE` for very large files
- For streaming large files, adjust `stream_buffer_size` 
- Consider using `max_chunk_size_chars` for more consistent sizing

## Contributing

Contributions are welcome! Please check out our [contributing guidelines](CONTRIBUTING.md).

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</file>

<file path="setup.py">
"""
Setup file for EnterpriseChunker package
"""

import os
from setuptools import setup, find_packages

# Read the README.md for the long description
readme_path = os.path.join(os.path.dirname(__file__), "README.md")
try:
    with open(readme_path, "r", encoding="utf-8") as fh:
        long_description = fh.read()
except (IOError, FileNotFoundError):
    long_description = "Advanced text chunking utility for LLM processing"

# Define package dependencies
INSTALL_REQUIRES = [
    # Core dependencies required for basic functionality
    "psutil>=5.9.0",
    "numpy>=1.22.0",
    "prometheus_client>=0.14.0",
    "requests>=2.27.0",
]

# Optional dependencies for extended functionality
EXTRAS_REQUIRE = {
    "dev": [
        "pytest>=7.0.0",
        "pytest-cov>=4.0.0",
        "black>=23.0.0",
        "isort>=5.0.0",
        "mypy>=1.0.0",
        "pylint>=2.17.0",
    ],
    "docs": [
        "sphinx>=6.0.0",
        "sphinx-rtd-theme>=1.0.0",
        "myst-parser>=2.0.0",
    ],
    "testing": [
        "pytest>=7.0.0",
        "pytest-cov>=4.0.0",
    ],
    # Convenience meta-package for development
    "all": [],  # Will be filled below
}

# Add all other extras to 'all'
EXTRAS_REQUIRE["all"] = sorted({
    dependency
    for extra_dependencies in EXTRAS_REQUIRE.values()
    for dependency in extra_dependencies
})

setup(
    name="enterprise-chunker",
    version="1.0.0-rc1",  # Bumped version to Release Candidate 1
    author="Your Organization",
    author_email="info@example.com",
    description="Advanced text chunking utility for LLM processing",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/your-org/enterprise-chunker",
    project_urls={
        "Bug Tracker": "https://github.com/your-org/enterprise-chunker/issues",
        "Documentation": "https://enterprise-chunker.readthedocs.io/",
        "Source Code": "https://github.com/your-org/enterprise-chunker",
    },
    packages=find_packages(include=["enterprise_chunker", "enterprise_chunker.*"]),
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Topic :: Text Processing",
        "Topic :: Software Development :: Libraries :: Python Modules",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.8",
    install_requires=INSTALL_REQUIRES,
    extras_require=EXTRAS_REQUIRE,
    test_suite="tests",
    include_package_data=True,
    zip_safe=False,
    keywords="llm, chunking, text-processing, language-model",
)
</file>

<file path="strategies/__init__.py">
"""
Package initialization for chunking strategies
"""

# Import strategies for easier access
from enterprise_chunker.strategies.base import BaseChunkingStrategy
from enterprise_chunker.strategies.semantic import SemanticChunkingStrategy
from enterprise_chunker.strategies.fixed_size import FixedSizeChunkingStrategy

# List available strategies for introspection
__all__ = [
    "BaseChunkingStrategy",
    "SemanticChunkingStrategy",
    "FixedSizeChunkingStrategy",
]
</file>

<file path="strategies/base.py">
"""
Base chunking strategy implementation with common functionality.

This module provides the foundation for all text chunking strategies in the
Enterprise Chunker system. It implements the Template Method pattern to define
the common chunking workflow while allowing specialized implementations to
customize critical steps of the chunking process.

Typical usage:
    strategy = ChunkingStrategyFactory.create_strategy(
        strategy=ChunkingStrategy.SEMANTIC,
        format_type=ContentFormat.TEXT
    )
    result = strategy.chunk(text, options)
"""

import time
import math
import logging
import uuid
from abc import ABC, abstractmethod
from typing import (
    List,
    Dict, 
    Any, 
    Optional, 
    Tuple, 
    Generator,
    TypeVar,
    cast,
    Union
)

from enterprise_chunker.models.enums import ContentFormat, ChunkingStrategy, TokenEstimationStrategy
from enterprise_chunker.models.chunk_metadata import ChunkMetadata, ChunkingResult, MetadataBuilder
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.utils.token_estimation import estimate_tokens
from enterprise_chunker.utils.performance import timing_decorator
from enterprise_chunker.exceptions import ChunkingError, BoundaryDetectionError

# Configure logging
logger = logging.getLogger(__name__)

# Type variable for return type annotation
T = TypeVar('T', bound='BaseChunkingStrategy')


class BaseChunkingStrategy(ABC):
    """
    Base class for all chunking strategies with common functionality.
    
    This abstract class implements the Template Method pattern to establish
    a standardized chunking workflow while allowing subclasses to customize
    specific aspects of the chunking process through abstract methods.
    
    Attributes:
        format_type: The content format this strategy handles
        operation_id: Unique identifier for tracking the chunking operation
        stats: Dictionary containing performance statistics
    """
    
    def __init__(self, format_type: ContentFormat):
        """
        Initialize the chunking strategy.
        
        Args:
            format_type: Content format this strategy handles (JSON, XML, etc.)
        """
        if not isinstance(format_type, ContentFormat):
            raise ValueError(f"Invalid format type: {format_type}")
            
        self.format_type: ContentFormat = format_type
        self.operation_id: str = ""
        self.stats: Dict[str, Any] = {
            "boundary_detection_time": 0.0,
            "chunk_processing_time": 0.0,
            "validation_time": 0.0
        }
    
    def set_operation_id(self, operation_id: str) -> T:
        """
        Set the operation ID for tracking and logging.
        
        Args:
            operation_id: Unique operation identifier
            
        Returns:
            Self reference for method chaining
        """
        if not operation_id:
            self.operation_id = str(uuid.uuid4())
            logger.debug(f"Generated new operation ID: {self.operation_id}")
        else:
            self.operation_id = operation_id
            
        return cast(T, self)
    
    @timing_decorator
    def chunk(self, text: str, options: ChunkingOptions) -> ChunkingResult:
        """
        Template method defining the chunking process workflow.
        
        This method implements the high-level chunking algorithm while delegating
        format-specific operations to subclasses.
        
        Args:
            text: Text content to chunk
            options: Chunking configuration options
            
        Returns:
            ChunkingResult containing chunks and associated metadata
            
        Raises:
            ChunkingError: If chunking process fails and fallback is unsuccessful
        """
        if not text:
            logger.warning("Received empty text for chunking")
            return self._create_empty_result(options)
            
        start_time = time.time()
        
        # Generate operation ID if not set
        if not self.operation_id:
            self.set_operation_id(str(uuid.uuid4()))
        
        try:
            # Check if chunking is needed at all
            token_count = estimate_tokens(text, options.token_strategy)
            if token_count <= options.max_tokens_per_chunk * options.safety_margin:
                logger.debug(
                    f"Content size ({token_count} tokens) below threshold "
                    f"of {options.max_tokens_per_chunk * options.safety_margin}, "
                    f"returning single chunk"
                )
                
                metadata = self._create_single_chunk_metadata(text, token_count, options)
                
                result = ChunkingResult(
                    chunks=[text], 
                    chunk_metadata=[metadata],
                    original_length=len(text),
                    detected_format=self.format_type,
                    token_estimation_strategy=options.token_strategy,
                    chunking_strategy=self._get_chunking_strategy(),
                    processing_time=time.time() - start_time,
                    total_token_count=token_count,
                    operation_id=self.operation_id
                )
                
                return result
            
            # Execute the main chunking logic
            result = self._execute_chunking_workflow(text, options, start_time)
            
            # Log chunking results
            logger.info(
                f"[{self.operation_id}] Chunked content ({len(text):,} chars) into "
                f"{len(result.chunks)} chunks using {self._get_chunking_strategy().value} "
                f"strategy in {result.processing_time:.3f}s"
            )
            
            self._log_chunking_stats(result)
            
            return result
            
        except Exception as e:
            logger.error(
                f"[{self.operation_id}] Error in chunking: {str(e)}", 
                exc_info=True
            )
            # Fall back to simple chunking
            try:
                logger.warning(f"[{self.operation_id}] Using fallback chunking method")
                return self._simple_chunk_text(text, options, start_time)
            except Exception as fallback_error:
                logger.critical(
                    f"[{self.operation_id}] Fallback chunking also failed: {str(fallback_error)}",
                    exc_info=True
                )
                raise ChunkingError(
                    f"Chunking failed with both primary and fallback methods: {str(e)}"
                ) from e
    
    def _create_empty_result(self, options: ChunkingOptions) -> ChunkingResult:
        """
        Create a result for empty input text.
        
        Args:
            options: Chunking options
            
        Returns:
            Empty chunking result
        """
        metadata = MetadataBuilder() \
            .with_index(0) \
            .with_total_chunks(0) \
            .with_format(self.format_type) \
            .with_token_count(0) \
            .with_char_count(0) \
            .with_content_slice(0, 0) \
            .build()
            
        return ChunkingResult(
            chunks=[],
            chunk_metadata=[metadata],
            original_length=0,
            detected_format=self.format_type,
            token_estimation_strategy=options.token_strategy,
            chunking_strategy=self._get_chunking_strategy(),
            processing_time=0.0,
            total_token_count=0,
            operation_id=self.operation_id
        )
    
    def _create_single_chunk_metadata(
        self, 
        text: str, 
        token_count: int,
        options: ChunkingOptions
    ) -> ChunkMetadata:
        """
        Create metadata for a single chunk scenario.
        
        Args:
            text: Full text content
            token_count: Number of tokens in the text
            options: Chunking options
            
        Returns:
            ChunkMetadata for the single chunk
        """
        return MetadataBuilder() \
            .with_index(0) \
            .with_total_chunks(1) \
            .with_format(self.format_type) \
            .with_token_count(token_count) \
            .with_char_count(len(text)) \
            .with_content_slice(0, len(text)) \
            .build()
    
    def _execute_chunking_workflow(
        self, 
        text: str, 
        options: ChunkingOptions,
        start_time: float
    ) -> ChunkingResult:
        """
        Execute the main chunking workflow.
        
        Args:
            text: Text to chunk
            options: Chunking options
            start_time: Process start time
            
        Returns:
            Chunking result
            
        Raises:
            BoundaryDetectionError: If boundary detection fails
        """
        # Detect boundaries for chunking with timing
        boundary_start = time.time()
        try:
            boundaries = self.detect_boundaries(text, options)
            self.stats["boundary_detection_time"] = time.time() - boundary_start
        except Exception as e:
            logger.error(f"[{self.operation_id}] Boundary detection failed: {str(e)}")
            raise BoundaryDetectionError(f"Failed to detect boundaries: {str(e)}") from e
        
        # Process chunks based on boundaries with timing
        processing_start = time.time()
        result = self.process_chunks_with_boundaries(text, boundaries, options)
        self.stats["chunk_processing_time"] = time.time() - processing_start
        
        # Update processing time
        result.processing_time = time.time() - start_time
        
        return result
    
    def _log_chunking_stats(self, result: ChunkingResult) -> None:
        """
        Log detailed chunking statistics.
        
        Args:
            result: Chunking result to analyze
        """
        if not result.chunks:
            return
            
        avg_chunk_size = sum(len(chunk) for chunk in result.chunks) / len(result.chunks)
        avg_tokens = result.total_token_count / len(result.chunks)
        
        logger.debug(
            f"[{self.operation_id}] Chunking stats: "
            f"avg chunk size={avg_chunk_size:.1f} chars, "
            f"avg tokens={avg_tokens:.1f}, "
            f"boundary detection={self.stats['boundary_detection_time']:.3f}s, "
            f"chunk processing={self.stats['chunk_processing_time']:.3f}s"
        )
    
    @abstractmethod
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        Detect boundary points in text (to be implemented by subclasses).
        
        Subclasses must implement this method to identify logical break points
        in the text based on its format and content structure.
        
        Args:
            text: Text to analyze
            options: Chunking options
            
        Returns:
            List of boundary dictionaries with at least 'index' key
            
        Raises:
            NotImplementedError: If not implemented by subclass
        """
        raise NotImplementedError("Subclasses must implement detect_boundaries")
    
    def process_chunks_with_boundaries(
        self, 
        text: str, 
        boundaries: List[Dict[str, Any]], 
        options: ChunkingOptions
    ) -> ChunkingResult:
        """
        Process chunks based on detected boundaries.
        
        This method implements the common logic for creating chunks based on
        the boundaries detected by the strategy-specific implementation.
        
        Args:
            text: Original text content
            boundaries: List of boundaries with positions
            options: Chunking options
            
        Returns:
            ChunkingResult with chunks and metadata
        """
        if not boundaries:
            # No boundaries found, fall back to simple chunking
            logger.debug(f"[{self.operation_id}] No boundaries detected, falling back to simple chunking")
            return self._simple_chunk_text(text, options, time.time())
        
        # Calculate max chars based on token limit
        max_chars = self._get_max_chars_from_tokens(options.max_tokens_per_chunk * options.safety_margin)
        overlap_chars = self._get_max_chars_from_tokens(options.overlap_tokens)
        
        chunks = []
        metadata = []
        current_chunk = ""
        current_pos = 0
        
        # Track context for preservation (used by subclasses)
        context_tracker = self._create_context_tracker()
        
        try:
            # Process text by boundaries
            for i, boundary in enumerate(boundaries):
                # Skip if we've already processed past this point
                if boundary['index'] < current_pos:
                    continue
                
                # Get content up to this boundary
                content_before = text[current_pos:boundary['index']]
                
                # Update context tracker with this boundary
                self._update_context_tracker(context_tracker, boundary)
                
                # Find the next boundary or end of text
                next_boundary_idx = next((j for j, b in enumerate(boundaries) 
                                       if j > i and b['index'] > boundary.get('end', boundary['index'])), 
                                      len(boundaries))
                
                if next_boundary_idx < len(boundaries):
                    section_end = boundaries[next_boundary_idx]['index']
                else:
                    section_end = len(text)
                
                # Get the section text
                section_text = text[boundary['index']:section_end]
                
                # Check if adding this section would exceed chunk size
                if current_chunk and len(current_chunk) + len(section_text) > max_chars:
                    # Finalize current chunk
                    chunks.append(current_chunk)
                    
                    metadata.append(
                        self._create_chunk_metadata(
                            current_chunk, 
                            options, 
                            len(chunks) - 1, 
                            current_pos, 
                            context_tracker,
                            False
                        )
                    )
                    
                    # Start new chunk with context and overlap
                    new_chunk = self._create_new_chunk_with_context(
                        current_chunk, 
                        context_tracker, 
                        overlap_chars, 
                        boundary, 
                        options
                    )
                    
                    current_chunk = new_chunk
                else:
                    # Add section to current chunk
                    if not current_chunk:
                        current_chunk = section_text
                    else:
                        current_chunk += section_text
                
                current_pos = section_end
            
            # Handle any remaining content
            self._process_remaining_content(
                text, current_pos, current_chunk, chunks, metadata, 
                context_tracker, max_chars, options
            )
            
            # Update total chunks in metadata
            metadata_builder = MetadataBuilder()
            updated_metadata = []
            for md in metadata:
                # Create a new metadata instance with updated total_chunks
                updated_md = metadata_builder \
                    .with_index(md.index) \
                    .with_total_chunks(len(chunks)) \
                    .with_format(md.format) \
                    .with_token_count(md.token_count) \
                    .with_char_count(md.char_count) \
                    .with_overlap(md.has_overlap, md.overlap_from) \
                    .with_content_slice(md.content_slice[0], md.content_slice[1]) \
                    .with_preserved_context(md.preserved_context) \
                    .build()
                updated_metadata.append(updated_md)
            metadata = updated_metadata
            
            # Add metadata comments if enabled
            if options.add_metadata_comments:
                chunks = self._add_metadata_comments(chunks, options)
            
            # Validate to ensure no chunk exceeds the token limit
            validation_start = time.time()
            safe_chunks = self._validate_chunks(chunks, options)
            self.stats["validation_time"] = time.time() - validation_start
            
            # Update metadata if chunks were modified
            if len(safe_chunks) != len(metadata):
                logger.info(
                    f"[{self.operation_id}] Chunks were further split during validation: "
                    f"{len(chunks)} -> {len(safe_chunks)}"
                )
                metadata = self._rebuild_metadata_for_chunks(safe_chunks, options)
            
            return ChunkingResult(
                chunks=safe_chunks,
                chunk_metadata=metadata,
                original_length=len(text),
                detected_format=self.format_type,
                token_estimation_strategy=options.token_strategy,
                chunking_strategy=self._get_chunking_strategy(),
                processing_time=0.0,  # Will be set by caller
                total_token_count=sum(md.token_count for md in metadata),
                operation_id=self.operation_id
            )
            
        except Exception as e:
            logger.error(
                f"[{self.operation_id}] Error in chunk processing: {str(e)}",
                exc_info=True
            )
            # In case of error, return what we've processed so far
            if chunks:
                logger.warning(
                    f"[{self.operation_id}] Returning {len(chunks)} chunks processed before error"
                )
                # Create new metadata instances with updated total_chunks
                metadata_builder = MetadataBuilder()
                updated_metadata = []
                for md in metadata:
                    updated_md = metadata_builder \
                        .with_index(md.index) \
                        .with_total_chunks(len(chunks)) \
                        .with_format(md.format) \
                        .with_token_count(md.token_count) \
                        .with_char_count(md.char_count) \
                        .with_overlap(md.has_overlap, md.overlap_from) \
                        .with_content_slice(md.content_slice[0], md.content_slice[1]) \
                        .with_preserved_context(md.preserved_context) \
                        .build()
                    updated_metadata.append(updated_md)
                
                return ChunkingResult(
                    chunks=chunks,
                    chunk_metadata=updated_metadata,
                    original_length=len(text),
                    detected_format=self.format_type,
                    token_estimation_strategy=options.token_strategy,
                    chunking_strategy=self._get_chunking_strategy(),
                    processing_time=time.time() - time.time(),
                    total_token_count=sum(md.token_count for md in metadata),
                    operation_id=self.operation_id
                )
            
            # No chunks processed, fall back to simple chunking
            logger.warning(f"[{self.operation_id}] No chunks processed, falling back to simple chunking")
            return self._simple_chunk_text(text, options, time.time())
    
    def _create_chunk_metadata(
        self,
        chunk: str,
        options: ChunkingOptions,
        index: int,
        current_pos: int,
        context_tracker: Optional[Dict[str, Any]],
        has_overlap: bool
    ) -> ChunkMetadata:
        """
        Create metadata for a chunk.
        
        Args:
            chunk: Chunk content
            options: Chunking options
            index: Chunk index
            current_pos: Current position in original text
            context_tracker: Context tracking information
            has_overlap: Whether this chunk has overlap with previous
            
        Returns:
            ChunkMetadata for the chunk
        """
        metadata_builder = MetadataBuilder() \
            .with_index(index) \
            .with_format(self.format_type) \
            .with_token_count(estimate_tokens(chunk, options.token_strategy)) \
            .with_char_count(len(chunk)) \
            .with_overlap(has_overlap)
        
        # Try to find the slice in original text
        # This might not be exact due to transformations
        try:
            if index == 0:
                slice_start = 0
            else:
                # Look for the chunk or a portion of it in the original text
                slice_start = self._find_chunk_position(chunk, current_pos)
            
            metadata_builder.with_content_slice(
                slice_start,
                slice_start + len(chunk)
            )
        except Exception as e:
            logger.warning(f"Could not determine precise content slice: {e}")
            metadata_builder.with_content_slice(0, len(chunk))
        
        # Add preserved context if available
        if context_tracker:
            preserved_context = self._get_preserved_context(context_tracker)
            if preserved_context:
                metadata_builder.with_preserved_context(preserved_context)
        
        return metadata_builder.build()
    
    def _find_chunk_position(self, chunk: str, current_pos: int) -> int:
        """
        Find the position of a chunk in the original text.
        
        Args:
            chunk: Chunk to locate
            current_pos: Current position in text processing
            
        Returns:
            Starting position of the chunk
        """
        # For larger chunks, use a sample portion for matching
        if len(chunk) > 1000:
            # Use the first 200 chars that aren't common metadata
            for start_idx in range(min(200, len(chunk))):
                sample = chunk[start_idx:start_idx+200]
                if not sample.startswith("/*") and not sample.startswith("//"):
                    return current_pos - len(chunk) + start_idx
        
        # Default fallback
        return current_pos - len(chunk)
    
    def _process_remaining_content(
        self,
        text: str,
        current_pos: int,
        current_chunk: str,
        chunks: List[str],
        metadata: List[ChunkMetadata],
        context_tracker: Optional[Dict[str, Any]],
        max_chars: int,
        options: ChunkingOptions
    ) -> None:
        """
        Process any remaining content after boundary processing.
        
        Args:
            text: Original text
            current_pos: Current position in text
            current_chunk: Current chunk being assembled
            chunks: List of chunks to append to
            metadata: List of metadata to append to
            context_tracker: Context tracking information
            max_chars: Maximum characters per chunk
            options: Chunking options
        """
        # Add any remaining content
        if current_pos < len(text):
            remaining = text[current_pos:]
            
            if current_chunk and len(current_chunk) + len(remaining) > max_chars:
                # Finalize current chunk
                chunks.append(current_chunk)
                
                metadata.append(
                    self._create_chunk_metadata(
                        current_chunk, 
                        options, 
                        len(chunks) - 1, 
                        current_pos - len(current_chunk), 
                        context_tracker,
                        len(chunks) > 1
                    )
                )
                
                # Add remaining as new chunk
                chunks.append(remaining)
                
                metadata.append(
                    self._create_chunk_metadata(
                        remaining, 
                        options, 
                        len(chunks) - 1, 
                        current_pos, 
                        context_tracker,
                        False
                    )
                )
            else:
                # Add to current chunk
                if not current_chunk:
                    current_chunk = remaining
                else:
                    current_chunk += remaining
                
                if current_chunk:
                    chunks.append(current_chunk)
                    
                    metadata.append(
                        self._create_chunk_metadata(
                            current_chunk, 
                            options, 
                            len(chunks) - 1, 
                            current_pos - len(current_chunk) + len(remaining), 
                            context_tracker,
                            len(chunks) > 1
                        )
                    )
        elif current_chunk:
            # Add final chunk
            chunks.append(current_chunk)
            
            metadata.append(
                self._create_chunk_metadata(
                    current_chunk, 
                    options, 
                    len(chunks) - 1, 
                    current_pos - len(current_chunk), 
                    context_tracker,
                    len(chunks) > 1
                )
            )
    
    def _rebuild_metadata_for_chunks(
        self, 
        chunks: List[str], 
        options: ChunkingOptions
    ) -> List[ChunkMetadata]:
        """
        Rebuild metadata when chunks have been modified.
        
        Args:
            chunks: List of final chunks
            options: Chunking options
            
        Returns:
            List of rebuilt metadata objects
        """
        metadata = []
        for i, chunk in enumerate(chunks):
            metadata_builder = MetadataBuilder() \
                .with_index(i) \
                .with_total_chunks(len(chunks)) \
                .with_format(self.format_type) \
                .with_token_count(estimate_tokens(chunk, options.token_strategy)) \
                .with_char_count(len(chunk)) \
                .with_overlap(i > 0)
            
            metadata.append(metadata_builder.build())
            
        return metadata
    
    def _create_context_tracker(self) -> Optional[Dict[str, Any]]:
        """
        Create a context tracker for preserving context between chunks.
        
        Returns:
            Context tracker dictionary or None
        """
        return None
    
    def _update_context_tracker(self, context_tracker: Optional[Dict[str, Any]], boundary: Dict[str, Any]) -> None:
        """
        Update context tracker with information from current boundary.
        
        Args:
            context_tracker: Context tracker to update
            boundary: Current boundary information
        """
        pass
    
    def _get_preserved_context(self, context_tracker: Dict[str, Any]) -> str:
        """
        Get preserved context from context tracker.
        
        Args:
            context_tracker: Context tracker
            
        Returns:
            Preserved context string
        """
        return ""
    
    def _create_new_chunk_with_context(
        self,
        previous_chunk: str,
        context_tracker: Optional[Dict[str, Any]],
        overlap_chars: int,
        boundary: Dict[str, Any],
        options: ChunkingOptions
    ) -> str:
        """
        Create a new chunk with context and overlap.
        
        Args:
            previous_chunk: Previous chunk content
            context_tracker: Context tracker
            overlap_chars: Number of chars to overlap
            boundary: Current boundary information
            options: Chunking options
            
        Returns:
            New chunk text with context
        """
        # Start with context information
        if context_tracker:
            preserved_context = self._get_preserved_context(context_tracker)
            if preserved_context:
                chunk = f"/* Context from previous chunk */\n{preserved_context}\n/* Current content */\n"
            else:
                chunk = "/* Continued from previous chunk */\n"
        else:
            chunk = "/* Continued from previous chunk */\n"
        
        # Add overlap if configured
        if overlap_chars > 0 and len(previous_chunk) > overlap_chars:
            overlap_content = previous_chunk[-overlap_chars:]
            chunk += overlap_content
        
        return chunk
    
    def _add_metadata_comments(self, chunks: List[str], options: ChunkingOptions) -> List[str]:
        """
        Add format-appropriate metadata comments to chunks.
        
        Args:
            chunks: List of chunks to add metadata to
            options: Chunking options
            
        Returns:
            Chunks with metadata comments
        """
        if not chunks:
            return chunks
        
        total_chunks = len(chunks)
        
        format_comments = {
            ContentFormat.JSON: lambda i, chunk: chunk,  # JSON already has metadata
            ContentFormat.MARKDOWN: lambda i, chunk: f"<!-- CHUNK {i+1}/{total_chunks} -->\n{chunk}",
            ContentFormat.XML: lambda i, chunk: f"<!-- XML CHUNK {i+1}/{total_chunks} -->\n{chunk}",
            ContentFormat.CODE: lambda i, chunk: f"// CODE CHUNK {i+1}/{total_chunks}\n{chunk}",
            ContentFormat.LOGS: lambda i, chunk: f"# LOG CHUNK {i+1}/{total_chunks}\n{chunk}",
            ContentFormat.CSV: lambda i, chunk: f"# CSV CHUNK {i+1}/{total_chunks}\n{chunk}",
            ContentFormat.TEXT: lambda i, chunk: f"/* CHUNK {i+1}/{total_chunks} */\n{chunk}",
        }
        
        comment_func = format_comments.get(self.format_type, format_comments[ContentFormat.TEXT])
        return [comment_func(i, chunk) for i, chunk in enumerate(chunks)]
    
    def _validate_chunks(self, chunks: List[str], options: ChunkingOptions) -> List[str]:
        """
        Ensure all chunks are within token limits.
        
        Args:
            chunks: List of chunks to validate
            options: Chunking options
            
        Returns:
            List of validated chunks
        """
        validated_chunks = []
        max_tokens = options.max_tokens_per_chunk
        
        for chunk in chunks:
            # Skip empty chunks
            if not chunk or not chunk.strip():
                continue
                
            token_count = estimate_tokens(chunk, options.token_strategy)
            
            if token_count <= max_tokens:
                # Chunk is within limits
                validated_chunks.append(chunk)
            else:
                # Chunk is too large, split it
                logger.warning(
                    f"[{self.operation_id}] Chunk exceeds token limit "
                    f"({token_count} > {max_tokens}), splitting..."
                )
                
                # Use emergency chunking
                sub_chunks = self._simple_chunk_text_list(chunk, options)
                validated_chunks.extend(sub_chunks)
        
        return validated_chunks
    
    def _get_max_chars_from_tokens(self, tokens: int) -> int:
        """
        Convert token count to approximate character count.
        
        Args:
            tokens: Number of tokens
            
        Returns:
            Approximate character count
        """
        # Conservative character-to-token ratio (varies by language and content)
        chars_per_token = 4.0
        return math.ceil(tokens * chars_per_token)
    
    def _simple_chunk_text(self, text: str, options: ChunkingOptions, start_time: float) -> ChunkingResult:
        """
        Simple fallback chunking when more sophisticated methods fail.
        
        Args:
            text: Text to chunk
            options: Chunking options
            start_time: Start time for processing
            
        Returns:
            ChunkingResult with chunks
        """
        chunks = self._simple_chunk_text_list(text, options)
        
        # Create metadata for chunks
        metadata = []
        for i, chunk in enumerate(chunks):
            metadata_builder = MetadataBuilder() \
                .with_index(i) \
                .with_total_chunks(len(chunks)) \
                .with_format(self.format_type) \
                .with_token_count(estimate_tokens(chunk, options.token_strategy)) \
                .with_char_count(len(chunk)) \
                .with_overlap(i > 0)
            
            metadata.append(metadata_builder.build())
        
        logger.info(
            f"[{self.operation_id}] Simple chunking completed with {len(chunks)} chunks "
            f"in {time.time() - start_time:.3f}s"
        )
        
        return ChunkingResult(
            chunks=chunks,
            chunk_metadata=metadata,
            original_length=len(text),
            detected_format=self.format_type,
            token_estimation_strategy=options.token_strategy,
            chunking_strategy=ChunkingStrategy.FIXED_SIZE,  # Always use fixed size for fallback
            processing_time=time.time() - start_time,
            total_token_count=sum(md.token_count for md in metadata),
            operation_id=self.operation_id
        )
    
    def _simple_chunk_text_list(self, text: str, options: ChunkingOptions) -> List[str]:
        """
        Simple fallback chunking when more sophisticated methods fail.
        
        Args:
            text: Text to chunk
            options: Chunking options
            
        Returns:
            List of text chunks
        """
        try:
            max_chars = self._get_max_chars_from_tokens(options.max_tokens_per_chunk * options.safety_margin)
            overlap_chars = self._get_max_chars_from_tokens(options.overlap_tokens)
            
            # Simple line-based chunking
            lines = text.split('\n')
            chunks = []
            current_chunk = ""
            
            for line in lines:
                # Check if adding this line would exceed chunk size
                if current_chunk and len(current_chunk) + len(line) + 1 > max_chars:
                    chunks.append(current_chunk)
                    
                    # Start new chunk with overlap
                    if overlap_chars > 0 and len(current_chunk) > overlap_chars:
                        current_chunk = f"// Continued...\n{current_chunk[-overlap_chars:]}\n{line}"
                    else:
                        current_chunk = f"// Continued...\n{line}"
                else:
                    # Add line to current chunk
                    if current_chunk:
                        current_chunk += f"\n{line}"
                    else:
                        current_chunk = line
                        
                # Handle very long lines by splitting them
                while len(current_chunk) > max_chars * 1.5:
                    split_point = max_chars - overlap_chars
                    chunks.append(current_chunk[:split_point])
                    current_chunk = f"// Continued...\n{current_chunk[split_point-overlap_chars:]}"
            
            # Add final chunk if not empty
            if current_chunk:
                chunks.append(current_chunk)
                
            # Add metadata comments for clarity
            chunks = [
                f"/* EMERGENCY CHUNK {i+1}/{len(chunks)} */\n{chunk}"
                for i, chunk in enumerate(chunks)
            ]
            
            logger.info(f"[{self.operation_id}] Simple chunking produced {len(chunks)} chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"[{self.operation_id}] Error in simple chunking: {str(e)}", exc_info=True)
            # Last resort: return single chunk or chunks of fixed size
            if len(text) <= options.max_tokens_per_chunk * 4:
                return [text]
            else:
                # Split into equal-sized chunks
                chunk_size = max(100, options.max_tokens_per_chunk // 2)
                return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    
    @abstractmethod
    def _get_chunking_strategy(self) -> ChunkingStrategy:
        """
        Get the chunking strategy this implementation represents.
        
        Returns:
            ChunkingStrategy enum value
        """
        raise NotImplementedError("Subclasses must implement _get_chunking_strategy")
</file>

<file path="strategies/fixed_size.py">
"""
Fixed-size chunking strategy implementation.

This module provides an optimized chunking strategy that splits text into chunks
of approximately equal size while respecting natural text boundaries when possible.
The strategy aims to balance chunk size consistency with semantic coherence.

Usage:
    from enterprise_chunker.strategies.fixed_size import FixedSizeChunkingStrategy
    
    strategy = FixedSizeChunkingStrategy()
    result = strategy.chunk(text, options)
"""

import re
import math
import time
import logging
from functools import lru_cache
from typing import List, Dict, Any, Optional, Tuple, Set, ClassVar, cast

from enterprise_chunker.strategies.base import BaseChunkingStrategy
from enterprise_chunker.models.enums import ContentFormat, ChunkingStrategy
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.exceptions import BoundaryDetectionError
from enterprise_chunker.utils.performance import timing_decorator

# Configure logging
logger = logging.getLogger(__name__)


class FixedSizeChunkingStrategy(BaseChunkingStrategy):
    """
    Fixed-size chunking strategy that splits text into chunks of approximately equal size.
    
    This strategy divides text into chunks with consistent token counts while attempting
    to preserve natural language boundaries (paragraphs, sentences, etc.) to maintain
    readability and coherence of content.
    
    Features:
        - Adaptive boundary detection based on text structure
        - Preservation of key context between chunks
        - Optimized search algorithms for natural boundaries
        - Configurable overlap between chunks
    """
    
    # Class constants for boundary priorities
    BOUNDARY_PRIORITY: ClassVar[Dict[str, int]] = {
        "paragraph": 10,
        "linebreak": 8,
        "sentence": 6,
        "clause": 4,
        "word": 2,
        "character": 0
    }
    
    # Patterns for boundary detection
    BOUNDARY_PATTERNS: ClassVar[Dict[str, re.Pattern]] = {
        "paragraph": re.compile(r'\n\s*\n'),
        "sentence": re.compile(r'[.!?][\s"\')\]]'),
        "clause": re.compile(r'[,;:][\s]'),
    }
    
    def __init__(self, format_type: ContentFormat = ContentFormat.TEXT):
        """
        Initialize the fixed-size chunking strategy.
        
        Args:
            format_type: Content format this strategy handles (defaults to TEXT)
        """
        super().__init__(format_type)
        self.boundary_stats: Dict[str, int] = {
            "paragraph": 0,
            "linebreak": 0,
            "sentence": 0,
            "clause": 0,
            "word": 0,
            "character": 0
        }
    
    @timing_decorator
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        Create boundaries at regular intervals with adjustments for natural breaks.
        
        This method divides the text into chunks by:
        1. Calculating target position based on token estimate
        2. Finding natural boundaries near each target position
        3. Creating boundary markers with context information
        
        Args:
            text: Text to analyze and chunk
            options: Chunking configuration options
            
        Returns:
            List of boundary dictionaries with position and metadata
            
        Raises:
            BoundaryDetectionError: If boundary detection fails
        """
        try:
            # Reset boundary statistics
            self.boundary_stats = {key: 0 for key in self.boundary_stats}
            
            # Calculate approximate character length for the target token count
            chars_per_token = 4.0  # Conservative estimate
            target_chars = int(options.max_tokens_per_chunk * options.safety_margin * chars_per_token)
            
            boundaries = []
            
            # Always add a boundary at the start
            boundaries.append({
                'index': 0,
                'end': 0,
                'text': '',
                'type': 'start',
                'is_fixed_boundary': True
            })
            
            # If text is small enough, don't add more boundaries
            if len(text) <= target_chars:
                logger.debug(f"[{self.operation_id}] Text length ({len(text)}) <= target chunk size ({target_chars}), using single chunk")
                return boundaries
            
            # Calculate optimal chunk count based on text length
            optimal_chunks = max(2, math.ceil(len(text) / target_chars))
            logger.debug(f"[{self.operation_id}] Planning approximately {optimal_chunks} chunks of ~{target_chars} chars each")
            
            current_pos = 0
            chunk_number = 1
            
            while current_pos < len(text):
                # Calculate next target position
                next_pos = min(current_pos + target_chars, len(text))
                
                # If we're near the end, just use the end
                if next_pos >= len(text) - target_chars / 4:
                    break
                    
                # Find natural boundary near the target position
                boundary_pos, boundary_type = self._find_natural_boundary(text, next_pos, target_chars // 4)
                
                # Record the boundary type used for analytics
                self.boundary_stats[boundary_type] += 1
                
                # Add boundary with metadata
                boundaries.append({
                    'index': boundary_pos,
                    'end': boundary_pos,
                    'text': text[boundary_pos:min(boundary_pos + 20, len(text))].replace('\n', '\\n'),
                    'type': boundary_type,
                    'chunk_number': chunk_number,
                    'is_fixed_boundary': True
                })
                
                # Move to the next position
                current_pos = boundary_pos
                chunk_number += 1
            
            # Log boundary selection statistics
            self._log_boundary_stats(len(boundaries) - 1)  # Subtract 1 for start boundary
            
            return boundaries
            
        except Exception as e:
            logger.error(f"[{self.operation_id}] Boundary detection failed: {str(e)}", exc_info=True)
            raise BoundaryDetectionError(f"Failed to detect fixed-size boundaries: {str(e)}") from e
    
    def _find_natural_boundary(self, text: str, target_pos: int, search_range: int) -> Tuple[int, str]:
        """
        Find a natural boundary near the target position using efficient string operations.
        
        This method searches for the most natural text boundary (paragraph, sentence, etc.)
        within the specified range of the target position, prioritizing boundaries that
        would create more coherent chunks.
        
        Args:
            text: Text to analyze
            target_pos: Target position for the boundary
            search_range: Range to search around the target position
            
        Returns:
            Tuple of (position of the natural boundary, boundary type)
        """
        # Define the search range
        start = max(0, target_pos - search_range)
        end = min(len(text), target_pos + search_range)
        search_text = text[start:end]
        
        # Look for paragraph breaks (highest priority)
        para_pos = search_text.rfind('\n\n', 0, target_pos - start)
        if para_pos != -1:
            return start + para_pos + 2, "paragraph"  # Include both newlines
            
        para_pos = search_text.find('\n\n', target_pos - start)
        if para_pos != -1:
            return start + para_pos + 2, "paragraph"
            
        # Look for single line breaks
        nl_pos = search_text.rfind('\n', 0, target_pos - start)
        if nl_pos != -1:
            return start + nl_pos + 1, "linebreak"
            
        nl_pos = search_text.find('\n', target_pos - start)
        if nl_pos != -1:
            return start + nl_pos + 1, "linebreak"
            
        # Look for sentence endings
        sentence_matches = list(re.finditer(r'[.!?][\s"\')\]]', search_text))
        if sentence_matches:
            # Find closest sentence boundary to target position
            closest_match = min(sentence_matches, 
                               key=lambda m: abs((start + m.start()) - target_pos))
            return start + closest_match.start() + 1, "sentence"
            
        # Look for clause boundaries (commas, etc.)
        clause_matches = list(re.finditer(r'[,;:][\s]', search_text))
        if clause_matches:
            closest_match = min(clause_matches, 
                               key=lambda m: abs((start + m.start()) - target_pos))
            return start + closest_match.start() + 1, "clause"
            
        # Look for word boundaries (spaces) - use rfind/find for efficiency
        space_pos = search_text.rfind(' ', 0, target_pos - start)
        if space_pos != -1:
            return start + space_pos + 1, "word"
            
        space_pos = search_text.find(' ', target_pos - start)
        if space_pos != -1:
            return start + space_pos + 1, "word"
            
        # If no natural boundary found, use the target position
        return target_pos, "character"
    
    def _log_boundary_stats(self, total_boundaries: int) -> None:
        """
        Log statistics about boundary types selected during chunking.
        
        Args:
            total_boundaries: Total number of boundaries created
        """
        if total_boundaries == 0:
            return
            
        stats_message = f"[{self.operation_id}] Boundary selection stats: "
        parts = []
        
        for boundary_type, count in sorted(
            self.boundary_stats.items(), 
            key=lambda x: self.BOUNDARY_PRIORITY.get(x[0], 0), 
            reverse=True
        ):
            if count > 0:
                percentage = (count / total_boundaries) * 100
                parts.append(f"{boundary_type}={count} ({percentage:.1f}%)")
                
        stats_message += ", ".join(parts)
        logger.debug(stats_message)
    
    def _create_context_tracker(self) -> Optional[Dict[str, Any]]:
        """
        Create a context tracker for fixed-size chunking.
        
        Returns:
            Context tracker dictionary with initial values
        """
        return {
            'last_split_position': 0, 
            'chunk_number': 0,
            'boundary_types': [],
            'last_chunk_text': '',
            'chunk_sizes': []
        }
    
    def _update_context_tracker(self, context_tracker: Optional[Dict[str, Any]], boundary: Dict[str, Any]) -> None:
        """
        Update context tracker with information from current boundary.
        
        Args:
            context_tracker: Context tracker to update
            boundary: Current boundary information
        """
        if not context_tracker:
            return
            
        context_tracker['last_split_position'] = boundary['index']
        context_tracker['chunk_number'] += 1
        
        if 'type' in boundary:
            context_tracker['boundary_types'].append(boundary['type'])
    
    def _get_preserved_context(self, context_tracker: Dict[str, Any]) -> str:
        """
        Get preserved context from context tracker.
        
        Args:
            context_tracker: Context tracker
            
        Returns:
            Preserved context string with continuation information
        """
        if not context_tracker:
            return ""
            
        chunk_num = context_tracker.get('chunk_number', 0)
        if chunk_num <= 0:
            return ""
            
        # Create informative continuation message
        boundary_types = context_tracker.get('boundary_types', [])
        if boundary_types and len(boundary_types) >= chunk_num:
            boundary_type = boundary_types[-1]
            return f"[Continued from chunk {chunk_num} at {boundary_type} boundary]"
        else:
            return f"[Continued from chunk {chunk_num}]"
    
    def _create_new_chunk_with_context(
        self,
        previous_chunk: str,
        context_tracker: Optional[Dict[str, Any]],
        overlap_chars: int,
        boundary: Dict[str, Any],
        options: ChunkingOptions
    ) -> str:
        """
        Create a new chunk with context and optimized overlap.
        
        This method intelligently selects overlap content based on natural
        language boundaries to maintain coherence between chunks.
        
        Args:
            previous_chunk: Previous chunk content
            context_tracker: Context tracker
            overlap_chars: Number of chars to overlap
            boundary: Current boundary information
            options: Chunking options
            
        Returns:
            New chunk text with context
        """
        chunk = ""
        
        # Add context comment
        chunk_num = context_tracker.get('chunk_number', 0) if context_tracker else 0
        boundary_type = boundary.get('type', 'unknown')
        
        # Create more informative continuation header
        chunk += f"/* Continued from chunk {chunk_num}"
        if boundary_type != 'unknown':
            chunk += f" (split at {boundary_type})"
        chunk += " */\n"
        
        # Add overlap content if configured
        if overlap_chars > 0 and len(previous_chunk) > overlap_chars:
            # Look for a natural boundary within the overlap section
            overlap_text = previous_chunk[-overlap_chars:]
            
            # Try to find an optimal starting point in the overlap
            overlap_start = 0
            
            # Try to find a paragraph break
            para_match = self.BOUNDARY_PATTERNS["paragraph"].search(overlap_text)
            if para_match:
                # Found paragraph break, use text after it
                overlap_start = para_match.end()
                chunk += overlap_text[overlap_start:]
            else:
                # No paragraph break, try sentence boundary
                sentence_match = self.BOUNDARY_PATTERNS["sentence"].search(overlap_text)
                if sentence_match:
                    # Found sentence break, use text after it
                    overlap_start = sentence_match.end()
                    chunk += overlap_text[overlap_start:]
                else:
                    # Try clause boundary
                    clause_match = self.BOUNDARY_PATTERNS["clause"].search(overlap_text)
                    if clause_match:
                        overlap_start = clause_match.end()
                        chunk += overlap_text[overlap_start:]
                    else:
                        # No natural boundary, search for word boundary
                        space_pos = overlap_text.find(' ', len(overlap_text) // 2)
                        if space_pos != -1:
                            chunk += overlap_text[space_pos+1:]
                        else:
                            # No good boundaries found, use the whole overlap
                            chunk += overlap_text
        
        return chunk
    
    @lru_cache(maxsize=1)
    def _get_chunking_strategy(self) -> ChunkingStrategy:
        """
        Get the chunking strategy this implementation represents.
        
        Returns:
            ChunkingStrategy.FIXED_SIZE enum value
        """
        return ChunkingStrategy.FIXED_SIZE
    
    def get_strategy_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about the chunking process.
        
        Returns:
            Dictionary with statistics about boundary selection and chunking
        """
        return {
            "boundary_stats": self.boundary_stats,
            "processing_times": self.stats,
            "operation_id": self.operation_id
        }
</file>

<file path="strategies/formats/__init__.py">
"""
Package initialization for format-specific chunking strategies.

This module provides centralized access to all format-specific chunking strategies
in the enterprise_chunker framework. Each strategy is optimized for a particular
file format to ensure semantically appropriate chunking.
"""

from __future__ import annotations

# Import format-specific strategies for easier access
from enterprise_chunker.strategies.formats.json_chunker import JsonChunkingStrategy
from enterprise_chunker.strategies.formats.markdown_chunker import MarkdownChunkingStrategy
from enterprise_chunker.strategies.formats.react_vue_chunker import ReactVueChunkingStrategy
from enterprise_chunker.strategies.formats.smalltalk_chunker import SmalltalkChunkingStrategy, SmalltalkDialect

# Additional format-specific strategies would be imported here
# from enterprise_chunker.strategies.formats.xml_chunker import XmlChunkingStrategy
# from enterprise_chunker.strategies.formats.code_chunker import CodeChunkingStrategy
# from enterprise_chunker.strategies.formats.logs_chunker import LogsChunkingStrategy
# from enterprise_chunker.strategies.formats.csv_chunker import CsvChunkingStrategy

# List available strategies for introspection
__all__ = [
    "JsonChunkingStrategy",
    "MarkdownChunkingStrategy",
    "ReactVueChunkingStrategy", 
    "SmalltalkChunkingStrategy",
    "SmalltalkDialect",  # Also export the dialect enum
    # Commented out until these are implemented
    # "XmlChunkingStrategy",
    # "CodeChunkingStrategy",
    # "LogsChunkingStrategy",
    # "CsvChunkingStrategy",
]

# Pre-compile heavy regex patterns at module load time for better performance
# This ensures patterns are compiled only once, not on each class instantiation
ReactVueChunkingStrategy._init_patterns() if hasattr(ReactVueChunkingStrategy, '_init_patterns') else None
SmalltalkChunkingStrategy._init_patterns() if hasattr(SmalltalkChunkingStrategy, '_init_patterns') else None
</file>

<file path="strategies/formats/json_chunker.py">
"""
JSON-specific chunking strategy implementation.

This module provides specialized chunking for JSON content with structure preservation
for optimal semantic processing of JSON arrays and objects.
"""

import re
import json
import logging
from typing import List, Dict, Any, Optional, Tuple, ClassVar
from functools import lru_cache

from enterprise_chunker.strategies.base import BaseChunkingStrategy
from enterprise_chunker.models.enums import ContentFormat, ChunkingStrategy, TokenEstimationStrategy
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.utils.token_estimation import estimate_tokens
from enterprise_chunker.models.chunk_metadata import MetadataBuilder, ChunkingResult

# Configure logging
logger = logging.getLogger(__name__)


class JsonChunkingStrategy(BaseChunkingStrategy):
    """
    Strategy for chunking JSON content with structural preservation.
    
    This strategy intelligently handles different JSON structures (arrays and objects)
    by preserving their structure while splitting into appropriate chunks. It maintains
    metadata about the chunking process within the JSON itself for reconstruction.
    
    Features:
    - Array chunking with smart item grouping
    - Object chunking with property preservation
    - Structure-aware overlap between chunks
    - JSON metadata for reconstruction
    """
    
    def __init__(self):
        """Initialize the JSON chunking strategy."""
        super().__init__(ContentFormat.JSON)
    
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        This method is overridden but not used for JSON chunking
        because we parse the JSON and handle it differently.
        
        Args:
            text: JSON text to analyze
            options: Chunking options
            
        Returns:
            List of boundary dictionaries (empty for JSON)
        """
        # JSON chunking doesn't use traditional boundaries,
        # so we return an empty list to trigger special handling
        return []
    
    def chunk(self, text: str, options: ChunkingOptions) -> ChunkingResult:
        """
        Override the chunking process for JSON with structure-aware processing.
        
        This method parses the JSON and routes to specialized handling based on
        whether the root is an array, object, or simple value.
        
        Args:
            text: JSON text to chunk
            options: Chunking options
            
        Returns:
            ChunkingResult with chunks and metadata
        """
        try:
            # Try to parse the JSON first
            parsed_json = json.loads(text)
            
            # Handle different JSON structures
            if isinstance(parsed_json, list):
                logger.debug(f"[{self.operation_id}] Processing JSON array with {len(parsed_json)} items")
                return self._chunk_json_array(text, parsed_json, options)
            elif isinstance(parsed_json, dict):
                logger.debug(f"[{self.operation_id}] Processing JSON object with {len(parsed_json)} properties")
                return self._chunk_json_object(text, parsed_json, options)
            else:
                # Simple value, return as single chunk
                logger.debug(f"[{self.operation_id}] Processing simple JSON value")
                return self._create_single_chunk_result(text, options)
        except json.JSONDecodeError as e:
            # Not valid JSON, fall back to parent implementation
            logger.warning(f"[{self.operation_id}] Invalid JSON ({str(e)}), falling back to semantic chunking")
            from enterprise_chunker.strategies.semantic import SemanticChunkingStrategy
            semantic_strategy = SemanticChunkingStrategy()
            semantic_strategy.set_operation_id(self.operation_id)
            return semantic_strategy.chunk(text, options)
    
    def _chunk_json_array(
        self, 
        text: str, 
        parsed_array: list, 
        options: ChunkingOptions
    ) -> ChunkingResult:
        """
        Chunk a JSON array with smart item grouping.
        
        Args:
            text: Original JSON text
            parsed_array: Parsed JSON array
            options: Chunking options
            
        Returns:
            ChunkingResult with chunks
        """
        if not parsed_array:
            # Empty array, return as is
            return self._create_single_chunk_result(text, options)
            
        # Calculate maximum size based on token limit
        max_tokens = options.max_tokens_per_chunk * options.safety_margin
        
        chunks = []
        metadata = []
        current_group = []
        current_tokens = 2  # Start with [] tokens
        
        for item in parsed_array:
            # Serialize this item
            item_json = json.dumps(item)
            item_tokens = estimate_tokens(item_json, options.token_strategy) + 1  # +1 for comma
            
            # Check if adding this item would exceed limits
            if current_group and (current_tokens + item_tokens > max_tokens):
                # Finalize current group
                group_json = json.dumps(current_group)
                chunk_json = self._create_json_chunk(
                    "json_array", 
                    group_json, 
                    len(chunks),
                    len(chunks) > 0  # has overlap
                )
                
                chunks.append(chunk_json)
                
                # Create metadata
                metadata_builder = self._create_metadata_builder(
                    index=len(chunks) - 1,
                    content=chunk_json,
                    token_strategy=options.token_strategy
                )
                metadata.append(metadata_builder.build())
                
                # Start new group with overlap if configured
                if options.overlap_tokens > 0:
                    overlap_items = []
                    overlap_tokens = 0
                    
                    # Add last items from previous group for context
                    for prev_item in reversed(current_group):
                        prev_json = json.dumps(prev_item)
                        prev_tokens = estimate_tokens(prev_json, options.token_strategy) + 1
                        
                        if overlap_tokens + prev_tokens <= options.overlap_tokens:
                            overlap_items.insert(0, prev_item)
                            overlap_tokens += prev_tokens
                        else:
                            break
                            
                    current_group = overlap_items
                    current_tokens = 2 + overlap_tokens  # [] + overlap
                else:
                    current_group = []
                    current_tokens = 2  # []
                    
            # Add item to current group
            current_group.append(item)
            current_tokens += item_tokens
            
        # Add final group if not empty
        if current_group:
            group_json = json.dumps(current_group)
            chunk_json = self._create_json_chunk(
                "json_array", 
                group_json, 
                len(chunks),
                len(chunks) > 0  # has overlap
            )
            
            chunks.append(chunk_json)
            
            metadata_builder = self._create_metadata_builder(
                index=len(chunks) - 1,
                content=chunk_json,
                token_strategy=options.token_strategy
            )
            metadata.append(metadata_builder.build())
            
        # Update total chunks in metadata
        for md in metadata:
            md.total_chunks = len(chunks)
            
        # Update chunk info in the JSON
        updated_chunks = []
        for i, chunk in enumerate(chunks):
            try:
                chunk_obj = json.loads(chunk)
                chunk_obj["_chunk_info"]["total"] = len(chunks)
                chunk_obj["_chunk_info"]["has_overlap"] = i > 0 and options.overlap_tokens > 0
                updated_chunks.append(json.dumps(chunk_obj))
            except json.JSONDecodeError:
                logger.warning(f"[{self.operation_id}] Failed to update chunk info for chunk {i}, using original")
                updated_chunks.append(chunk)
            
        return self._create_chunking_result(
            chunks=updated_chunks,
            metadata=metadata,
            original_text=text,
            options=options
        )
    
    def _chunk_json_object(
        self, 
        text: str, 
        parsed_obj: dict, 
        options: ChunkingOptions
    ) -> ChunkingResult:
        """
        Chunk a JSON object with smart property grouping.
        
        Args:
            text: Original JSON text
            parsed_obj: Parsed JSON object
            options: Chunking options
            
        Returns:
            ChunkingResult with chunks
        """
        # Skip chunking if the object is small enough
        total_tokens = estimate_tokens(text, options.token_strategy)
        if total_tokens <= options.max_tokens_per_chunk * options.safety_margin:
            return self._create_single_chunk_result(text, options)
        
        # Calculate max tokens to use
        max_tokens = options.max_tokens_per_chunk * options.safety_margin
        
        chunks = []
        metadata = []
        current_obj = {}
        current_tokens = 2  # Start with {} tokens
        
        # Sort properties by size to better distribute them
        properties = []
        for key, value in parsed_obj.items():
            prop_json = f'"{key}":{json.dumps(value)}'
            prop_tokens = estimate_tokens(prop_json, options.token_strategy) + 1  # +1 for comma
            properties.append((key, value, prop_tokens))
            
        # Sort properties: smaller ones first makes most efficient packing
        properties.sort(key=lambda x: x[2])
        
        for key, value, prop_tokens in properties:
            # Skip metadata if present
            if key == "_chunk_info":
                continue
                
            # Check if adding this property would exceed limits
            if current_obj and (current_tokens + prop_tokens > max_tokens):
                # Finalize current object
                chunk_info = {
                    "index": len(chunks),
                    "type": "json_object",
                    "properties": list(current_obj.keys())
                }
                
                if len(chunks) > 0:
                    chunk_info["continued"] = True
                    chunk_info["previous_chunk"] = len(chunks) - 1
                
                current_obj["_chunk_info"] = chunk_info
                
                chunk_json = json.dumps(current_obj)
                chunks.append(chunk_json)
                
                # Add metadata
                metadata_builder = self._create_metadata_builder(
                    index=len(chunks) - 1,
                    content=chunk_json,
                    token_strategy=options.token_strategy
                )
                metadata.append(metadata_builder.build())
                
                # Start new object
                current_obj = {
                    "_chunk_info": {
                        "continued": True,
                        "previous_chunk": len(chunks) - 1
                    }
                }
                current_tokens = estimate_tokens(json.dumps(current_obj), options.token_strategy)
                
            # Add property to current object
            current_obj[key] = value
            current_tokens += prop_tokens
            
        # Add final object if not empty
        if current_obj and any(key != "_chunk_info" for key in current_obj):
            if "_chunk_info" not in current_obj:
                current_obj["_chunk_info"] = {}
                
            current_obj["_chunk_info"].update({
                "index": len(chunks),
                "type": "json_object",
                "properties": [k for k in current_obj.keys() if k != "_chunk_info"],
                "final": True
            })
            
            chunk_json = json.dumps(current_obj)
            chunks.append(chunk_json)
            
            metadata_builder = self._create_metadata_builder(
                index=len(chunks) - 1,
                content=chunk_json,
                token_strategy=options.token_strategy
            )
            metadata.append(metadata_builder.build())
            
        # Update total chunks in metadata
        for md in metadata:
            md.total_chunks = len(chunks)
            
        # Update chunk info in the JSON
        updated_chunks = []
        for i, chunk in enumerate(chunks):
            try:
                chunk_obj = json.loads(chunk)
                chunk_obj["_chunk_info"]["total"] = len(chunks)
                updated_chunks.append(json.dumps(chunk_obj))
            except json.JSONDecodeError:
                # In case of an error, keep the chunk as is
                updated_chunks.append(chunk)
            
        return self._create_chunking_result(
            chunks=updated_chunks,
            metadata=metadata,
            original_text=text,
            options=options
        )
    
    def _create_json_chunk(
        self, 
        chunk_type: str, 
        data_json: str, 
        index: int,
        has_overlap: bool = False
    ) -> str:
        """
        Create a JSON chunk with metadata.
        
        Args:
            chunk_type: Type of JSON chunk (array or object)
            data_json: JSON data string
            index: Chunk index
            has_overlap: Whether this chunk has overlap with previous
            
        Returns:
            JSON string with chunk info
        """
        return f'{{"_chunk_info":{{"index":{index},"type":"{chunk_type}","has_overlap":{str(has_overlap).lower()}}},"data":{data_json}}}'
    
    def _create_metadata_builder(
        self,
        index: int,
        content: str,
        token_strategy: TokenEstimationStrategy
    ) -> MetadataBuilder:
        """
        Create a metadata builder for a JSON chunk.
        
        Args:
            index: Chunk index
            content: Chunk content
            token_strategy: Token estimation strategy
            
        Returns:
            Configured MetadataBuilder
        """
        return MetadataBuilder() \
            .with_index(index) \
            .with_format(ContentFormat.JSON) \
            .with_token_count(estimate_tokens(content, token_strategy)) \
            .with_char_count(len(content)) \
            .with_overlap(index > 0)
    
    def _create_single_chunk_result(self, text: str, options: ChunkingOptions) -> ChunkingResult:
        """
        Create a single-chunk result for small JSON.
        
        Args:
            text: JSON text
            options: Chunking options
            
        Returns:
            ChunkingResult with a single chunk
        """
        token_count = estimate_tokens(text, options.token_strategy)
        
        # FIX: Don't pass a tuple to with_content_slice, pass separate arguments
        metadata_builder = MetadataBuilder() \
            .with_index(0) \
            .with_total_chunks(1) \
            .with_format(ContentFormat.JSON) \
            .with_token_count(token_count) \
            .with_char_count(len(text)) \
            .with_content_slice(0, len(text))  # Passing separate arguments instead of a tuple
        
        return self._create_chunking_result(
            chunks=[text],
            metadata=[metadata_builder.build()],
            original_text=text,
            options=options
        )
    
    def _create_chunking_result(
        self,
        chunks: List[str],
        metadata: List,
        original_text: str,
        options: ChunkingOptions
    ) -> ChunkingResult:
        """
        Create a ChunkingResult.
        
        Args:
            chunks: List of chunks
            metadata: List of chunk metadata
            original_text: Original text
            options: Chunking options
            
        Returns:
            ChunkingResult
        """
        import time
        
        return ChunkingResult(
            chunks=chunks,
            chunk_metadata=metadata,
            original_length=len(original_text),
            detected_format=ContentFormat.JSON,
            token_estimation_strategy=options.token_strategy,
            chunking_strategy=self._get_chunking_strategy(),
            processing_time=time.time(),  # Will be adjusted by caller
            total_token_count=sum(md.token_count for md in metadata),
            operation_id=self.operation_id
        )
    
    @lru_cache(maxsize=1)
    def _get_chunking_strategy(self) -> ChunkingStrategy:
        """
        Get the chunking strategy this implementation represents.
        
        Returns:
            ChunkingStrategy enum value
        """
        return ChunkingStrategy.STRUCTURAL
</file>

<file path="strategies/formats/markdown_chunker.py">
"""
Markdown-specific chunking strategy implementation
"""

import re
import logging
import time
from typing import List, Dict, Any, Optional, Tuple

from enterprise_chunker.strategies.base import BaseChunkingStrategy
from enterprise_chunker.models.enums import ContentFormat, ChunkingStrategy
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.utils.token_estimation import estimate_tokens
from enterprise_chunker.patterns.regex_patterns import RegexPatterns

# Configure logging
logger = logging.getLogger(__name__)


class MarkdownChunkingStrategy(BaseChunkingStrategy):
    """
    Strategy for chunking Markdown content with header/section awareness
    """
    
    def __init__(self):
        """Initialize the Markdown chunking strategy"""
        super().__init__(ContentFormat.MARKDOWN)
    
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        Detect Markdown boundaries like headers, lists, and code blocks
        
        Args:
            text: Markdown text to analyze
            options: Chunking options
            
        Returns:
            List of boundary dictionaries
        """
        boundaries = []
        
        # Get Markdown patterns
        md_patterns = RegexPatterns.get_format_patterns(ContentFormat.MARKDOWN)
        
        # Pattern for Markdown headers (# Header, ## Header, etc.)
        for match in md_patterns['headers'].finditer(text):
            level = len(match.group(1))  # Number of # characters
            header_text = match.group(2)  # The header text
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'is_header': True,
                'level': level,
                'header_text': header_text
            })
        
        # Pattern for alt-style headers (underlined with === or ---)
        for match in md_patterns['alt_headers'].finditer(text):
            # Determine level: = is level 1, - is level 2
            level = 1 if '=' in match.group(2) else 2
            header_text = match.group(1)
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'is_header': True,
                'level': level,
                'header_text': header_text
            })
        
        # Add list items as boundaries (lower priority)
        for match in md_patterns['list_items'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'is_header': False,
                'is_list_item': True
            })
        
        # Add numbered list items
        for match in md_patterns['numbered_list'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'is_header': False,
                'is_list_item': True
            })
        
        # Add code blocks (important to preserve)
        for match in md_patterns['code_blocks'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'is_header': False,
                'is_code_block': True
            })
        
        # Add blockquotes
        for match in md_patterns['blockquotes'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'is_header': False,
                'is_blockquote': True
            })
        
        # Add horizontal rules
        for match in md_patterns['horizontal_rule'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'is_header': False,
                'is_hr': True
            })
        
        # Sort boundaries by position
        boundaries.sort(key=lambda x: x['index'])
        
        # Add a boundary at the beginning if none exists
        if not boundaries or boundaries[0]['index'] > 0:
            boundaries.insert(0, {
                'index': 0,
                'end': 0,
                'text': '',
                'is_header': False
            })
        
        return boundaries
    
    def _create_context_tracker(self) -> Optional[Dict[str, Any]]:
        """
        Create a context tracker for Markdown
        
        Returns:
            Context tracker dictionary
        """
        return {
            'headers': [],  # Stack of headers for hierarchy tracking
            'current_level': 0  # Current header level
        }
    
    def _update_context_tracker(self, context_tracker: Optional[Dict[str, Any]], boundary: Dict[str, Any]):
        """
        Update context tracker with information from current boundary
        
        Args:
            context_tracker: Context tracker to update
            boundary: Current boundary information
        """
        if not context_tracker:
            return
            
        # Update headers stack
        if boundary.get('is_header', False) and 'level' in boundary and 'header_text' in boundary:
            level = boundary['level']
            header_text = boundary['header_text']
            
            # Remove headers at same or lower level
            while (context_tracker['headers'] and 
                   context_tracker['headers'][-1]['level'] >= level):
                context_tracker['headers'].pop()
            
            # Add the current header
            context_tracker['headers'].append({
                'level': level,
                'text': header_text
            })
            
            # Update current level
            context_tracker['current_level'] = level
    
    def _get_preserved_context(self, context_tracker: Dict[str, Any]) -> str:
        """
        Get preserved context from context tracker for Markdown
        
        Args:
            context_tracker: Context tracker
            
        Returns:
            Markdown context string showing hierarchy
        """
        if not context_tracker or not context_tracker.get('headers'):
            return ""
        
        # Build a tree of headers to show context
        context_lines = []
        for i, header in enumerate(context_tracker['headers']):
            # Format each header with proper indent based on its level
            # Use markdown format with proper heading level
            context_lines.append(f"{'#' * header['level']} {header['text']}")
            
            # Add a blank line after each header except the last one
            if i < len(context_tracker['headers']) - 1:
                context_lines.append("")
                
        return "\n".join(context_lines)
    
    def _create_new_chunk_with_context(
        self,
        previous_chunk: str,
        context_tracker: Optional[Dict[str, Any]],
        overlap_chars: int,
        boundary: Dict[str, Any],
        options: ChunkingOptions
    ) -> str:
        """
        Create a new chunk with Markdown-specific context and overlap
        
        Args:
            previous_chunk: Previous chunk content
            context_tracker: Context tracker
            overlap_chars: Number of chars to overlap
            boundary: Current boundary information
            options: Chunking options
            
        Returns:
            New chunk text with context
        """
        # Start with context headers
        if context_tracker and context_tracker.get('headers'):
            preserved_context = self._get_preserved_context(context_tracker)
            chunk = f"<!-- Context from previous chunk -->\n{preserved_context}\n\n<!-- Current content -->\n"
        else:
            chunk = "<!-- Continued from previous chunk -->\n"
        
        # Add overlap if configured - for Markdown we want to be careful 
        # about not splitting in the middle of a structure
        if overlap_chars > 0 and len(previous_chunk) > overlap_chars:
            # Don't add overlap for header boundaries to avoid duplication
            if not boundary.get('is_header', False):
                overlap_content = previous_chunk[-overlap_chars:]
                
                # Try to find a clean paragraph break
                para_match = re.search(r'\n\s*\n', overlap_content)
                if para_match:
                    # Start from the paragraph break for cleaner context
                    overlap_content = overlap_content[para_match.end():]
                
                chunk += overlap_content
        
        return chunk
    
    def _add_metadata_comments(self, chunks: List[str], options: ChunkingOptions) -> List[str]:
        """
        Add Markdown-specific metadata comments to chunks
        
        Args:
            chunks: List of chunks to add metadata to
            options: Chunking options
            
        Returns:
            Chunks with metadata comments
        """
        if not chunks:
            return chunks
        
        total_chunks = len(chunks)
        return [
            f"<!-- MARKDOWN CHUNK {i+1}/{total_chunks} -->\n{chunk}"
            for i, chunk in enumerate(chunks)
        ]
    
    def _get_chunking_strategy(self) -> ChunkingStrategy:
        """
        Get the chunking strategy this implementation represents
        
        Returns:
            ChunkingStrategy enum value
        """
        return ChunkingStrategy.STRUCTURAL
</file>

<file path="strategies/formats/react_vue_chunker.py">
"""
React and Vue component-specific chunking strategy implementation.

This module provides specialized chunking for React (JSX/TSX) and Vue (SFC) components,
maintaining component structure awareness for optimal semantic chunking.
"""

import re
import logging
import time
from typing import List, Dict, Any, Optional, Tuple, ClassVar, Pattern
from functools import lru_cache

from enterprise_chunker.strategies.base import BaseChunkingStrategy
from enterprise_chunker.models.enums import ContentFormat, ChunkingStrategy
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.utils.token_estimation import estimate_tokens
from enterprise_chunker.patterns.regex_patterns import RegexPatterns
from enterprise_chunker.models.chunk_metadata import ChunkingResult

# Configure logging
logger = logging.getLogger(__name__)


class ReactVueChunkingStrategy(BaseChunkingStrategy):
    """
    Strategy for chunking React and Vue component files with structure awareness.
    
    This handles JSX, TSX for React and .vue single-file components with intelligent
    boundary detection that preserves component structure and semantics.
    
    Features:
        - React component detection (functional and class components)
        - Vue SFC section detection (template, script, style)
        - Component hierarchy preservation
        - Hook and lifecycle method detection
        - Intelligent context preservation between chunks
    """
    
    # Cached pattern sets for improved performance
    _VUE_PATTERNS: ClassVar[Dict[str, Pattern]] = {
        # Vue template section
        'template_tag': re.compile(r'(<template.*?>)([\s\S]*?)(<\/template>)'),
        'script_tag': re.compile(r'(<script.*?>)([\s\S]*?)(<\/script>)'),
        'style_tag': re.compile(r'(<style.*?>)([\s\S]*?)(<\/style>)'),
        'component_tag': re.compile(r'<([A-Z][A-Za-z0-9]*|[a-z]+-[a-z-]+)[^>]*>', re.MULTILINE),
        
        # Options API patterns
        'component_definition': re.compile(r'export\s+default\s*{'),
        'props_section': re.compile(r'props\s*:'),
        'data_section': re.compile(r'data\s*\(\)\s*{'),
        'computed_section': re.compile(r'computed\s*:'),
        'methods_section': re.compile(r'methods\s*:'),
        'watch_section': re.compile(r'watch\s*:'),
        'lifecycle_method': re.compile(r'(mounted|created|beforeMount|updated|beforeUpdate)\s*\(\)'),
        
        # Composition API patterns
        'setup_function': re.compile(r'setup\s*\([^)]*\)\s*{'),
        'ref_definition': re.compile(r'const\s+\w+\s*=\s*ref\('),
        'reactive_definition': re.compile(r'const\s+\w+\s*=\s*reactive\('),
        'computed_definition': re.compile(r'const\s+\w+\s*=\s*computed\('),
        'watch_definition': re.compile(r'watch\(\w+,'),
        'lifecycle_hook': re.compile(r'on(Mounted|Created|BeforeMount|Updated|BeforeUpdate)\('),
        
        # CSS patterns
        'css_selector': re.compile(r'([.#]?[a-zA-Z0-9_-]+(?:\s+[.#]?[a-zA-Z0-9_-]+)*)\s*{', re.MULTILINE),
        'media_query': re.compile(r'@media\s+[^{]+{', re.MULTILINE),
    }
    
    _REACT_PATTERNS: ClassVar[Dict[str, Pattern]] = {
        # Import and export patterns
        'import': re.compile(r'^import\s+.*?;?\s*$', re.MULTILINE),
        'export': re.compile(r'^export\s+default\s+\w+', re.MULTILINE),
        
        # Component definitions
        'func_component': re.compile(
            r'^(?:export\s+)?(?:const|function)\s+([A-Z]\w*)\s*(?:=\s*(?:\([^)]*\)|)\s*=>|(?:\([^)]*\)))', 
            re.MULTILINE
        ),
        'class_component': re.compile(
            r'^(?:export\s+)?class\s+([A-Z]\w*)\s+extends\s+(?:React\.)?Component',
            re.MULTILINE
        ),
        
        # Hooks and handlers
        'hook': re.compile(
            r'(?:const|let)\s+\[(\w+),\s*set(\w+)\]\s*=\s*useState|' +
            r'const\s+\w+\s*=\s*useRef|' +
            r'useEffect\(\s*\(\)\s*=>|' +
            r'useCallback\(\s*\([^)]*\)\s*=>|' +
            r'useMemo\(\s*\(\)\s*=>',
            re.MULTILINE
        ),
        'event_handler': re.compile(
            r'(?:const|function)\s+handle\w+\s*=|' +
            r'(?:const|function)\s+on[A-Z]\w+\s*=|' +
            r'(?:public|private|protected)?\s*\w+\s*=\s*(?:\([^)]*\)|event)',
            re.MULTILINE
        ),
        
        # JSX
        'jsx_block': re.compile(r'return\s*\(\s*<', re.MULTILINE | re.DOTALL),
    }
    
    def __init__(self):
        """Initialize the React/Vue chunking strategy with cached patterns."""
        super().__init__(ContentFormat.CODE)
        self.is_vue = False
    
    def chunk(self, text: str, options: ChunkingOptions) -> ChunkingResult:
        """
        Override to detect Vue or React before chunking.
        
        Args:
            text: Component code to chunk
            options: Chunking options
            
        Returns:
            ChunkingResult with chunks and metadata
        """
        # Detect if this is a Vue component
        self.is_vue = self._detect_vue_component(text)
        logger.debug(f"[{self.operation_id}] Detected component type: {'Vue' if self.is_vue else 'React'}")
        
        # Continue with normal chunking process
        return super().chunk(text, options)
    
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        Detect boundaries in React or Vue components based on component type.
        
        Args:
            text: Component code to analyze
            options: Chunking options
            
        Returns:
            List of boundary dictionaries
        """
        if self.is_vue:
            return self._detect_vue_boundaries(text)
        else:
            return self._detect_react_boundaries(text)
    
    def _detect_vue_boundaries(self, text: str) -> List[Dict[str, Any]]:
        """
        Detect boundaries in Vue single-file components.
        
        Args:
            text: Vue component code
            
        Returns:
            List of boundary dictionaries
        """
        boundaries = []
        
        # Find the main section tags: <template>, <script>, <style>
        template_match = self._VUE_PATTERNS['template_tag'].search(text)
        script_match = self._VUE_PATTERNS['script_tag'].search(text)
        style_match = self._VUE_PATTERNS['style_tag'].search(text)
        
        # Add template section
        if template_match:
            # Add template start tag
            boundaries.append({
                'index': template_match.start(1),
                'end': template_match.end(1),
                'text': template_match.group(1),
                'type': 'template_tag',
                'is_section': True
            })
            
            # Parse template content for component boundaries
            template_content = template_match.group(2)
            template_start = template_match.start(2)
            
            # Find component tags in template
            for match in self._VUE_PATTERNS['component_tag'].finditer(template_content):
                boundaries.append({
                    'index': template_start + match.start(),
                    'end': template_start + match.end(),
                    'text': match.group(0),
                    'type': 'component_tag',
                    'is_section': False
                })
                
            # Add template end tag
            boundaries.append({
                'index': template_match.start(3),
                'end': template_match.end(3),
                'text': template_match.group(3),
                'type': 'template_tag',
                'is_section': True
            })
        
        # Add script section
        if script_match:
            # Add script start tag
            boundaries.append({
                'index': script_match.start(1),
                'end': script_match.end(1),
                'text': script_match.group(1),
                'type': 'script_tag',
                'is_section': True
            })
            
            # Parse script content for JS structure
            script_content = script_match.group(2)
            script_start = script_match.start(2)
            
            # Options API patterns
            options_api_patterns = [
                ('component_definition', 'component_definition'),
                ('props_section', 'props_section'),
                ('data_section', 'data_section'),
                ('computed_section', 'computed_section'),
                ('methods_section', 'methods_section'),
                ('watch_section', 'watch_section'),
                ('lifecycle_method', 'lifecycle_method')
            ]
            
            # Composition API patterns
            composition_api_patterns = [
                ('setup_function', 'setup_function'),
                ('ref_definition', 'ref_definition'),
                ('reactive_definition', 'reactive_definition'),
                ('computed_definition', 'computed_definition'),
                ('watch_definition', 'watch_definition'),
                ('lifecycle_hook', 'lifecycle_hook')
            ]
            
            # Check all patterns
            for pattern_list in [options_api_patterns, composition_api_patterns]:
                for pattern_key, type_name in pattern_list:
                    pattern = self._VUE_PATTERNS[pattern_key]
                    for match in pattern.finditer(script_content):
                        boundaries.append({
                            'index': script_start + match.start(),
                            'end': script_start + match.end(),
                            'text': match.group(0),
                            'type': type_name,
                            'is_section': False
                        })
            
            # Add script end tag
            boundaries.append({
                'index': script_match.start(3),
                'end': script_match.end(3),
                'text': script_match.group(3),
                'type': 'script_tag',
                'is_section': True
            })
        
        # Add style section
        if style_match:
            # Add style start tag
            boundaries.append({
                'index': style_match.start(1),
                'end': style_match.end(1),
                'text': style_match.group(1),
                'type': 'style_tag',
                'is_section': True
            })
            
            # Add CSS structure boundaries
            style_content = style_match.group(2)
            style_start = style_match.start(2)
            
            # Find CSS selectors
            for match in self._VUE_PATTERNS['css_selector'].finditer(style_content):
                boundaries.append({
                    'index': style_start + match.start(),
                    'end': style_start + match.end(),
                    'text': match.group(0),
                    'type': 'css_selector',
                    'is_section': False
                })
            
            # Add media queries
            for match in self._VUE_PATTERNS['media_query'].finditer(style_content):
                boundaries.append({
                    'index': style_start + match.start(),
                    'end': style_start + match.end(),
                    'text': match.group(0),
                    'type': 'media_query',
                    'is_section': False
                })
            
            # Add style end tag
            boundaries.append({
                'index': style_match.start(3),
                'end': style_match.end(3),
                'text': style_match.group(3),
                'type': 'style_tag',
                'is_section': True
            })
        
        # Add a boundary at the beginning if none exists
        if not boundaries or boundaries[0]['index'] > 0:
            boundaries.insert(0, {
                'index': 0,
                'end': 0,
                'text': '',
                'type': 'start',
                'is_section': False
            })
        
        # Sort boundaries by position
        boundaries.sort(key=lambda x: x['index'])
        
        return boundaries
    
    def _detect_react_boundaries(self, text: str) -> List[Dict[str, Any]]:
        """
        Detect boundaries in React components.
        
        Args:
            text: React component code
            
        Returns:
            List of boundary dictionaries
        """
        boundaries = []
        
        # Find import statements
        for match in self._REACT_PATTERNS['import'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'type': 'import',
                'is_section': True
            })
        
        # Find function component definitions
        for match in self._REACT_PATTERNS['func_component'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'type': 'component_function',
                'is_section': True,
                'component_name': match.group(1) if match.groups() else None
            })
        
        # Find class component definitions
        for match in self._REACT_PATTERNS['class_component'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'type': 'component_class',
                'is_section': True,
                'component_name': match.group(1) if match.groups() else None
            })
        
        # Find hooks
        for match in self._REACT_PATTERNS['hook'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'type': 'hook',
                'is_section': False
            })
        
        # Find JSX elements in return statements
        for match in self._REACT_PATTERNS['jsx_block'].finditer(text):
            # Find the closing parenthesis
            open_count = 1
            start_pos = match.end()
            end_pos = start_pos
            
            for i in range(start_pos, len(text)):
                if text[i] == '(':
                    open_count += 1
                elif text[i] == ')':
                    open_count -= 1
                    if open_count == 0:
                        end_pos = i + 1
                        break
            
            if end_pos > start_pos:
                boundaries.append({
                    'index': match.start(),
                    'end': end_pos,
                    'text': text[match.start():end_pos],
                    'type': 'jsx_block',
                    'is_section': True
                })
        
        # Find event handlers and methods
        for match in self._REACT_PATTERNS['event_handler'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'type': 'event_handler',
                'is_section': False
            })
        
        # Find export statement
        for match in self._REACT_PATTERNS['export'].finditer(text):
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'type': 'export',
                'is_section': True
            })
            
        # Add a boundary at the beginning if none exists
        if not boundaries or boundaries[0]['index'] > 0:
            boundaries.insert(0, {
                'index': 0,
                'end': 0,
                'text': '',
                'type': 'start',
                'is_section': False
            })
            
        # Sort boundaries by position
        boundaries.sort(key=lambda x: x['index'])
        
        return boundaries
    
    def _create_context_tracker(self) -> Optional[Dict[str, Any]]:
        """
        Create a context tracker for React/Vue components.
        
        Returns:
            Context tracker dictionary
        """
        return {
            'imports': [],
            'component_name': None,
            'sections': [],
            'is_vue': self.is_vue,
            'current_section': None
        }
    
    def _update_context_tracker(self, context_tracker: Optional[Dict[str, Any]], boundary: Dict[str, Any]):
        """
        Update context tracker with React/Vue component information.
        
        Args:
            context_tracker: Context tracker to update
            boundary: Current boundary information
        """
        if not context_tracker:
            return
            
        boundary_type = boundary.get('type', '')
        boundary_text = boundary.get('text', '')
        
        # Track imports
        if boundary_type == 'import' and boundary_text:
            if boundary_text not in context_tracker['imports']:
                context_tracker['imports'].append(boundary_text)
        
        # Track component name
        if 'component_name' in boundary:
            context_tracker['component_name'] = boundary.get('component_name')
        
        # Track sections for Vue components
        if self.is_vue and boundary.get('is_section', False):
            if boundary_type in ['template_tag', 'script_tag', 'style_tag']:
                if boundary_text.startswith('</'):
                    # End tag
                    if context_tracker['current_section'] == boundary_type.replace('_tag', ''):
                        context_tracker['current_section'] = None
                else:
                    # Start tag
                    section_name = boundary_type.replace('_tag', '')
                    context_tracker['current_section'] = section_name
                    if section_name not in context_tracker['sections']:
                        context_tracker['sections'].append(section_name)
        
        # Track sections for React components
        if not self.is_vue and boundary.get('is_section', False):
            if boundary_type not in context_tracker['sections']:
                context_tracker['sections'].append(boundary_type)
    
    def _get_preserved_context(self, context_tracker: Dict[str, Any]) -> str:
        """
        Get preserved context for React/Vue components.
        
        Args:
            context_tracker: Context tracker
            
        Returns:
            Component context string
        """
        if not context_tracker:
            return ""
            
        context_lines = []
        
        # Add framework identifier
        if self.is_vue:
            context_lines.append("// Vue Component")
        else:
            context_lines.append("// React Component")
            
        # Add component name if available
        if context_tracker['component_name']:
            context_lines.append(f"// Component: {context_tracker['component_name']}")
            
        # Add import statements
        if context_tracker['imports']:
            context_lines.append("\n// Important imports:")
            # Limit to 5 most important imports to avoid context bloat
            top_imports = context_tracker['imports'][:5]
            context_lines.extend(top_imports)
            if len(context_tracker['imports']) > 5:
                context_lines.append(f"// ... and {len(context_tracker['imports']) - 5} more imports")
            
        # Add section information
        if context_tracker['sections']:
            context_lines.append("\n// Component structure:")
            for section in context_tracker['sections']:
                context_lines.append(f"// - {section}")
                
        return "\n".join(context_lines)
    
    def _create_new_chunk_with_context(
        self,
        previous_chunk: str,
        context_tracker: Optional[Dict[str, Any]],
        overlap_chars: int,
        boundary: Dict[str, Any],
        options: ChunkingOptions
    ) -> str:
        """
        Create a new chunk with React/Vue-specific context and overlap.
        
        Args:
            previous_chunk: Previous chunk content
            context_tracker: Context tracker
            overlap_chars: Number of chars to overlap
            boundary: Current boundary information
            options: Chunking options
            
        Returns:
            New chunk text with context
        """
        # Start with context
        if context_tracker:
            preserved_context = self._get_preserved_context(context_tracker)
            if preserved_context:
                chunk = f"// COMPONENT CONTEXT FROM PREVIOUS CHUNK\n{preserved_context}\n\n// CURRENT CODE CONTENT\n"
            else:
                chunk = "// Continued from previous chunk\n"
        else:
            chunk = "// Continued from previous chunk\n"
        
        # Add overlap if configured
        if overlap_chars > 0 and len(previous_chunk) > overlap_chars:
            # For React/Vue, try to start at a clean line
            overlap_content = previous_chunk[-overlap_chars:]
            
            # Look for newlines to start at a clean line
            line_match = re.search(r'\n', overlap_content)
            if line_match:
                # Start from the line break for cleaner context
                overlap_content = overlap_content[line_match.end():]
            
            chunk += overlap_content
        
        return chunk
    
    def _add_metadata_comments(self, chunks: List[str], options: ChunkingOptions) -> List[str]:
        """
        Add component-specific metadata comments to chunks.
        
        Args:
            chunks: List of chunks to add metadata to
            options: Chunking options
            
        Returns:
            Chunks with metadata comments
        """
        if not chunks:
            return chunks
        
        total_chunks = len(chunks)
        framework = "Vue" if self.is_vue else "React"
        
        return [
            f"// {framework} COMPONENT CHUNK {i+1}/{total_chunks}\n{chunk}"
            for i, chunk in enumerate(chunks)
        ]
    
    @lru_cache(maxsize=8)
    def _detect_vue_component(self, text: str) -> bool:
        """
        Detect if text is a Vue component.
        
        Args:
            text: Code to analyze
            
        Returns:
            True if it's a Vue component
        """
        # Look for Vue SFC structure
        has_template = '<template>' in text and '</template>' in text
        has_script = '<script>' in text and '</script>' in text
        
        # Alternative detection for Vue options API or composition API
        vue_api_patterns = [
            r'createApp\(',
            r'new Vue\(',
            r'Vue\.component\(',
            r'defineComponent\(',
            r'setup\s*\(',
            r'export\s+default\s*{[\s\S]*?(?:data|methods|computed|watch|props)\s*:'
        ]
        
        vue_api_match = any(re.search(pattern, text) for pattern in vue_api_patterns)
        
        return has_template or (has_script and vue_api_match)
    
    @lru_cache(maxsize=1)
    def _get_chunking_strategy(self) -> ChunkingStrategy:
        """
        Get the chunking strategy this implementation represents.
        
        Returns:
            ChunkingStrategy enum value
        """
        return ChunkingStrategy.STRUCTURAL
</file>

<file path="strategies/formats/smalltalk_chunker.py">
"""
Smalltalk-specific chunking strategy implementation.

Provides intelligent chunking for Smalltalk code with robust support for:
- Class and method boundary detection
- Dialect-specific syntax handling
- Context preservation across chunks
- Support for multiple Smalltalk dialects:
  - Squeak
  - Pharo
  - VisualWorks
  - GemStone
  - Dolphin
  - GNU Smalltalk
  - Amber
  - Cuis
"""

import re
import logging
import time
from typing import List, Dict, Any, Optional, Tuple, Set, ClassVar, Pattern
from functools import lru_cache

from enterprise_chunker.strategies.base import BaseChunkingStrategy
from enterprise_chunker.models.enums import ContentFormat, ChunkingStrategy
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.utils.token_estimation import estimate_tokens
from enterprise_chunker.patterns.regex_patterns import RegexPatterns
from enterprise_chunker.models.chunk_metadata import ChunkingResult

# Configure logging
logger = logging.getLogger(__name__)


class SmalltalkDialect:
    """Supported Smalltalk dialects"""
    STANDARD = "standard"
    VISUALWORKS = "visualworks"
    PHARO = "pharo"
    SQUEAK = "squeak"
    GEMSTONE = "gemstone"
    DOLPHIN = "dolphin"
    GNU = "gnu"
    AMBER = "amber"
    CUIS = "cuis"
    UNKNOWN = "unknown"


class SmalltalkChunkingStrategy(BaseChunkingStrategy):
    """
    Strategy for chunking Smalltalk code with method and class awareness.
    
    Supports multiple Smalltalk dialects with specific pattern recognition 
    and structure-preserving chunking. The strategy intelligently handles
    various Smalltalk syntax variants and file formats.
    
    Features:
        - Multi-dialect support with automatic detection
        - Class and method boundary preservation
        - Context tracking across chunks
        - Support for file-in format and source code
    """
    
    # Cached regex patterns compiled at module load time
    _CLASS_PATTERNS: ClassVar[Dict[str, Dict[str, Pattern]]] = {}
    _METHOD_PATTERNS: ClassVar[Dict[str, Dict[str, Pattern]]] = {}
    
    # Dialect detection patterns - compiled at class initialization
    _DIALECT_PATTERNS: ClassVar[Dict[str, List[str]]] = {
        SmalltalkDialect.VISUALWORKS: [
            r'!!.*?methodsFor:',       # VisualWorks file-in format
            r'!classDefinition:',      # VisualWorks class definition
            r'Smalltalk\s+at:',        # VisualWorks global access
            r'core\.Object',           # VisualWorks namespace
            r'VisualWorks',            # Direct mention
            r'parcel:\s+',             # VisualWorks parcels
            r'USERINIT\.ST',           # Common VisualWorks file
            r'abtVariableSubclass:',   # ABT VisualWorks extension
            r'NameSpace current'       # VisualWorks namespace access
        ],
        
        SmalltalkDialect.PHARO: [
            r'Pharo(?:\s+class)?',         # Direct mention
            r'SystemVersion',               # Pharo system class
            r'package:',                    # Pharo packaging
            r'Pragma',                      # Pharo pragmas
            r'ZnHttpServer',                # Pharo-specific class
            r'RPackage',                    # Pharo package system
            r'Metacello new',               # Pharo Metacello
            r'PharoKernel',                 # Pharo kernel
            r'Spec',                        # Pharo Spec UI framework
            r'SystemAnnouncer',             # Pharo announcer
            r'Iceberg',                     # Pharo Iceberg
            r'StPlayground'                 # Pharo Playground
        ],
        
        SmalltalkDialect.SQUEAK: [
            r'Squeak(?:\s+class)?',        # Direct mention
            r'SystemWindow',                # Squeak GUI
            r'Morphic',                     # Squeak Morphic
            r'ScriptingSystem',             # Squeak scripting
            r'SmalltalkImage',              # Squeak system
            r'WorldState',                  # Squeak world
            r'StandardSystemView',          # Squeak MVC
            r'initializeImageFormat',       # Squeak image format
            r'Project current',             # Squeak projects
            r'FileList open'                # Squeak file browser
        ],
        
        SmalltalkDialect.GEMSTONE: [
            r'GemStone(?:\s+class)?',      # Direct mention
            r'System\s+class',              # GemStone system class
            r'inDictionary:',               # GemStone-specific syntax
            r'transient',                   # GemStone transaction
            r'commitTransaction',           # GemStone commit
            r'UserGlobals',                 # GemStone globals
            r'abort:',                      # GemStone abort
            r'System myUserProfile',        # GemStone user profile
            r'SymbolDictionary',            # GemStone symbol dict
            r'GsObject'                     # GemStone object
        ],
        
        SmalltalkDialect.DOLPHIN: [
            r'Dolphin(?:\s+class)?',       # Direct mention
            r'package\s+paxVersion',        # Dolphin PAX
            r'DolphinSureFire',             # Dolphin SureFire
            r'Win32Constants',              # Dolphin Win32
            r'Filename class',              # Dolphin filename
            r'SmalltalkSystem current',     # Dolphin system
            r'SmalltalkParseError',         # Dolphin parse error
            r'ImageStripper',               # Dolphin image stripper
            r'DolphinDependencyChecker',    # Dolphin dependency checker
            r'AXControlLibrary'             # Dolphin ActiveX
        ],
        
        SmalltalkDialect.GNU: [
            r'GNU Smalltalk',              # Direct mention
            r'FileStream stderr',           # GNU stderr
            r'ObjectMemory',                # GNU memory
            r'CObject',                     # GNU C object
            r'CPtr',                        # GNU C pointer
            r'CString',                     # GNU C string
            r'CStringPtr',                  # GNU C string pointer
            r'CType',                       # GNU C type
            r'ArrayCType',                  # GNU array C type
            r'Namespace current'            # GNU namespace
        ],
        
        SmalltalkDialect.AMBER: [
            r'Amber(?:\s+class)?',         # Direct mention
            r'package\s*\(\s*[\'"].*?[\'"]', # Amber package
            r'defineClass:',                # Amber class definition
            r'exportAs:',                   # Amber export
            r'amber\/.*?\.js',              # Amber JS
            r'smalltalk\.classes',          # Amber classes
            r'^\s*return\s+self\._',        # Amber method pattern
            r'_[\w]+_',                     # Amber internal methods
            r'@AmberComponent',             # Amber component annotation
            r'HTMLCanvas'                   # Amber HTML canvas
        ],
        
        SmalltalkDialect.CUIS: [
            r'Cuis(?:\s+class)?',          # Direct mention
            r'ClassComment',                # Cuis class comment
            r'SystemWindow subclass',       # Cuis window
            r'Feature require:',            # Cuis feature
            r'\'Cuis\'',                    # Cuis string
            r'Feature\s+require:\s+#',      # Cuis feature
            r'Theme',                       # Cuis theme
            r'MorphicCanvas',               # Cuis morphic
            r'WorldState',                  # Cuis world
            r'ExtendedFileStream'           # Cuis file stream
        ]
    }
    
    # Initialize class and method pattern dictionaries
    @classmethod
    def _init_patterns(cls):
        """Initialize cached regex pattern dictionaries."""
        if not cls._CLASS_PATTERNS:
            cls._CLASS_PATTERNS = cls._compile_class_patterns()
            
        if not cls._METHOD_PATTERNS:
            cls._METHOD_PATTERNS = cls._compile_method_patterns()
    
    def __init__(self):
        """Initialize the Smalltalk chunking strategy with cached patterns."""
        super().__init__(ContentFormat.CODE)
        self.dialect = SmalltalkDialect.UNKNOWN  # Will detect the dialect during chunking
        
        # Initialize patterns at first instantiation
        if not self.__class__._CLASS_PATTERNS:
            self.__class__._init_patterns()
    
    def chunk(self, text: str, options: ChunkingOptions) -> ChunkingResult:
        """
        Override to detect Smalltalk dialect before chunking.
        
        Args:
            text: Smalltalk code to chunk
            options: Chunking options
            
        Returns:
            ChunkingResult with chunks and metadata
        """
        # Detect the Smalltalk dialect
        self.dialect = self._detect_smalltalk_dialect(text)
        logger.debug(f"[{self.operation_id}] Detected Smalltalk dialect: {self.dialect}")
        
        # Continue with normal chunking process
        return super().chunk(text, options)
    
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        Detect method and class boundaries in Smalltalk code.
        
        This method analyzes Smalltalk code to identify structural elements such as
        classes, methods, and blocks to create logical chunking boundaries.
        
        Args:
            text: Smalltalk code to analyze
            options: Chunking options
            
        Returns:
            List of boundary dictionaries
        """
        boundaries = []
        
        # Detect file format (fileIn vs. source code)
        is_file_in = self._is_file_in_format(text)
        
        # First, detect file chunks (for file-in format)
        if is_file_in:
            self._detect_file_in_chunks(text, boundaries)
        
        # Next, detect class definitions
        self._detect_class_definitions(text, boundaries)
        
        # Detect method definitions
        self._detect_method_definitions(text, boundaries)
        
        # Detect blocks and other structural elements
        self._detect_block_structures(text, boundaries)
        
        # Detect pragmas
        self._detect_pragmas(text, boundaries)
        
        # Detect trait compositions
        self._detect_trait_compositions(text, boundaries)
        
        # Add a boundary at the beginning if none exists
        if not boundaries or boundaries[0]['index'] > 0:
            boundaries.insert(0, {
                'index': 0,
                'end': 0,
                'text': '',
                'type': 'start',
            })
        
        # Sort boundaries by position
        boundaries.sort(key=lambda x: x['index'])
        
        return boundaries
    
    def _is_file_in_format(self, text: str) -> bool:
        """
        Detect if the text is in file-in format.
        
        Args:
            text: Smalltalk code to analyze
            
        Returns:
            True if file-in format, False otherwise
        """
        # Check for file-in format patterns
        file_in_markers = [
            r'^".*"!',                          # Filename marker
            r'!!.*!',                           # Chunk marker
            r'!classDefinition:',               # VisualWorks class definition
            r'^[-a-zA-Z0-9_]+\s+methodsFor:',   # Method category marker
        ]
        
        for marker in file_in_markers:
            if re.search(marker, text, re.MULTILINE):
                return True
                
        return False
    
    def _detect_file_in_chunks(self, text: str, boundaries: List[Dict[str, Any]]):
        """
        Detect file-in format chunk boundaries.
        
        Args:
            text: Smalltalk code to analyze
            boundaries: List to add boundaries to
        """
        # File-in chunks are typically delimited by ! characters
        
        # Filename/class markers
        filename_pattern = re.compile(r'^"([^"]+)"!', re.MULTILINE)
        for match in filename_pattern.finditer(text):
            filename = match.group(1)
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'type': 'filename_declaration',
                'is_declaration': True,
                'filename': filename
            })
        
        # VisualWorks style chunk declarations
        if self.dialect == SmalltalkDialect.VISUALWORKS:
            chunk_patterns = [
                # Method category chunk
                (r'!([A-Z][a-zA-Z0-9_]*)\s+methodsFor:\s*[\'"](.*?)[\'"](.*?)!', 'method_category'),
                # Class-side method category chunk
                (r'!([A-Z][a-zA-Z0-9_]*)\s+class\s+methodsFor:\s*[\'"](.*?)[\'"](.*?)!', 'class_method_category'),
                # Class definition chunk
                (r'!classDefinition:[\s\S]*?!', 'class_definition_chunk'),
                # Class comment chunk
                (r'!([A-Z][a-zA-Z0-9_]*)\s+commentStamp:[\s\S]*?!', 'class_comment'),
                # Initialize chunk
                (r'!([A-Z][a-zA-Z0-9_]*)\s+initialize!', 'initialize_chunk'),
                # General double-bang chunk
                (r'!![\s\S]*?!', 'generic_chunk')
            ]
            
            for pattern, chunk_type in chunk_patterns:
                for match in re.finditer(pattern, text, re.DOTALL):
                    # Extract class name and category if available
                    class_name = None
                    category = None
                    
                    if chunk_type in ('method_category', 'class_method_category') and match.groups():
                        class_name = match.group(1)
                        if len(match.groups()) > 1:
                            category = match.group(2)
                    
                    boundaries.append({
                        'index': match.start(),
                        'end': match.end(),
                        'text': match.group(0),
                        'type': chunk_type,
                        'is_chunk': True,
                        'class_name': class_name,
                        'category': category
                    })
        
        # Squeak/Pharo style chunk declarations
        elif self.dialect in (SmalltalkDialect.SQUEAK, SmalltalkDialect.PHARO, SmalltalkDialect.CUIS):
            # In Squeak/Pharo, methods often appear after class definitions with ---
            method_chunk_pattern = re.compile(r'^([\s\S]*?)(\r?\n---+\r?\n)', re.MULTILINE)
            for match in method_chunk_pattern.finditer(text):
                boundaries.append({
                    'index': match.start(2),  # Start of separator
                    'end': match.end(2),      # End of separator
                    'text': match.group(2),
                    'type': 'method_separator',
                    'is_separator': True
                })
    
    def _detect_class_definitions(self, text: str, boundaries: List[Dict[str, Any]]):
        """
        Detect class definition boundaries.
        
        Args:
            text: Smalltalk code to analyze
            boundaries: List to add boundaries to
        """
        # Get dialect-specific class patterns
        dialect_patterns = self._CLASS_PATTERNS.get(self.dialect, self._CLASS_PATTERNS[SmalltalkDialect.STANDARD])
        
        # Also try standard patterns as fallback
        patterns_to_try = [dialect_patterns]
        if self.dialect != SmalltalkDialect.STANDARD:
            patterns_to_try.append(self._CLASS_PATTERNS[SmalltalkDialect.STANDARD])
        
        for pattern_dict in patterns_to_try:
            for pattern_type, pattern in pattern_dict.items():
                for match in pattern.finditer(text, re.MULTILINE):
                    # Extract class information
                    superclass = match.group(1) if match.groups() else None
                    subclass = match.group(2) if len(match.groups()) > 1 else None
                    
                    # Extract variables if present
                    instance_vars = []
                    class_vars = []
                    
                    # Look for instance variables
                    if 'instanceVariableNames:' in match.group(0) or 'instVarNames:' in match.group(0):
                        var_match = re.search(r'(?:instanceVariableNames:|instVarNames:)\s*[\'"](.+?)[\'"]', match.group(0))
                        if var_match:
                            instance_vars = [v.strip() for v in var_match.group(1).split()]
                    
                    # Look for class variables
                    if 'classVariableNames:' in match.group(0) or 'classVars:' in match.group(0):
                        var_match = re.search(r'(?:classVariableNames:|classVars:)\s*[\'"](.+?)[\'"]', match.group(0))
                        if var_match:
                            class_vars = [v.strip() for v in var_match.group(1).split()]
                    
                    # Determine category/package
                    category = None
                    for cat_pattern in (r'category:\s*[\'"](.+?)[\'"]', r'package:\s*[\'"](.+?)[\'"]'):
                        cat_match = re.search(cat_pattern, match.group(0))
                        if cat_match:
                            category = cat_match.group(1)
                            break
                    
                    boundaries.append({
                        'index': match.start(),
                        'end': match.end(), 
                        'text': match.group(0),
                        'type': 'class_definition',
                        'is_class': True,
                        'definition_type': pattern_type,
                        'superclass': superclass,
                        'subclass': subclass,
                        'instance_vars': instance_vars,
                        'class_vars': class_vars,
                        'category': category
                    })
    
    def _detect_method_definitions(self, text: str, boundaries: List[Dict[str, Any]]):
        """
        Detect method definition boundaries.
        
        Args:
            text: Smalltalk code to analyze
            boundaries: List to add boundaries to
        """
        # In Smalltalk, methods are defined with the format:
        # methodName: arg1 with: arg2
        #    "method comment"
        #    | temp1 temp2 |
        #    statements.
        #    ^returnValue
        
        # Get dialect-specific method patterns
        method_patterns = self._METHOD_PATTERNS.get(self.dialect, self._METHOD_PATTERNS[SmalltalkDialect.STANDARD])
        
        for pattern_type, pattern in method_patterns.items():
            for match in pattern.finditer(text, re.MULTILINE):
                # Different patterns have different group structures
                method_name = None
                if match.groups():
                    method_name = match.group(1)
                
                # Skip if we can't determine the method name
                if not method_name:
                    continue
                
                # Determine method type and structure
                is_keyword_method = ':' in method_name
                is_binary_method = not is_keyword_method and re.match(r'^[-+*/~<>=@,%|&?!]', method_name.strip())
                is_unary_method = not (is_keyword_method or is_binary_method)
                
                # Check if this is a class-side method
                is_class_method = False
                if pattern_type == 'class_method':
                    is_class_method = True
                
                boundaries.append({
                    'index': match.start(),
                    'end': match.end(),
                    'text': match.group(0),
                    'type': 'method_definition',
                    'is_method': True,
                    'method_name': method_name.strip(),
                    'is_class_method': is_class_method,
                    'is_keyword_method': is_keyword_method,
                    'is_binary_method': is_binary_method,
                    'is_unary_method': is_unary_method
                })
    
    def _detect_block_structures(self, text: str, boundaries: List[Dict[str, Any]]):
        """
        Detect block structures in Smalltalk code.
        
        Args:
            text: Smalltalk code to analyze
            boundaries: List to add boundaries to
        """
        # Block structure patterns
        block_patterns = [
            # Block with arguments
            (r'\[\s*(?::\w+\s*)+\|', 'block_with_args'),
            # Block with temps
            (r'\[\s*\|\s*\w+(?:\s+\w+)*\s*\|', 'block_with_temps'),
            # Simple block
            (r'\[\s*(?!\|)(?!:)', 'simple_block'),
            # Block return
            (r'\^\s*\[', 'block_return'),
            # Method return
            (r'^\s*\^', 'method_return'),
            # Temp variables
            (r'^\s*\|\s*\w+(?:\s+\w+)*\s*\|', 'temp_vars')
        ]
        
        for pattern, block_type in block_patterns:
            for match in re.finditer(pattern, text, re.MULTILINE):
                boundaries.append({
                    'index': match.start(),
                    'end': match.end(),
                    'text': match.group(0),
                    'type': block_type,
                    'is_block': block_type.startswith('block'),
                    'is_return': 'return' in block_type
                })
    
    def _detect_pragmas(self, text: str, boundaries: List[Dict[str, Any]]):
        """
        Detect method pragmas in Smalltalk code.
        
        Args:
            text: Smalltalk code to analyze
            boundaries: List to add boundaries to
        """
        # Pragma patterns vary by dialect
        pragma_patterns = {
            SmalltalkDialect.PHARO: r'<(\w+)(?::.*?)?(?:\s+.*?)?>',
            SmalltalkDialect.SQUEAK: r'<(\w+)(?::.*?)?(?:\s+.*?)?>',
            SmalltalkDialect.VISUALWORKS: r'<(\w+)(?::.*?)?(?:\s+.*?)?>',
            SmalltalkDialect.GEMSTONE: r'<(\w+)(?::.*?)?(?:\s+.*?)?>',
            SmalltalkDialect.DOLPHIN: r'\[\s*<(\w+)(?::.*?)?(?:\s+.*?)?>\s*\]',
            SmalltalkDialect.GNU: r'<(\w+)(?::.*?)?(?:\s+.*?)?>',
            SmalltalkDialect.STANDARD: r'<(\w+)(?::.*?)?(?:\s+.*?)?>',
        }
        
        # Get the appropriate pattern
        pragma_pattern = pragma_patterns.get(self.dialect, pragma_patterns[SmalltalkDialect.STANDARD])
        
        for match in re.finditer(pragma_pattern, text, re.MULTILINE):
            pragma_name = match.group(1) if match.groups() else "unknown"
            
            boundaries.append({
                'index': match.start(),
                'end': match.end(),
                'text': match.group(0),
                'type': 'pragma',
                'is_pragma': True,
                'pragma_name': pragma_name
            })
    
    def _detect_trait_compositions(self, text: str, boundaries: List[Dict[str, Any]]):
        """
        Detect trait compositions in Smalltalk code (Pharo-specific).
        
        Args:
            text: Smalltalk code to analyze
            boundaries: List to add boundaries to
        """
        # Trait composition patterns (mainly for Pharo)
        if self.dialect in (SmalltalkDialect.PHARO, SmalltalkDialect.CUIS):
            trait_patterns = [
                # Trait composition
                (r'^\s*(\w+)\s+uses:\s+(\w+(?:\s*\+\s*\w+)*)', 'trait_composition'),
                # Trait method exclusion
                (r'^\s*(\w+)\s+uses:\s+\w+(?:\s*\+\s*\w+)*\s+-\s+\{(.*?)\}', 'trait_exclusion'),
                # Trait alias
                (r'^\s*(\w+)\s+uses:\s+\w+(?:\s*\+\s*\w+)*\s+@\s+\{(.*?)\}', 'trait_alias'),
            ]
            
            for pattern, trait_type in trait_patterns:
                for match in re.finditer(pattern, text, re.MULTILINE):
                    class_name = match.group(1) if match.groups() else None
                    
                    boundaries.append({
                        'index': match.start(),
                        'end': match.end(),
                        'text': match.group(0),
                        'type': trait_type,
                        'is_trait': True,
                        'class_name': class_name
                    })
    
    def _create_context_tracker(self) -> Optional[Dict[str, Any]]:
        """
        Create a context tracker for Smalltalk code.
        
        Returns:
            Context tracker dictionary
        """
        return {
            'current_class': None,
            'current_superclass': None,
            'current_category': None,
            'current_protocol': None,
            'instance_vars': [],
            'class_vars': [],
            'dialect': self.dialect,
            'current_method': None,
            'is_class_side': False,
            'traits': [],
            'in_file_chunk': False,
            'file_chunk_type': None
        }
    
    def _update_context_tracker(self, context_tracker: Optional[Dict[str, Any]], boundary: Dict[str, Any]):
        """
        Update context tracker with Smalltalk class and method information.
        
        Args:
            context_tracker: Context tracker to update
            boundary: Current boundary information
        """
        if not context_tracker:
            return
            
        boundary_type = boundary.get('type', '')
        
        # Update class information
        if boundary_type == 'class_definition':
            context_tracker['current_class'] = boundary.get('subclass')
            context_tracker['current_superclass'] = boundary.get('superclass')
            context_tracker['instance_vars'] = boundary.get('instance_vars', [])
            context_tracker['class_vars'] = boundary.get('class_vars', [])
            context_tracker['current_category'] = boundary.get('category')
            context_tracker['is_class_side'] = False  # Reset to instance side
            
        # Update method information
        elif boundary_type == 'method_definition':
            context_tracker['current_method'] = boundary.get('method_name')
            context_tracker['is_class_side'] = boundary.get('is_class_method', False)
            
        # Update chunk information (for file-in format)
        elif 'is_chunk' in boundary and boundary.get('is_chunk'):
            context_tracker['in_file_chunk'] = True
            context_tracker['file_chunk_type'] = boundary_type
            
            # Update class from chunk if available
            if 'class_name' in boundary and boundary['class_name']:
                context_tracker['current_class'] = boundary['class_name']
            
            # Update protocol/category from chunk if available
            if 'category' in boundary and boundary['category']:
                context_tracker['current_protocol'] = boundary['category']
                
            # Set class-side flag
            if boundary_type == 'class_method_category':
                context_tracker['is_class_side'] = True
            elif boundary_type == 'method_category':
                context_tracker['is_class_side'] = False
                
        # Update trait information
        elif 'is_trait' in boundary and boundary.get('is_trait'):
            if 'class_name' in boundary and boundary['class_name']:
                trait_text = boundary.get('text', '')
                if trait_text not in context_tracker['traits']:
                    context_tracker['traits'].append(trait_text)
    
    def _get_preserved_context(self, context_tracker: Dict[str, Any]) -> str:
        """
        Get preserved context information for Smalltalk code.
        
        Args:
            context_tracker: Context tracker
            
        Returns:
            Smalltalk context string
        """
        if not context_tracker:
            return ""
            
        context_lines = []
        
        # Add dialect information
        context_lines.append(f'"Smalltalk dialect: {context_tracker["dialect"]}"')
        
        # Add class context if available
        if context_tracker['current_class'] and context_tracker['current_superclass']:
            context_lines.append(f'"{context_tracker["current_superclass"]} subclass: #{context_tracker["current_class"]}"')
        
        # Add category if available
        if context_tracker['current_category']:
            context_lines.append(f'"Category: {context_tracker["current_category"]}"')
            
        # Add protocol if available
        if context_tracker['current_protocol']:
            context_lines.append(f'"Protocol: {context_tracker["current_protocol"]}"')
            
        # Add side information (class vs instance)
        if context_tracker['is_class_side']:
            context_lines.append('"Class side"')
        else:
            context_lines.append('"Instance side"')
            
        # Add current method if available
        if context_tracker['current_method']:
            context_lines.append(f'"Current method: {context_tracker["current_method"]}"')
            
        # Add instance variables if available
        if context_tracker['instance_vars']:
            vars_str = ' '.join(context_tracker['instance_vars'])
            context_lines.append(f'"Instance variables: {vars_str}"')
            
        # Add class variables if available
        if context_tracker['class_vars']:
            vars_str = ' '.join(context_tracker['class_vars'])
            context_lines.append(f'"Class variables: {vars_str}"')
            
        # Add trait information if available
        if context_tracker['traits']:
            context_lines.append('"Traits:"')
            for trait in context_tracker['traits'][:3]:  # Limit to 3 traits for brevity
                context_lines.append(f'"{trait.strip()}"')
                
        # Add file chunk information if applicable
        if context_tracker['in_file_chunk'] and context_tracker['file_chunk_type']:
            context_lines.append(f'"In {context_tracker["file_chunk_type"]}"')
            
        return "\n".join(context_lines)
    
    def _create_new_chunk_with_context(
        self,
        previous_chunk: str,
        context_tracker: Optional[Dict[str, Any]],
        overlap_chars: int,
        boundary: Dict[str, Any],
        options: ChunkingOptions
    ) -> str:
        """
        Create a new chunk with Smalltalk-specific context and overlap.
        
        Args:
            previous_chunk: Previous chunk content
            context_tracker: Context tracker
            overlap_chars: Number of chars to overlap
            boundary: Current boundary information
            options: Chunking options
            
        Returns:
            New chunk text with context
        """
        # Get context information
        context = self._get_preserved_context(context_tracker) if context_tracker else ""
        
        # Create header with context
        if context:
            chunk = f"{context}\n\n"
        else:
            chunk = '"Continued from previous chunk"\n\n'
            
        # Add overlap if configured
        if overlap_chars > 0 and len(previous_chunk) > overlap_chars:
            # For Smalltalk, try to start at a clean statement
            overlap_content = previous_chunk[-overlap_chars:]
            
            # Look for statement endings (period followed by whitespace or newline)
            statement_match = re.search(r'\.\s+', overlap_content)
            if statement_match:
                # Start from after the statement end
                overlap_content = overlap_content[statement_match.end():]
            elif re.search(r'!\s*$', overlap_content):
                # File-in format chunk ending
                overlap_content = ""
            
            chunk += overlap_content
        
        return chunk
    
    def _add_metadata_comments(self, chunks: List[str], options: ChunkingOptions) -> List[str]:
        """
        Add Smalltalk-specific metadata comments to chunks.
        
        Args:
            chunks: List of chunks to add metadata to
            options: Chunking options
            
        Returns:
            Chunks with metadata comments
        """
        if not chunks:
            return chunks
        
        total_chunks = len(chunks)
        
        return [
            f'"SMALLTALK CHUNK {i+1}/{total_chunks} ({self.dialect})"\n{chunk}'
            for i, chunk in enumerate(chunks)
        ]
    
    @lru_cache(maxsize=16)
    def _detect_smalltalk_dialect(self, text: str) -> str:
        """
        Detect the Smalltalk dialect from code patterns.
        
        Args:
            text: Smalltalk code to analyze
            
        Returns:
            Dialect identifier string
        """
        # Check each dialect's patterns and count matches
        dialect_scores = {}
        
        for dialect, patterns in self._DIALECT_PATTERNS.items():
            matches = sum(1 for pattern in patterns if re.search(pattern, text))
            dialect_scores[dialect] = matches
        
        # Get dialect with highest score
        best_dialect, best_score = max(dialect_scores.items(), key=lambda x: x[1])
        
        # If score is 0 or very low, return "standard"
        if best_score < 2:
            return SmalltalkDialect.STANDARD
            
        return best_dialect
    
    @classmethod
    def _compile_class_patterns(cls) -> Dict[str, Dict[str, Pattern]]:
        """
        Compile regex patterns for class definitions.
        
        Returns:
            Dictionary of dialect-specific pattern dictionaries
        """
        patterns = {
            # Standard patterns
            SmalltalkDialect.STANDARD: {
                'subclass': r'^\s*([A-Z][a-zA-Z0-9_]*)(?:\s+class)?\s+(?:subclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?)(?:!|\.)?$',
                'variableSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)(?:\s+class)?\s+(?:variableSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?)(?:!|\.)?$',
            },
            
            # VisualWorks patterns
            SmalltalkDialect.VISUALWORKS: {
                'subclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+(?:subclass):\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+(?:poolDictionaries|imports):\s*[\'"](.*?)[\'"])?(?:\s+(?:category|asAbstract):\s*[\'"](.*?)[\'"])?',
                'variableSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+(?:variableSubclass):\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+(?:poolDictionaries|imports):\s*[\'"](.*?)[\'"])?(?:\s+(?:category|asAbstract):\s*[\'"](.*?)[\'"])?',
                'variableByteSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+(?:variableByteSubclass):\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+(?:poolDictionaries|imports):\s*[\'"](.*?)[\'"])?(?:\s+(?:category|asAbstract):\s*[\'"](.*?)[\'"])?',
                'classDefinition': r'!classDefinition:\s+#([A-Z][a-zA-Z0-9_]*)\s+category:\s*[\'"](.*?)[\'"]\s+superclass:\s+#([A-Z][a-zA-Z0-9_]*)',
            },
            
            # Pharo patterns
            SmalltalkDialect.PHARO: {
                'subclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+subclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+uses:\s+[\'"](.*?)[\'"])?(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+package:\s*[\'"](.*?)[\'"])?',
                'variableSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+package:\s*[\'"](.*?)[\'"])?',
                'variableByteSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableByteSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+package:\s*[\'"](.*?)[\'"])?',
                'variableWordSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableWordSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+package:\s*[\'"](.*?)[\'"])?',
                'trait': r'^\s*Trait\s+named:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+uses:\s+[\'"](.*?)[\'"])?(?:\s+package:\s*[\'"](.*?)[\'"])?',
            },
            
            # Squeak patterns
            SmalltalkDialect.SQUEAK: {
                'subclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+subclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
                'variableSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
                'variableByteSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableByteSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
                'variableWordSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableWordSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
            },
            
            # GemStone patterns
            SmalltalkDialect.GEMSTONE: {
                'subclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+(?:subclass):\s+\'([A-Z][a-zA-Z0-9_]*)\'(?:\s+instVarNames:\s+#\((.*?)\))?(?:\s+classVars:\s+#\((.*?)\))?(?:\s+classInstVars:\s+#\((.*?)\))?(?:\s+poolDictionaries:\s+#\((.*?)\))?(?:\s+inDictionary:\s+[\'"](.*?)[\'"])?',
                'variableSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+(?:variableSubclass):\s+\'([A-Z][a-zA-Z0-9_]*)\'(?:\s+instVarNames:\s+#\((.*?)\))?(?:\s+classVars:\s+#\((.*?)\))?(?:\s+classInstVars:\s+#\((.*?)\))?(?:\s+poolDictionaries:\s+#\((.*?)\))?(?:\s+inDictionary:\s+[\'"](.*?)[\'"])?',
                'indexableSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+(?:indexableSubclass):\s+\'([A-Z][a-zA-Z0-9_]*)\'(?:\s+instVarNames:\s+#\((.*?)\))?(?:\s+classVars:\s+#\((.*?)\))?(?:\s+classInstVars:\s+#\((.*?)\))?(?:\s+poolDictionaries:\s+#\((.*?)\))?(?:\s+inDictionary:\s+[\'"](.*?)[\'"])?',
            },
            
            # Dolphin patterns
            SmalltalkDialect.DOLPHIN: {
                'subclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+subclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?',
                'variableSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?',
                'externalClass': r'ExternalClass\s+subclass:\s*#([A-Z][a-zA-Z0-9_]*)\s+instanceVariableNames:\s*[\'"](.*?)[\'"]\s+classVariableNames:\s*[\'"](.*?)[\'"]\s+poolDictionaries:\s*[\'"](.*?)[\'"]\s+superclass:\s*[\'"]([A-Z][a-zA-Z0-9_]*)[\'"]',
            },
            
            # GNU patterns
            SmalltalkDialect.GNU: {
                'subclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+subclass:\s+([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
                'variableSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableSubclass:\s+([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
                'variableByteSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableByteSubclass:\s+([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
                'variableWordSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableWordSubclass:\s+([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
            },
            
            # Amber patterns
            SmalltalkDialect.AMBER: {
                'subclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+subclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+package:\s*[\'"](.*?)[\'"])?',
                'defineClass': r'smalltalk\s+defineClass:\s+[\'"](.*?)[\'"](?:\s+superclass:\s+[\'"](.*?)[\'"])?(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+package:\s*[\'"](.*?)[\'"])?',
            },
            
            # Cuis patterns
            SmalltalkDialect.CUIS: {
                'subclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+subclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
                'variableSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
                'variableByteSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableByteSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
                'variableWordSubclass': r'^\s*([A-Z][a-zA-Z0-9_]*)\s+variableWordSubclass:\s+#([A-Z][a-zA-Z0-9_]*)(?:\s+instanceVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+classVariableNames:\s*[\'"](.*?)[\'"])?(?:\s+poolDictionaries:\s*[\'"](.*?)[\'"])?(?:\s+category:\s*[\'"](.*?)[\'"])?',
            },
        }
        
        # Compile all patterns
        for dialect, dialect_patterns in patterns.items():
            for pattern_name, pattern in list(dialect_patterns.items()):
                dialect_patterns[pattern_name] = re.compile(pattern, re.MULTILINE)
                
        return patterns
    
    @classmethod
    def _compile_method_patterns(cls) -> Dict[str, Dict[str, Pattern]]:
        """
        Compile regex patterns for method definitions.
        
        Returns:
            Dictionary of dialect-specific pattern dictionaries
        """
        patterns = {
            # Standard patterns that work across dialects
            SmalltalkDialect.STANDARD: {
                # Unary method (no arguments)
                'unary_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*)\s*$', re.MULTILINE),
                
                # Keyword method (with colons)
                'keyword_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)+)', re.MULTILINE),
                
                # Binary method (symbols)
                'binary_method': re.compile(r'^\s*([-+*/~<>=@,%|&?!]+)\s+([a-zA-Z][a-zA-Z0-9_]*)', re.MULTILINE),
                
                # Class method detection pattern - in standard smalltalk, usually from file structure
                'class_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)\s*(?:$|["|\[])', re.MULTILINE),
            },
            
            # VisualWorks specific patterns
            SmalltalkDialect.VISUALWORKS: {
                # Methods in chunk format
                'chunk_method': re.compile(r'^(!.*?methodsFor:.*?!)\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)', re.MULTILINE),
                
                # Class-side methods in chunk format
                'chunk_class_method': re.compile(r'^(!.*?class\s+methodsFor:.*?!)\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)', re.MULTILINE),
                
                # Standard method patterns
                'unary_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*)\s*$', re.MULTILINE),
                'keyword_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)+)', re.MULTILINE),
                'binary_method': re.compile(r'^\s*([-+*/~<>=@,%|&?!]+)\s+([a-zA-Z][a-zA-Z0-9_]*)', re.MULTILINE),
            },
            
            # Pharo specific patterns
            SmalltalkDialect.PHARO: {
                # Methods with pragmas
                'pragma_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)\s*<\w+', re.MULTILINE),
                
                # Standard method patterns
                'unary_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*)\s*$', re.MULTILINE),
                'keyword_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)+)', re.MULTILINE),
                'binary_method': re.compile(r'^\s*([-+*/~<>=@,%|&?!]+)\s+([a-zA-Z][a-zA-Z0-9_]*)', re.MULTILINE),
            },
            
            # Squeak specific patterns
            SmalltalkDialect.SQUEAK: {
                # Methods with formatting hints
                'formatted_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)\s*"(?:stamp:|primitive:)', re.MULTILINE),
                
                # Standard method patterns
                'unary_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*)\s*$', re.MULTILINE),
                'keyword_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)+)', re.MULTILINE),
                'binary_method': re.compile(r'^\s*([-+*/~<>=@,%|&?!]+)\s+([a-zA-Z][a-zA-Z0-9_]*)', re.MULTILINE),
            },
            
            # GemStone specific patterns
            SmalltalkDialect.GEMSTONE: {
                # Methods with GemStone environment info
                'gemstone_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)\s*\%', re.MULTILINE),
                
                # Standard method patterns
                'unary_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*)\s*$', re.MULTILINE),
                'keyword_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)+)', re.MULTILINE),
                'binary_method': re.compile(r'^\s*([-+*/~<>=@,%|&?!]+)\s+([a-zA-Z][a-zA-Z0-9_]*)', re.MULTILINE),
            },
            
            # Dolphin specific patterns
            SmalltalkDialect.DOLPHIN: {
                # Method with Dolphin's metadata
                'dolphin_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)\s*\[\s*<\w+', re.MULTILINE),
                
                # Standard method patterns
                'unary_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*)\s*$', re.MULTILINE),
                'keyword_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)+)', re.MULTILINE),
                'binary_method': re.compile(r'^\s*([-+*/~<>=@,%|&?!]+)\s+([a-zA-Z][a-zA-Z0-9_]*)', re.MULTILINE),
            },
            
            # GNU Smalltalk specific patterns
            SmalltalkDialect.GNU: {
                # GNU's primitive method format
                'primitive_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)\s*<primitive:', re.MULTILINE),
                
                # Standard method patterns
                'unary_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*)\s*$', re.MULTILINE),
                'keyword_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)+)', re.MULTILINE),
                'binary_method': re.compile(r'^\s*([-+*/~<>=@,%|&?!]+)\s+([a-zA-Z][a-zA-Z0-9_]*)', re.MULTILINE),
            },
            
            # Amber (JavaScript) Smalltalk specific patterns
            SmalltalkDialect.AMBER: {
                # Amber method with JS syntax
                'amber_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)\s*\{\s*return\s+', re.MULTILINE),
                
                # Standard method patterns
                'unary_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*)\s*$', re.MULTILINE),
                'keyword_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)+)', re.MULTILINE),
                'binary_method': re.compile(r'^\s*([-+*/~<>=@,%|&?!]+)\s+([a-zA-Z][a-zA-Z0-9_]*)', re.MULTILINE),
            },
            
            # Cuis specific patterns
            SmalltalkDialect.CUIS: {
                # Cuis file-in format
                'cuis_method': re.compile(r'^\s*!([a-zA-Z][a-zA-Z0-9_]*)\s+methodsFor:\s*[\'"].*?[\'"]\s*!\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)*)', re.MULTILINE),
                
                # Standard method patterns
                'unary_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*)\s*$', re.MULTILINE),
                'keyword_method': re.compile(r'^\s*([a-zA-Z][a-zA-Z0-9_]*(?::\s*[a-zA-Z][a-zA-Z0-9_]*\s*)+)', re.MULTILINE),
                'binary_method': re.compile(r'^\s*([-+*/~<>=@,%|&?!]+)\s+([a-zA-Z][a-zA-Z0-9_]*)', re.MULTILINE),
            },
        }
        
        return patterns
    
    @lru_cache(maxsize=1)
    def _get_chunking_strategy(self) -> ChunkingStrategy:
        """
        Get the chunking strategy this implementation represents.
        
        Returns:
            ChunkingStrategy enum value
        """
        return ChunkingStrategy.STRUCTURAL
</file>

<file path="strategies/semantic.py">
"""
Semantic chunking strategy implementation.

This module provides an enterprise-grade implementation of the semantic chunking strategy,
which intelligently divides text based on natural language and structural boundaries
to preserve meaning and context across chunks.

Usage:
    from enterprise_chunker.strategies.semantic import SemanticChunkingStrategy
    
    strategy = SemanticChunkingStrategy()
    result = strategy.chunk(text, options)
"""

import re
import math
import time
import logging
import heapq
from functools import lru_cache
from typing import List, Dict, Any, Optional, Tuple, Iterator, Set, ClassVar, cast
from datetime import datetime

from enterprise_chunker.strategies.base import BaseChunkingStrategy
from enterprise_chunker.models.enums import ContentFormat, ChunkingStrategy
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.patterns.regex_patterns import RegexPatterns
from enterprise_chunker.exceptions import BoundaryDetectionError
from enterprise_chunker.utils.performance import timing_decorator

# Configure logging
logger = logging.getLogger(__name__)

# Constants
MAX_BOUNDARIES = 20000  # Hard limit on number of boundaries to prevent quadratic processing
BOUNDARY_SAMPLE_THRESHOLD = 100000  # Text size threshold for boundary sampling
BOUNDARY_SAMPLING_RATE = 0.25  # Sampling rate for large texts


class SemanticChunkingStrategy(BaseChunkingStrategy):
    """
    Semantic chunking strategy that preserves natural language boundaries.
    
    This strategy identifies meaningful sections, paragraphs, and linguistic structures
    in text to create chunks that maintain semantic coherence and context. The algorithm
    prioritizes preserving natural boundaries like headings, paragraphs, and complete sentences.
    
    Features:
        - Intelligent section and subsection detection
        - Context preservation between chunks
        - Format-aware boundary detection
        - Performance optimizations for large documents
        - Configurable boundary prioritization
    """
    
    # Boundary priority tiers for ranking
    BOUNDARY_PRIORITY: ClassVar[Dict[str, int]] = {
        "heading1": 100,
        "heading2": 90,
        "heading3": 80,
        "section_marker": 70,
        "paragraph": 60,
        "list_item": 50,
        "code_block": 40,
        "sentence": 30,
        "log_entry": 20,
        "fallback": 10
    }
    
    def __init__(self, format_type: ContentFormat = ContentFormat.TEXT):
        """
        Initialize the semantic chunking strategy.
        
        Args:
            format_type: Content format this strategy handles (defaults to TEXT)
        """
        super().__init__(format_type)
        self.boundary_stats: Dict[str, int] = {}
        self.section_hierarchy: List[Dict[str, Any]] = []
    
    @timing_decorator
    def detect_boundaries(self, text: str, options: ChunkingOptions) -> List[Dict[str, Any]]:
        """
        Detect semantic boundaries in text based on content structure and natural language patterns.
        
        This method analyzes text to identify meaningful boundaries such as section headings,
        paragraphs, and sentence breaks, optimized to handle enterprise-scale content.
        
        Args:
            text: Text to analyze
            options: Chunking options
            
        Returns:
            List of boundary dictionaries with position and metadata
            
        Raises:
            BoundaryDetectionError: If boundary detection fails
        """
        logger.info(f"[{self.operation_id}] Starting semantic boundary detection for {len(text):,} chars")
        start_time = time.time()
        
        try:
            # Reset statistics
            self.boundary_stats = {}
            
            # Get appropriate pattern sets based on format
            section_patterns = self._get_section_patterns()
            
            # Initialize boundary collector
            all_boundaries = []
            
            # Determine if we need sampling for very large texts
            use_sampling = len(text) > BOUNDARY_SAMPLE_THRESHOLD
            if use_sampling:
                logger.info(
                    f"[{self.operation_id}] Large text detected ({len(text):,} chars), "
                    f"using boundary sampling at {BOUNDARY_SAMPLING_RATE:.2f} rate"
                )
            
            # Apply all boundary patterns to find potential split points
            for pattern_idx, pattern in enumerate(section_patterns):
                pattern_type = self._get_pattern_type(pattern_idx)
                priority = self._get_boundary_priority(pattern_type)
                
                try:
                    # Count matches for this pattern
                    matches_count = 0
                    
                    # Use iterator to avoid loading all matches at once
                    for match in pattern.finditer(text):
                        matches_count += 1
                        
                        # Apply sampling for very large texts if needed
                        if use_sampling and pattern_type not in ("heading1", "heading2", "heading3"):
                            if matches_count % int(1/BOUNDARY_SAMPLING_RATE) != 0:
                                continue
                        
                        # Extract boundary information
                        boundary = {
                            'index': match.start(),
                            'end': match.end(),
                            'text': match.group(0),
                            'is_section_break': priority >= self.BOUNDARY_PRIORITY["paragraph"],
                            'pattern_type': pattern_type,
                            'priority': priority
                        }
                        
                        all_boundaries.append(boundary)
                        
                        # Track boundary type statistics
                        self.boundary_stats[pattern_type] = self.boundary_stats.get(pattern_type, 0) + 1
                        
                        # Hard limit on number of boundaries to prevent quadratic processing
                        if len(all_boundaries) >= MAX_BOUNDARIES:
                            logger.warning(
                                f"[{self.operation_id}] Reached maximum boundary limit ({MAX_BOUNDARIES}), "
                                f"truncating additional boundaries"
                            )
                            break
                    
                except Exception as e:
                    logger.warning(
                        f"[{self.operation_id}] Error applying pattern {pattern_type}: {str(e)}", 
                        exc_info=True
                    )
                
                # If we've hit the boundary limit, stop processing patterns
                if len(all_boundaries) >= MAX_BOUNDARIES:
                    break
            
            # Check if we need fallback strategies
            self._apply_fallback_strategies(text, options, all_boundaries)
            
            # Ensure we haven't exceeded the maximum boundary count
            if len(all_boundaries) > MAX_BOUNDARIES:
                logger.warning(
                    f"[{self.operation_id}] Too many boundaries ({len(all_boundaries)}), "
                    f"selecting top {MAX_BOUNDARIES} by priority"
                )
                # Keep only the highest priority boundaries based on their priority score
                all_boundaries = self._select_best_boundaries(all_boundaries, MAX_BOUNDARIES)
            
            # Sort boundaries by position
            all_boundaries.sort(key=lambda x: x['index'])
            
            # Add a boundary at the beginning if none exists
            if not all_boundaries or all_boundaries[0]['index'] > 0:
                all_boundaries.insert(0, {
                    'index': 0,
                    'end': 0,
                    'text': '',
                    'is_section_break': False,
                    'pattern_type': 'document_start',
                    'priority': 0
                })
            
            # Log boundary statistics
            detection_time = time.time() - start_time
            logger.info(
                f"[{self.operation_id}] Detected {len(all_boundaries):,} boundaries "
                f"in {detection_time:.3f}s ({len(text) / detection_time:.1f} chars/s)"
            )
            self._log_boundary_stats()
            
            return all_boundaries
            
        except Exception as e:
            logger.error(f"[{self.operation_id}] Boundary detection failed: {str(e)}", exc_info=True)
            raise BoundaryDetectionError(f"Failed to detect semantic boundaries: {str(e)}") from e
    
    def _select_best_boundaries(self, boundaries: List[Dict[str, Any]], max_count: int) -> List[Dict[str, Any]]:
        """
        Select the best boundaries when there are too many.
        
        This method uses a combination of priority scoring and position distribution
        to select the most important boundaries while maintaining coverage.
        
        Args:
            boundaries: List of all detected boundaries
            max_count: Maximum number of boundaries to keep
            
        Returns:
            Filtered list of boundaries
        """
        if len(boundaries) <= max_count:
            return boundaries
            
        # First, always include section-level boundaries (headings, etc.)
        high_priority_boundaries = [
            b for b in boundaries 
            if b.get('priority', 0) >= self.BOUNDARY_PRIORITY["paragraph"]
        ]
        
        # If we still have space, select evenly distributed boundaries from the remainder
        if len(high_priority_boundaries) < max_count:
            remaining_boundaries = [
                b for b in boundaries 
                if b.get('priority', 0) < self.BOUNDARY_PRIORITY["paragraph"]
            ]
            
            # Sort remaining by priority
            remaining_boundaries.sort(key=lambda x: x.get('priority', 0), reverse=True)
            
            # Select top boundaries up to a percentage of remaining slots
            top_remaining_count = min(
                len(remaining_boundaries),
                int((max_count - len(high_priority_boundaries)) * 0.7)
            )
            selected_remaining = remaining_boundaries[:top_remaining_count]
            
            # For the rest, sample evenly across the document
            if top_remaining_count < len(remaining_boundaries):
                remaining_slots = max_count - len(high_priority_boundaries) - top_remaining_count
                if remaining_slots > 0 and len(remaining_boundaries) > top_remaining_count:
                    step = (len(remaining_boundaries) - top_remaining_count) // remaining_slots
                    if step > 0:
                        for i in range(top_remaining_count, len(remaining_boundaries), step):
                            if len(selected_remaining) < (max_count - len(high_priority_boundaries)):
                                selected_remaining.append(remaining_boundaries[i])
            
            # Combine high priority with selected remaining
            result = high_priority_boundaries + selected_remaining
            
            # Sort by position
            result.sort(key=lambda x: x['index'])
            return result
        else:
            # If we have too many high-priority boundaries, select based on priority and position
            high_priority_boundaries.sort(
                key=lambda x: (x.get('priority', 0), -x['index']), 
                reverse=True
            )
            return high_priority_boundaries[:max_count]
    
    def _apply_fallback_strategies(
        self, 
        text: str, 
        options: ChunkingOptions, 
        boundaries: List[Dict[str, Any]]
    ) -> None:
        """
        Apply fallback boundary detection strategies when primary strategies yield few results.
        
        Args:
            text: Source text
            options: Chunking options
            boundaries: List of boundaries to append to
        """
        # If few boundaries found, try paragraph detection
        if len(boundaries) < 3 and len(text) > 1000:
            logger.debug(f"[{self.operation_id}] Few boundaries detected, applying paragraph fallback")
            
            paragraph_pattern = re.compile(r'\n\s*\n')
            matches_count = 0
            
            for match in paragraph_pattern.finditer(text):
                matches_count += 1
                
                # Apply sampling for large texts
                if len(text) > BOUNDARY_SAMPLE_THRESHOLD and matches_count % 4 != 0:
                    continue
                    
                boundaries.append({
                    'index': match.end(),  # Use end so we start with the new paragraph
                    'end': match.end(),
                    'text': match.group(0),
                    'is_section_break': True,
                    'pattern_type': 'paragraph',
                    'priority': self.BOUNDARY_PRIORITY["paragraph"]
                })
                
                # Track statistics
                self.boundary_stats['paragraph'] = self.boundary_stats.get('paragraph', 0) + 1
                
                if len(boundaries) >= MAX_BOUNDARIES:
                    break
        
        # If still few boundaries, add sentence boundaries if configured
        if len(boundaries) < 5 and options.respect_sentences and len(text) > 500:
            logger.debug(f"[{self.operation_id}] Few boundaries detected, applying sentence fallback")
            
            sentence_pattern = RegexPatterns.get_sentence_boundaries()
            matches_count = 0
            
            for match in sentence_pattern.finditer(text):
                matches_count += 1
                
                # Apply higher sampling rate for sentences
                if len(text) > BOUNDARY_SAMPLE_THRESHOLD and matches_count % 10 != 0:
                    continue
                    
                boundaries.append({
                    'index': match.end(),
                    'end': match.end(),
                    'text': match.group(0),
                    'is_section_break': False,
                    'pattern_type': 'sentence',
                    'priority': self.BOUNDARY_PRIORITY["sentence"]
                })
                
                # Track statistics
                self.boundary_stats['sentence'] = self.boundary_stats.get('sentence', 0) + 1
                
                if len(boundaries) >= MAX_BOUNDARIES:
                    break
    
    def _log_boundary_stats(self) -> None:
        """
        Log statistics about boundary types detected during chunking.
        """
        if not self.boundary_stats:
            return
            
        # Sort by count
        sorted_stats = sorted(
            self.boundary_stats.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        stats_message = f"[{self.operation_id}] Boundary types: " + ", ".join(
            f"{btype}={count}" for btype, count in sorted_stats
        )
        
        logger.debug(stats_message)
    
    def _create_context_tracker(self) -> Optional[Dict[str, Any]]:
        """
        Create a context tracker for semantic chunking.
        
        The context tracker maintains hierarchical section information
        to preserve context between chunks.
        
        Returns:
            Context tracker dictionary
        """
        return {
            'sections': [],
            'section_levels': {},
            'last_boundary_type': None,
            'current_section': None,
            'section_counters': {}
        }
    
    def _update_context_tracker(self, context_tracker: Optional[Dict[str, Any]], boundary: Dict[str, Any]) -> None:
        """
        Update context tracker with information from current boundary.
        
        This method maintains a hierarchical record of document sections
        to provide accurate context in subsequent chunks.
        
        Args:
            context_tracker: Context tracker to update
            boundary: Current boundary information
        """
        if not context_tracker:
            return
            
        # Extract boundary type
        boundary_type = boundary.get('pattern_type', '')
        
        # Update last boundary type
        context_tracker['last_boundary_type'] = boundary_type
        
        # Check if this is a section break
        if boundary.get('is_section_break', False):
            # Extract any section title from the text
            section_title = self._extract_section_title(boundary.get('text', ''), boundary_type)
            if section_title:
                # Determine section level
                level = self._get_section_level(boundary_type)
                
                # Track in counters
                counter_key = f"level_{level}"
                context_tracker['section_counters'][counter_key] = context_tracker['section_counters'].get(counter_key, 0) + 1
                
                # Create section entry with level information
                section_entry = {
                    'title': section_title,
                    'level': level,
                    'type': boundary_type,
                    'number': context_tracker['section_counters'].get(counter_key, 1)
                }
                
                # Add to sections list
                context_tracker['sections'].append(section_entry)
                
                # Update current section
                context_tracker['current_section'] = section_entry
                
                # Update section levels mapping
                context_tracker['section_levels'][level] = section_title
                
                # Clear lower levels when a higher-level section is encountered
                if level < 3:  # if this is a high-level section
                    levels_to_clear = [l for l in context_tracker['section_levels'] if l > level]
                    for l in levels_to_clear:
                        context_tracker['section_levels'].pop(l, None)
    
    def _get_section_level(self, boundary_type: str) -> int:
        """
        Determine section level based on boundary type.
        
        Args:
            boundary_type: Type of boundary
            
        Returns:
            Numeric section level (1=highest)
        """
        level_map = {
            'heading1': 1,
            'heading2': 2,
            'heading3': 3,
            'section_marker': 2,
            'paragraph': 4,
            'list_item': 5,
            'code_block': 4,
            'sentence': 6
        }
        return level_map.get(boundary_type, 5)
    
    def _get_preserved_context(self, context_tracker: Dict[str, Any]) -> str:
        """
        Get preserved context from context tracker.
        
        This method generates a hierarchical context summary based on
        the document's section structure.
        
        Args:
            context_tracker: Context tracker
            
        Returns:
            Preserved context string
        """
        # Return the section hierarchy for context
        if not context_tracker or not context_tracker.get('sections'):
            return ""
            
        # Get section levels dictionary
        section_levels = context_tracker.get('section_levels', {})
        if not section_levels:
            return ""
            
        # Build hierarchical context
        context_lines = ["Document context:"]
        
        # Add major sections first
        for level in sorted(section_levels.keys()):
            title = section_levels[level]
            indentation = "  " * (level - 1)
            context_lines.append(f"{indentation}- {title}")
            
        # Limit context size
        if len(context_lines) > 7:
            context_lines = context_lines[:2] + ["  ..."] + context_lines[-4:]
            
        return "\n".join(context_lines)
    
    def _create_new_chunk_with_context(
        self,
        previous_chunk: str,
        context_tracker: Optional[Dict[str, Any]],
        overlap_chars: int,
        boundary: Dict[str, Any],
        options: ChunkingOptions
    ) -> str:
        """
        Create a new chunk with semantic context and intelligent overlap.
        
        This method generates contextual information about the document structure
        and ensures proper content continuity between chunks.
        
        Args:
            previous_chunk: Previous chunk content
            context_tracker: Context tracker
            overlap_chars: Number of chars to overlap
            boundary: Current boundary information
            options: Chunking options
            
        Returns:
            New chunk text with context
        """
        # Initialize chunk with continuation marker
        if context_tracker and 'current_section' in context_tracker and context_tracker['current_section']:
            current_section = context_tracker['current_section']
            chunk = f"/* Continued in section: {current_section['title']} */\n"
        else:
            chunk = "/* Continued from previous section */\n"
        
        # Add hierarchical context if available
        if context_tracker:
            preserved_context = self._get_preserved_context(context_tracker)
            if preserved_context:
                chunk += f"{preserved_context}\n\n"
        
        # Add overlap if configured, with intelligent boundary detection
        if overlap_chars > 0 and len(previous_chunk) > overlap_chars:
            overlap_text = previous_chunk[-overlap_chars:]
            
            # Look for natural boundary in the overlap text
            paragraph_break = overlap_text.rfind("\n\n")
            if paragraph_break != -1 and paragraph_break > overlap_chars * 0.2:
                # Found good paragraph break
                chunk += overlap_text[paragraph_break+2:]
            else:
                # Look for sentence boundary
                sentence_break = re.search(r'(?<=[.!?])\s+(?=[A-Z])', overlap_text)
                if sentence_break and sentence_break.start() > overlap_chars * 0.3:
                    chunk += overlap_text[sentence_break.end():]
                else:
                    # Fall back to using the entire overlap
                    chunk += overlap_text
                    
        # Add appropriate separator based on boundary type
        boundary_type = boundary.get('pattern_type', '')
        if boundary_type in ('heading1', 'heading2', 'heading3'):
            chunk += "\n"  # Add extra space before headings
            
        return chunk
    
    def _extract_section_title(self, text: str, boundary_type: str) -> str:
        """
        Extract a section title from boundary text with type-specific handling.
        
        Args:
            text: Boundary text
            boundary_type: Type of boundary
            
        Returns:
            Extracted section title or empty string
        """
        # Skip empty text
        if not text or not text.strip():
            return ""
            
        # Handle based on boundary type
        if boundary_type.startswith('heading'):
            # If it's a heading, extract the heading text
            heading_match = re.match(r'^#+\s+(.+)', text.strip())
            if heading_match:
                return heading_match.group(1)
            
            # If it's a line followed by === or ---, it's a setext heading
            setext_match = re.match(r'^([^\n]+)\n[=\-]+', text.strip())
            if setext_match:
                return setext_match.group(1)
            
            # If it's an HTML heading
            html_match = re.match(r'<h[1-6]>(.*?)<\/h[1-6]>', text.strip())
            if html_match:
                return html_match.group(1)
                
        elif boundary_type == 'section_marker':
            # For section markers like [Section] in INI files
            ini_match = re.match(r'\[([^\]]+)\]', text.strip())
            if ini_match:
                return ini_match.group(1)
                
        elif boundary_type in ('log_entry', 'exception'):
            # For log entries, use timestamp or log level if present
            log_match = re.match(r'^\[?(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})', text.strip())
            if log_match:
                return f"Log entry {log_match.group(1)}"
                
            level_match = re.match(r'^(ERROR|WARN(?:ING)?|INFO|DEBUG|TRACE|FATAL|CRITICAL|NOTICE|SEVERE)', text.strip())
            if level_match:
                return f"{level_match.group(1)} message"
                
        # Default: extract first line with length limit
        lines = text.strip().split('\n')
        if lines and lines[0].strip():
            # Limit title length
            title = lines[0].strip()
            if len(title) > 60:
                title = title[:57] + "..."
            return title
        
        return ""
    
    def _get_pattern_type(self, pattern_index: int) -> str:
        """
        Map pattern index to semantic type name.
        
        Args:
            pattern_index: Index of the pattern in the pattern list
            
        Returns:
            String type name for the pattern
        """
        # Pattern index to type mapping for standard semantic patterns
        pattern_types = [
            # Headers (0-5)
            "heading1", "heading2", "heading3", "heading1", "heading2", "heading3", 
            # Log patterns (6-9)
            "log_entry", "log_entry", "log_entry", "log_entry",
            # Log levels (10-11)
            "log_entry", "log_entry",
            # Exceptions (12-14)
            "exception", "exception", "exception",
            # Process info (15-17)
            "log_entry", "log_entry", "log_entry",
            # Code structure (18-19)
            "code_block", "code_block",
            # Configuration (20-21)
            "section_marker", "section_marker",
            # Paragraphs (22)
            "paragraph",
            # Special content (23-24)
            "section_marker", "section_marker"
        ]
        
        if pattern_index < len(pattern_types):
            return pattern_types[pattern_index]
        else:
            return "fallback"
    
    def _get_boundary_priority(self, pattern_type: str) -> int:
        """
        Get priority score for a boundary type.
        
        Args:
            pattern_type: Type of pattern/boundary
            
        Returns:
            Numeric priority score (higher = more important)
        """
        return self.BOUNDARY_PRIORITY.get(pattern_type, self.BOUNDARY_PRIORITY["fallback"])
    
    @lru_cache(maxsize=1)
    def _get_section_patterns(self) -> List[re.Pattern]:
        """
        Get regex patterns for identifying section boundaries.
        
        Returns:
            List of compiled regex patterns
        """
        return [
            # Markdown/text headings (0-5)
            re.compile(r'^#\s+.+$', re.MULTILINE),  # h1
            re.compile(r'^##\s+.+$', re.MULTILINE),  # h2
            re.compile(r'^###\s+.+$', re.MULTILINE),  # h3
            re.compile(r'^#{4,6}\s+.+$', re.MULTILINE),  # h4-h6
            re.compile(r'^[^\n]+\n=+\s*$', re.MULTILINE),  # Setext h1
            re.compile(r'^[^\n]+\n-+\s*$', re.MULTILINE),  # Setext h2
            
            # Log patterns (6-9)
            re.compile(r'^\[\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?\]', re.MULTILINE),  # ISO timestamps with brackets
            re.compile(r'^\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?', re.MULTILINE),  # ISO timestamps without brackets
            re.compile(r'^\d{1,2}\/\d{1,2}\/\d{2,4}\s+\d{1,2}:\d{2}(?::\d{2})?(?:\s+[AP]M)?', re.MULTILINE),  # US date format
            re.compile(r'^[A-Z][a-z]{2}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}', re.MULTILINE),  # Unix log format (Jan 15 14:23:01)
            
            # Log levels (10-11)
            re.compile(r'^(?:ERROR|WARN(?:ING)?|INFO|DEBUG|TRACE|FATAL|CRITICAL|NOTICE|SEVERE)\b', re.MULTILINE | re.IGNORECASE),
            re.compile(r'^(?:E|W|I|D|T|F|C|N|S)\/[\w.]+\(\s*\d+\):', re.MULTILINE),  # Android log format
            
            # Exception and stacktrace indicators (12-14)
            re.compile(r'^(?:Exception|Error|Traceback|Caused by|at)\s+[\w.$]+(?:[:]\s|[:]\s+\w+|\s+[\w.(]+\()', re.MULTILINE),
            re.compile(r'^\s+at\s+[\w.$]+(?:\.[\w.$]+)+\([^)]*\)$', re.MULTILINE),  # Java/JS stack trace
            re.compile(r'^\s+File ".*", line \d+', re.MULTILINE),  # Python stack trace
            
            # Process and thread indicators (15-17)
            re.compile(r'^Process ID:?\s+\d+', re.MULTILINE),
            re.compile(r'^Thread(?: ID)?:?\s+\d+', re.MULTILINE),
            re.compile(r'^pid=\d+\s+tid=\d+', re.MULTILINE),
            
            # Code structure (18-19)
            re.compile(r'^(?:function|class|def|public|private|protected|internal|static|async|export|import)\s+[\w<>]+', re.MULTILINE),
            re.compile(r'^(?:\s*if|\s*for|\s*while|\s*switch|\s*try|\s*catch|\s*finally)\s*\(', re.MULTILINE),
            
            # Configuration sections (20-21)
            re.compile(r'^\s*\[[^\]]+\]\s*$', re.MULTILINE),  # INI section header [section]
            re.compile(r'^---\s*$', re.MULTILINE),  # YAML document separator
            
            # Natural language paragraph boundaries (22)
            re.compile(r'\n\s*\n', re.MULTILINE),  # Empty line (paragraph break)
            
            # Special structured content (23-24)
            re.compile(r'^-{3,}BEGIN [A-Z]+-{3,}$', re.MULTILINE),  # BEGIN CERTIFICATE or similar
            re.compile(r'^-{3,}END [A-Z]+-{3,}$', re.MULTILINE)  # END CERTIFICATE or similar
        ]
    
    @lru_cache(maxsize=1)
    def _get_chunking_strategy(self) -> ChunkingStrategy:
        """
        Get the chunking strategy this implementation represents.
        
        Returns:
            ChunkingStrategy enum value
        """
        return ChunkingStrategy.SEMANTIC
    
    def get_strategy_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about the semantic chunking process.
        
        Returns:
            Dictionary with statistics about boundary detection and sectioning
        """
        return {
            "boundary_stats": self.boundary_stats,
            "processing_times": self.stats,
            "operation_id": self.operation_id
        }
</file>

<file path="tests/smalltalk-tests.txt">
"""
Unit tests for Smalltalk chunking strategy
"""

import unittest
import re
from typing import List, Dict, Any

from enterprise_chunker import EnterpriseChunker, ChunkingStrategy
from enterprise_chunker.strategies.formats.smalltalk_chunker import SmalltalkChunkingStrategy, SmalltalkDialect


class TestSmalltalkChunking(unittest.TestCase):
    """Test cases for Smalltalk chunking strategy"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.chunker = EnterpriseChunker()
        self.smalltalk_strategy = SmalltalkChunkingStrategy()
    
    def test_dialect_detection(self):
        """Test detection of different Smalltalk dialects"""
        # Test VisualWorks detection
        visualworks_code = """
        !classDefinition: #MyClass category: 'MyApp-Core' superclass: #Object!
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'MyApp-Core'!
            
        !MyClass methodsFor: 'accessing'!
        var1
            ^ var1!
            
        var1: aValue
            var1 := aValue!
            
        !MyClass methodsFor: 'initialization'!
        initialize
            "Initialize the receiver"
            super initialize.
            var1 := nil.
            var2 := 'test'!
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(visualworks_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.VISUALWORKS)
        
        # Test Pharo detection
        pharo_code = """
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            package: 'MyApp-Core'
            
        MyClass >> var1
            <getter>
            ^ var1
            
        MyClass >> var1: aValue
            <setter>
            var1 := aValue
            
        MyClass class >> initialize
            "Initialize the class"
            Smalltalk globals at: #MyGlobal put: self new.
            SystemAnnouncer uniqueInstance announce: ClassInitialized
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(pharo_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.PHARO)
        
        # Test Squeak detection
        squeak_code = """
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'MyApp-Core'
            
        !MyClass methodsFor: 'accessing' stamp: 'jdoe 5/1/2023 10:00'!
        var1
            "Return the value of var1"
            ^ var1
        !
            
        !MyClass methodsFor: 'accessing' stamp: 'jdoe 5/1/2023 10:01'!
        var1: aValue
            "Set the value of var1"
            var1 := aValue.
            self changed: #var1
        !
            
        Morph subclass: #MyMorph
            instanceVariableNames: 'position extent'
            classVariableNames: ''
            poolDictionaries: ''
            category: 'MyApp-UI'
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(squeak_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.SQUEAK)
        
        # Test GemStone detection
        gemstone_code = """
        Object subclass: 'MyClass'
            instVarNames: #(var1 var2)
            classVars: #(ClassVar1)
            classInstVars: #()
            poolDictionaries: #()
            inDictionary: UserGlobals
            constraints: #()
            classConstraints: #()
            
        category: 'accessing'
        method: MyClass
        var1
            ^ var1
        %
            
        category: 'accessing'
        method: MyClass
        var1: aValue
            var1 := aValue
        %
            
        commitTransaction
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(gemstone_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.GEMSTONE)
        
        # Test Dolphin detection
        dolphin_code = """
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            
        !MyClass methodsFor: 'accessing'!
        var1
            "Return the value of var1"
            ^var1!
            
        var1: aValue
            "Set the value of var1"
            var1 := aValue!
            
        !MyClass class methodsFor: 'class initialization'!
        initialize
            "Initialize the class"
            self register!
            
        package paxVersion: 1;
            basicComment: 'Dolphin Smalltalk package'.
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(dolphin_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.DOLPHIN)
    
    def test_class_definition_detection(self):
        """Test detection of class definitions across dialects"""
        # Standard class definition
        std_class_def = """
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'MyApp-Core'
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.STANDARD
        self.smalltalk_strategy._detect_class_definitions(std_class_def, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'class_definition')
        self.assertEqual(boundaries[0]['superclass'], 'Object')
        self.assertEqual(boundaries[0]['subclass'], 'MyClass')
        
        # Pharo class definition with traits
        pharo_class_def = """
        Object subclass: #MyClass
            uses: MyTrait
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            package: 'MyApp-Core'
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.PHARO
        self.smalltalk_strategy._detect_class_definitions(pharo_class_def, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'class_definition')
        self.assertEqual(boundaries[0]['superclass'], 'Object')
        self.assertEqual(boundaries[0]['subclass'], 'MyClass')
        
        # VisualWorks class definition
        vw_class_def = """
        !classDefinition: #MyClass category: 'MyApp-Core' superclass: #Object!
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'MyApp-Core'!
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.VISUALWORKS
        self.smalltalk_strategy._detect_class_definitions(vw_class_def, boundaries)
        
        # Should detect both the class definition statement and the class definition chunk
        self.assertGreaterEqual(len(boundaries), 1)
        self.assertTrue(any(b['type'] == 'class_definition' for b in boundaries))
        self.assertTrue(any(b['subclass'] == 'MyClass' for b in boundaries))
    
    def test_method_detection(self):
        """Test detection of method definitions in different formats"""
        # Test unary method
        unary_method = """
        initialize
            "Initialize the receiver"
            super initialize.
            var1 := nil.
            var2 := 'test'
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_method_definitions(unary_method, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'method_definition')
        self.assertEqual(boundaries[0]['method_name'], 'initialize')
        self.assertTrue(boundaries[0]['is_unary_method'])
        
        # Test keyword method
        keyword_method = """
        var1: aValue with: anotherValue
            "Set the values"
            var1 := aValue.
            var2 := anotherValue.
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_method_definitions(keyword_method, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'method_definition')
        self.assertEqual(boundaries[0]['method_name'], 'var1: aValue with: anotherValue')
        self.assertTrue(boundaries[0]['is_keyword_method'])
        
        # Test binary method
        binary_method = """
        + aNumber
            "Add aNumber to the receiver"
            ^ self value + aNumber
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_method_definitions(binary_method, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'method_definition')
        self.assertTrue(boundaries[0]['is_binary_method'])
    
    def test_block_detection(self):
        """Test detection of block structures"""
        # Test block with arguments
        block_code = """
        collect: aBlock
            "Evaluate aBlock with each of my elements as the argument."
            | newCollection |
            newCollection := self species new: self size.
            self do: [:each | newCollection add: (aBlock value: each)].
            ^ newCollection
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_block_structures(block_code, boundaries)
        
        # Should find at least the block with arguments [:each |
        self.assertGreaterEqual(len(boundaries), 1)
        self.assertTrue(any(b['type'] == 'block_with_args' for b in boundaries))
        
        # Test method return
        return_code = """
        value
            ^ self calculateValue
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_block_structures(return_code, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'method_return')
        
        # Test block with temp variables
        temp_vars_code = """
        calculate
            | temp1 temp2 |
            temp1 := 10.
            temp2 := 20.
            ^ temp1 + temp2
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_block_structures(temp_vars_code, boundaries)
        
        self.assertGreaterEqual(len(boundaries), 1)
        self.assertTrue(any(b['type'] == 'temp_vars' for b in boundaries))
    
    def test_pragma_detection(self):
        """Test detection of method pragmas"""
        # Pharo-style pragma
        pharo_pragma = """
        doSomething
            <primitive: 60>
            <important>
            self doSomethingElse
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.PHARO
        self.smalltalk_strategy._detect_pragmas(pharo_pragma, boundaries)
        
        self.assertEqual(len(boundaries), 2)
        self.assertTrue(all(b['is_pragma'] for b in boundaries))
        self.assertTrue(any(b['pragma_name'] == 'primitive' for b in boundaries))
        self.assertTrue(any(b['pragma_name'] == 'important' for b in boundaries))
        
        # Dolphin-style pragma
        dolphin_pragma = """
        doSomething
            [<primitive: 60>]
            self doSomethingElse
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.DOLPHIN
        self.smalltalk_strategy._detect_pragmas(dolphin_pragma, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertTrue(boundaries[0]['is_pragma'])
        self.assertEqual(boundaries[0]['pragma_name'], 'primitive')
    
    def test_trait_detection(self):
        """Test detection of trait compositions in Pharo"""
        pharo_trait = """
        Object subclass: #MyClass
            uses: TComparable
            instanceVariableNames: 'var1 var2'
            classVariableNames: ''
            package: 'MyApp-Core'
            
        Object subclass: #AnotherClass
            uses: TComparable + TEnumerable - {#collect:. #select:} @ {#at:->#fetch:}
            instanceVariableNames: 'var1 var2'
            classVariableNames: ''
            package: 'MyApp-Core'
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.PHARO
        self.smalltalk_strategy._detect_trait_compositions(pharo_trait, boundaries)
        
        self.assertGreaterEqual(len(boundaries), 1)
        self.assertTrue(any(b['type'] == 'trait_composition' for b in boundaries))
    
    def test_full_chunking(self):
        """Test full chunking process with a realistic Smalltalk file"""
        # Create a sample Smalltalk file with multiple classes and methods
        smalltalk_file = """
        "This is a sample Smalltalk file with multiple classes and methods"
        
        Object subclass: #Person
            instanceVariableNames: 'name age'
            classVariableNames: ''
            poolDictionaries: ''
            category: 'Examples'!
            
        !Person methodsFor: 'accessing'!
        name
            "Return the name"
            ^ name!
            
        name: aString
            "Set the name"
            name := aString!
            
        age
            "Return the age"
            ^ age!
            
        age: anInteger
            "Set the age"
            age := anInteger!
            
        !Person methodsFor: 'printing'!
        printOn: aStream
            "Print a representation of the receiver on aStream"
            super printOn: aStream.
            aStream 
                nextPutAll: ' named ';
                print: name;
                nextPutAll: ' aged ';
                print: age!
                
        !Person methodsFor: 'initialization'!
        initialize
            "Initialize a new instance of the receiver"
            super initialize.
            name := ''.
            age := 0!
            
        !Person class methodsFor: 'instance creation'!
        named: aString
            "Create a new instance with the given name"
            ^ self new
                name: aString;
                yourself!
                
        named: aString aged: anInteger
            "Create a new instance with the given name and age"
            ^ self new
                name: aString;
                age: anInteger;
                yourself!
        """
        
        # Perform chunking
        chunks = self.chunker.adaptive_chunk_text(
            smalltalk_file,
            max_tokens_per_chunk=100,  # Small to force chunking
            strategy=ChunkingStrategy.STRUCTURAL
        )
        
        # Check that we got multiple chunks
        self.assertGreater(len(chunks), 1)
        
        # Check that chunks contain context information
        context_markers = 0
        for chunk in chunks[1:]:  # Skip first chunk
            if '"Smalltalk dialect:' in chunk:
                context_markers += 1
                
        # At least some chunks should have context information
        self.assertGreater(context_markers, 0)
        
        # Check that no method is split in the middle
        incomplete_methods = 0
        for i, chunk in enumerate(chunks):
            # Look for method starts without ends
            method_starts = len(re.findall(r'^\s*\w+(?::\s*\w+\s*)*\s*, chunk, re.MULTILINE))
            method_ends = len(re.findall(r'\^\s*\w+.*?[!\n]', chunk, re.MULTILINE))
            
            if method_starts > method_ends and i < len(chunks) - 1:
                # Check if the next chunk has the ending
                if not re.search(r'^\s*\^', chunks[i+1], re.MULTILINE):
                    incomplete_methods += 1
        
        # There should be no incomplete methods
        self.assertEqual(incomplete_methods, 0, "Found methods split across chunks")
    
    def test_create_context_tracker(self):
        """Test creating and updating context tracker"""
        # Create context tracker
        context = self.smalltalk_strategy._create_context_tracker()
        
        # Check initial state
        self.assertIsNone(context['current_class'])
        self.assertIsNone(context['current_superclass'])
        self.assertEqual(context['instance_vars'], [])
        
        # Update with class definition
        class_boundary = {
            'type': 'class_definition',
            'subclass': 'TestClass',
            'superclass': 'Object',
            'instance_vars': ['var1', 'var2'],
            'class_vars': ['ClassVar'],
            'category': 'Testing'
        }
        
        self.smalltalk_strategy._update_context_tracker(context, class_boundary)
        
        # Check updated state
        self.assertEqual(context['current_class'], 'TestClass')
        self.assertEqual(context['current_superclass'], 'Object')
        self.assertEqual(context['instance_vars'], ['var1', 'var2'])
        self.assertEqual(context['current_category'], 'Testing')
        
        # Update with method definition
        method_boundary = {
            'type': 'method_definition',
            'method_name': 'testMethod',
            'is_class_method': True
        }
        
        self.smalltalk_strategy._update_context_tracker(context, method_boundary)
        
        # Check updated state
        self.assertEqual(context['current_method'], 'testMethod')
        self.assertTrue(context['is_class_side'])
        
        # Get preserved context
        preserved_context = self.smalltalk_strategy._get_preserved_context(context)
        
        # Verify content
        self.assertIn('TestClass', preserved_context)
        self.assertIn('Object', preserved_context)
        self.assertIn('var1', preserved_context)
        self.assertIn('Class side', preserved_context)
    
    def test_file_in_format_detection(self):
        """Test detection of file-in format"""
        # VisualWorks file-in format
        vw_file_in = """
        "This is a VisualWorks file-in"
        !classDefinition: #Person category: 'Examples' superclass: #Object!
        Object subclass: #Person
            instanceVariableNames: 'name age'
            classVariableNames: ''
            poolDictionaries: ''
            category: 'Examples'!
            
        !Person methodsFor: 'accessing'!
        name
            ^ name!
        """
        
        is_file_in = self.smalltalk_strategy._is_file_in_format(vw_file_in)
        self.assertTrue(is_file_in)
        
        # Non file-in format
        regular_code = """
        Object subclass: #Person
            instanceVariableNames: 'name age'
            classVariableNames: ''
            package: 'Examples'.
            
        Person >> name [
            ^ name
        ]
        """
        
        is_file_in = self.smalltalk_strategy._is_file_in_format(regular_code)
        self.assertFalse(is_file_in)
    
    def test_context_preservation(self):
        """Test context preservation between chunks"""
        # Create a sample Smalltalk file
        smalltalk_code = """
        Object subclass: #TestClass
            instanceVariableNames: 'var1 var2 var3'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'Test-Category'!
            
        !TestClass methodsFor: 'accessing'!
        var1
            "Return var1"
            ^ var1!
            
        var1: aValue
            "Set var1"
            var1 := aValue!
            
        !TestClass methodsFor: 'operations'!
        operation1
            "First operation"
            | temp |
            temp := self var1.
            temp doSomething.
            ^ temp!
            
        operation2
            "Second operation"
            | temp |
            temp := self var1.
            temp doSomethingElse.
            ^ temp!
        """
        
        # Force small chunk size to ensure multiple chunks
        chunks = self.chunker.adaptive_chunk_text(
            smalltalk_code,
            max_tokens_per_chunk=50,  # Very small to force multiple chunks
            strategy=ChunkingStrategy.STRUCTURAL
        )
        
        # Verify we got multiple chunks
        self.assertGreater(len(chunks), 2)
        
        # Count chunks with context preservation
        context_chunks = 0
        for chunk in chunks[1:]:  # Skip first chunk
            if 'Smalltalk dialect:' in chunk or 'Current method:' in chunk:
                context_chunks += 1
        
        # Most chunks after the first should have context
        self.assertGreater(context_chunks, len(chunks) * 0.5)
</file>

<file path="tests/test_chunker.py">
"""
Unit tests for the EnterpriseChunker
"""

import unittest
import json
from enterprise_chunker import (
    EnterpriseChunker,
    ChunkingStrategy,
    TokenEstimationStrategy,
    ContentFormat
)


class TestEnterpriseChunker(unittest.TestCase):
    """Test cases for the EnterpriseChunker"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.chunker = EnterpriseChunker()
    
    def test_empty_input(self):
        """Test chunking with empty input"""
        result = self.chunker.adaptive_chunk_text("")
        self.assertEqual(result, [])
    
    def test_small_input(self):
        """Test chunking with small input (no chunking needed)"""
        text = "This is a small text that doesn't need chunking."
        result = self.chunker.adaptive_chunk_text(text)
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0], text)
    
    def test_context_manager(self):
        """Test context manager for temporary configuration"""
        text = "This is a test of the context manager API. " * 20  # Make it long enough
        
        # Default configuration
        default_result = self.chunker.chunk(text)
        
        # Using context manager to override config temporarily
        with self.chunker.semantic_context(max_tokens=50, overlap=5):
            modified_result = self.chunker.chunk(text)
            
            # Should produce more chunks due to smaller max_tokens
            self.assertGreater(len(modified_result), len(default_result))
        
        # Configuration should be restored after context
        restored_result = self.chunker.chunk(text)
        self.assertEqual(len(restored_result), len(default_result))
    
    def test_stream_chunking(self):
        """Test streaming chunking"""
        from io import StringIO
        
        # Create a string stream
        text = "Line 1\nLine 2\nLine 3\n" * 50
        stream = StringIO(text)
        
        # Process stream
        chunks = list(self.chunker.chunk_stream(stream, max_tokens_per_chunk=100))
        
        # Should produce multiple chunks
        self.assertGreater(len(chunks), 1)
        
        # Check content is preserved
        combined = "".join(chunks)
        # The combined content might include overlaps, 
        # so we check that all original lines are present
        for line in ["Line 1", "Line 2", "Line 3"]:
            self.assertIn(line, combined)
    
    def test_markdown_chunking(self):
        """Test markdown-specific chunking"""
        markdown_text = """
        # Heading 1
        
        This is some content under heading 1.
        
        ## Subheading 1.1
        
        More content here.
        
        # Heading 2
        
        Content under heading 2.
        
        - List item 1
        - List item 2
        
        ## Subheading 2.1
        
        Final content section.
        """
        
        result = self.chunker.adaptive_chunk_text(
            markdown_text,
            max_tokens_per_chunk=70,  # Small to force chunking
            strategy=ChunkingStrategy.STRUCTURAL
        )
        
        # Should split at heading boundaries
        self.assertGreater(len(result), 1)
        
        # Headers should be preserved
        heading1_found = False
        heading2_found = False
        
        for chunk in result:
            if "# Heading 1" in chunk:
                heading1_found = True
            if "# Heading 2" in chunk:
                heading2_found = True
        
        self.assertTrue(heading1_found)
        self.assertTrue(heading2_found)
        
        # Context should be preserved across chunks
        context_found = False
        for chunk in result:
            if "Context from previous chunk" in chunk:
                context_found = True
                break
        
        self.assertTrue(context_found)
    
    def test_error_recovery(self):
        """Test error recovery with malformed content"""
        # Create malformed JSON that would cause parsing errors
        malformed_json = '{"key": "value", "broken": }'
        
        # Should not raise an exception
        try:
            result = self.chunker.adaptive_chunk_text(
                malformed_json,
                strategy=ChunkingStrategy.STRUCTURAL
            )
            # Should fall back to a working strategy
            self.assertGreater(len(result), 0)
        except Exception as e:
            self.fail(f"Error recovery failed, raised: {str(e)}")
    
    def test_token_estimation(self):
        """Test token estimation"""
        from enterprise_chunker.utils.token_estimation import estimate_tokens
        
        # Basic English text
        english_text = "This is a simple English text for testing token estimation."
        english_tokens = estimate_tokens(english_text)
        self.assertGreater(english_tokens, 0)
        
        # Text with emoji (should count higher per character)
        emoji_text = "Hello! 😊 How are you today? 🌟"
        emoji_tokens = estimate_tokens(emoji_text)
        self.assertGreater(emoji_tokens, 0)
        
        # Character-per-token ratio should be lower for emoji text
        english_ratio = len(english_text) / english_tokens
        emoji_ratio = len(emoji_text) / emoji_tokens
        self.assertLess(emoji_ratio, english_ratio)
        
        # Very small inputs
        self.assertEqual(estimate_tokens(""), 0)
        self.assertEqual(estimate_tokens("a"), 1)
    
    def test_format_detection(self):
        """Test format detection"""
        from enterprise_chunker.utils.format_detection import detect_content_format
        
        # JSON detection
        json_text = '{"key1": "value1", "key2": 42, "nested": {"inner": "value"}}'
        self.assertEqual(detect_content_format(json_text), ContentFormat.JSON)
        
        # XML detection
        xml_text = '<?xml version="1.0"?><root><item id="1">Value</item></root>'
        self.assertEqual(detect_content_format(xml_text), ContentFormat.XML)
        
        # Markdown detection
        markdown_text = "# Heading\n\nParagraph text\n\n- List item 1\n- List item 2"
        self.assertEqual(detect_content_format(markdown_text), ContentFormat.MARKDOWN)
        
        # Code detection
        code_text = "function test() {\n  const x = 10;\n  return x * 2;\n}"
        self.assertEqual(detect_content_format(code_text), ContentFormat.CODE)
        
        # Plain text 
        plain_text = "This is just regular text without any special format."
        self.assertEqual(detect_content_format(plain_text), ContentFormat.TEXT)
    
    def test_semantic_chunking(self):
        """Test semantic chunking with explicit sections"""
        text = """
        # Section 1
        
        This is the first section with some content.
        It has multiple lines of text.
        
        # Section 2
        
        This is the second section with more content.
        It also has multiple lines.
        
        # Section 3
        
        This is the third section.
        """
        
        result = self.chunker.adaptive_chunk_text(
            text, 
            max_tokens_per_chunk=50,  # Small to force chunking
            strategy=ChunkingStrategy.SEMANTIC
        )
        
        # Should split into at least 2 chunks
        self.assertGreater(len(result), 1)
        
        # Check that sections are preserved
        section1_found = any("Section 1" in chunk for chunk in result)
        section2_found = any("Section 2" in chunk for chunk in result)
        
        self.assertTrue(section1_found)
        self.assertTrue(section2_found)
    
    def test_fixed_size_chunking(self):
        """Test fixed size chunking"""
        # Create a string that repeats to a known size
        text = "0123456789" * 100  # 1000 characters
        
        result = self.chunker.adaptive_chunk_text(
            text,
            max_tokens_per_chunk=50,  # Small to force chunking
            strategy=ChunkingStrategy.FIXED_SIZE
        )
        
        # Should split into multiple chunks
        self.assertGreater(len(result), 1)
        
        # Each chunk (except maybe the last) should be roughly the same size
        chunk_sizes = [len(chunk) for chunk in result]
        for i in range(len(chunk_sizes) - 1):  # Skip the last chunk
            # Allow for some variation due to overlap and boundaries
            self.assertLess(abs(chunk_sizes[i] - chunk_sizes[0]), 100)
    
    def test_json_chunking(self):
        """Test JSON chunking"""
        # Create a JSON array with many items
        json_array = [{"id": i, "value": f"Item {i}"} for i in range(100)]
        text = json.dumps(json_array)
        
        result = self.chunker.adaptive_chunk_text(
            text,
            max_tokens_per_chunk=100,  # Small to force chunking
            strategy=ChunkingStrategy.STRUCTURAL
        )
        
        # Should split into multiple chunks
        self.assertGreater(len(result), 1)
        
        # Each chunk should be valid JSON
        for chunk in result:
            try:
                parsed = json.loads(chunk)
                # Each chunk should have metadata
                self.assertIn("_chunk_info", parsed)
            except json.JSONDecodeError:
                self.fail(f"Chunk is not valid JSON: {chunk[:50]}...")
    
    def test_fluent_api(self):
        """Test fluent API for configuration"""
        text = "This is a test for the fluent API."
        
        # Configure with fluent API
        result = self.chunker \
            .with_max_tokens(100) \
            .with_overlap(10) \
            .with_strategy(ChunkingStrategy.SEMANTIC) \
            .chunk(text)
        
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0], text)
</file>

<file path="tests/test_orchestrator.py">
"""
Tests for the orchestrator module
"""

import unittest
import tempfile
import os
from unittest.mock import patch, MagicMock

from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.models.enums import ChunkingStrategy
from enterprise_chunker.orchestrator import (
    create_auto_chunker,
    SmartParallelChunker,
    DynamicConfig
)


class TestOrchestrator(unittest.TestCase):
    """Test cases for the orchestrator module"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.options = ChunkingOptions(
            strategy=ChunkingStrategy.SIMPLE,
            chunk_size=1000,
            chunk_overlap=200
        )
        
        # Test chunker function
        self.test_chunker = lambda text: [text[i:i+500] for i in range(0, len(text), 400)]
        
        # Sample text
        self.sample_text = "This is a sample text.\n" * 100
    
    def test_create_auto_chunker(self):
        """Test auto chunker creation with different modes"""
        # Test auto mode
        chunker_auto = create_auto_chunker(self.options, mode="auto")
        self.assertIsInstance(chunker_auto, SmartParallelChunker)
        
        # Test performance mode
        chunker_perf = create_auto_chunker(self.options, mode="performance")
        self.assertIsInstance(chunker_perf, SmartParallelChunker)
        self.assertFalse(chunker_perf.memory_safety)
        
        # Test memory-safe mode
        chunker_mem = create_auto_chunker(self.options, mode="memory-safe")
        self.assertIsInstance(chunker_mem, SmartParallelChunker)
        self.assertTrue(chunker_mem.memory_safety)
        
        # Test balanced mode
        chunker_bal = create_auto_chunker(self.options, mode="balanced")
        self.assertIsInstance(chunker_bal, SmartParallelChunker)
    
    def test_simple_strategy(self):
        """Test chunking with simple strategy"""
        chunker = create_auto_chunker(self.options)
        chunker.force_strategy = 'simple'
        
        # Process text
        chunks = chunker.chunk(self.sample_text, self.test_chunker)
        
        # Verify results
        self.assertIsInstance(chunks, list)
        self.assertTrue(len(chunks) > 0)
        self.assertEqual(chunker.metrics.last_strategy, 'simple')
    
    @patch('enterprise_chunker.orchestrator.SmartParallelChunker._create_advanced_chunker')
    def test_strategy_selection(self, mock_create_advanced):
        """Test strategy selection logic"""
        # Setup mock advanced chunker
        mock_advanced = MagicMock()
        mock_advanced.chunk_in_parallel.return_value = ["Chunk1", "Chunk2"]
        mock_create_advanced.return_value = mock_advanced
        
        # Create chunker
        chunker = create_auto_chunker(self.options)
        
        # Force complexity to be high
        with patch('enterprise_chunker.orchestrator.SmartParallelChunker._estimate_complexity', return_value=1.0):
            # Process text
            chunks = chunker.chunk(self.sample_text * 10, self.test_chunker)
            
            # Verify advanced strategy was selected
            self.assertEqual(chunker.metrics.last_strategy, 'advanced')
            mock_advanced.chunk_in_parallel.assert_called_once()
    
    def test_dynamic_config(self):
        """Test dynamic configuration"""
        # Create config
        config = DynamicConfig({
            'processing_timeout': 60.0,
            'max_retries': 2
        })
        
        # Create chunker with config
        chunker = create_auto_chunker(self.options, config=config)
        
        # Verify config values were used
        self.assertEqual(chunker.timeout, 60.0)
        self.assertEqual(chunker.max_retries, 2)
        
        # Update config
        config.set('processing_timeout', 120.0)
        
        # Value should be updated when used
        self.assertEqual(chunker.config.get('processing_timeout'), 120.0)
    
    def test_with_file(self):
        """Test chunking with a file"""
        # Create temporary file
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp:
            temp.write(self.sample_text * 10)
            temp_path = temp.name
        
        try:
            # Create chunker
            chunker = create_auto_chunker(self.options, memory_safety=True)
            
            # Read file and process
            with open(temp_path, 'r') as f:
                text = f.read()
                chunks = chunker.chunk(text, self.test_chunker)
            
            # Verify results
            self.assertIsInstance(chunks, list)
            self.assertTrue(len(chunks) > 0)
            
            # Check metrics
            metrics = chunker.get_metrics()
            self.assertIn('total_chunks_processed', metrics)
            self.assertEqual(metrics['total_chunks_processed'], len(chunks))
            
        finally:
            # Clean up
            os.unlink(temp_path)
    
    def test_priority_chunking(self):
        """Test priority-based chunking"""
        chunker = create_auto_chunker(self.options)
        
        # Process with different priorities
        high_priority_chunks = chunker.chunk_with_priority(
            self.sample_text, self.test_chunker, priority="high"
        )
        
        normal_priority_chunks = chunker.chunk_with_priority(
            self.sample_text, self.test_chunker, priority="normal"
        )
        
        # Both should produce the same results
        self.assertEqual(len(high_priority_chunks), len(normal_priority_chunks))
    
    def test_context_manager(self):
        """Test using the chunker as a context manager"""
        with create_auto_chunker(self.options) as chunker:
            # Process text
            chunks = chunker.chunk(self.sample_text, self.test_chunker)
            self.assertTrue(len(chunks) > 0)
        
        # Chunker should be shut down
        self.assertTrue(chunker._shutdown_event.is_set())
    
    def tearDown(self):
        """Clean up after tests"""
        # Nothing to clean up currently
        pass


if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_strategies/test_formats/test_smalltalk.py">
"""
Unit tests for Smalltalk chunking strategy
"""

import unittest
import re
from typing import List, Dict, Any

from enterprise_chunker import EnterpriseChunker, ChunkingStrategy
from enterprise_chunker.strategies.formats.smalltalk_chunker import SmalltalkChunkingStrategy, SmalltalkDialect


class TestSmalltalkChunking(unittest.TestCase):
    """Test cases for Smalltalk chunking strategy"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.chunker = EnterpriseChunker()
        self.smalltalk_strategy = SmalltalkChunkingStrategy()
    
    def test_dialect_detection(self):
        """Test detection of different Smalltalk dialects"""
        # Test VisualWorks detection
        visualworks_code = """
        !classDefinition: #MyClass category: 'MyApp-Core' superclass: #Object!
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'MyApp-Core'!
            
        !MyClass methodsFor: 'accessing'!
        var1
            ^ var1!
            
        var1: aValue
            var1 := aValue!
            
        !MyClass methodsFor: 'initialization'!
        initialize
            "Initialize the receiver"
            super initialize.
            var1 := nil.
            var2 := 'test'!
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(visualworks_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.VISUALWORKS)
        
        # Test Pharo detection
        pharo_code = """
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            package: 'MyApp-Core'
            
        MyClass >> var1
            <getter>
            ^ var1
            
        MyClass >> var1: aValue
            <setter>
            var1 := aValue
            
        MyClass class >> initialize
            "Initialize the class"
            Smalltalk globals at: #MyGlobal put: self new.
            SystemAnnouncer uniqueInstance announce: ClassInitialized
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(pharo_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.PHARO)
        
        # Test Squeak detection
        squeak_code = """
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'MyApp-Core'
            
        !MyClass methodsFor: 'accessing' stamp: 'jdoe 5/1/2023 10:00'!
        var1
            "Return the value of var1"
            ^ var1
        !
            
        !MyClass methodsFor: 'accessing' stamp: 'jdoe 5/1/2023 10:01'!
        var1: aValue
            "Set the value of var1"
            var1 := aValue.
            self changed: #var1
        !
            
        Morph subclass: #MyMorph
            instanceVariableNames: 'position extent'
            classVariableNames: ''
            poolDictionaries: ''
            category: 'MyApp-UI'
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(squeak_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.SQUEAK)
        
        # Test GemStone detection
        gemstone_code = """
        Object subclass: 'MyClass'
            instVarNames: #(var1 var2)
            classVars: #(ClassVar1)
            classInstVars: #()
            poolDictionaries: #()
            inDictionary: UserGlobals
            constraints: #()
            classConstraints: #()
            
        category: 'accessing'
        method: MyClass
        var1
            ^ var1
        %
            
        category: 'accessing'
        method: MyClass
        var1: aValue
            var1 := aValue
        %
            
        commitTransaction
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(gemstone_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.GEMSTONE)
        
        # Test Dolphin detection
        dolphin_code = """
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            
        !MyClass methodsFor: 'accessing'!
        var1
            "Return the value of var1"
            ^var1!
            
        var1: aValue
            "Set the value of var1"
            var1 := aValue!
            
        !MyClass class methodsFor: 'class initialization'!
        initialize
            "Initialize the class"
            self register!
            
        package paxVersion: 1;
            basicComment: 'Dolphin Smalltalk package'.
        """
        
        detected_dialect = self.smalltalk_strategy._detect_smalltalk_dialect(dolphin_code)
        self.assertEqual(detected_dialect, SmalltalkDialect.DOLPHIN)
    
    def test_class_definition_detection(self):
        """Test detection of class definitions across dialects"""
        # Standard class definition
        std_class_def = """
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'MyApp-Core'
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.STANDARD
        self.smalltalk_strategy._detect_class_definitions(std_class_def, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'class_definition')
        self.assertEqual(boundaries[0]['superclass'], 'Object')
        self.assertEqual(boundaries[0]['subclass'], 'MyClass')
        
        # Pharo class definition with traits
        pharo_class_def = """
        Object subclass: #MyClass
            uses: MyTrait
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            package: 'MyApp-Core'
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.PHARO
        self.smalltalk_strategy._detect_class_definitions(pharo_class_def, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'class_definition')
        self.assertEqual(boundaries[0]['superclass'], 'Object')
        self.assertEqual(boundaries[0]['subclass'], 'MyClass')
        
        # VisualWorks class definition
        vw_class_def = """
        !classDefinition: #MyClass category: 'MyApp-Core' superclass: #Object!
        Object subclass: #MyClass
            instanceVariableNames: 'var1 var2'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'MyApp-Core'!
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.VISUALWORKS
        self.smalltalk_strategy._detect_class_definitions(vw_class_def, boundaries)
        
        # Should detect both the class definition statement and the class definition chunk
        self.assertGreaterEqual(len(boundaries), 1)
        self.assertTrue(any(b['type'] == 'class_definition' for b in boundaries))
        self.assertTrue(any(b['subclass'] == 'MyClass' for b in boundaries))
    
    def test_method_detection(self):
        """Test detection of method definitions in different formats"""
        # Test unary method
        unary_method = """
        initialize
            "Initialize the receiver"
            super initialize.
            var1 := nil.
            var2 := 'test'
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_method_definitions(unary_method, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'method_definition')
        self.assertEqual(boundaries[0]['method_name'], 'initialize')
        self.assertTrue(boundaries[0]['is_unary_method'])
        
        # Test keyword method
        keyword_method = """
        var1: aValue with: anotherValue
            "Set the values"
            var1 := aValue.
            var2 := anotherValue.
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_method_definitions(keyword_method, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'method_definition')
        self.assertEqual(boundaries[0]['method_name'], 'var1: aValue with: anotherValue')
        self.assertTrue(boundaries[0]['is_keyword_method'])
        
        # Test binary method
        binary_method = """
        + aNumber
            "Add aNumber to the receiver"
            ^ self value + aNumber
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_method_definitions(binary_method, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'method_definition')
        self.assertTrue(boundaries[0]['is_binary_method'])
    
    def test_block_detection(self):
        """Test detection of block structures"""
        # Test block with arguments
        block_code = """
        collect: aBlock
            "Evaluate aBlock with each of my elements as the argument."
            | newCollection |
            newCollection := self species new: self size.
            self do: [:each | newCollection add: (aBlock value: each)].
            ^ newCollection
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_block_structures(block_code, boundaries)
        
        # Should find at least the block with arguments [:each |
        self.assertGreaterEqual(len(boundaries), 1)
        self.assertTrue(any(b['type'] == 'block_with_args' for b in boundaries))
        
        # Test method return
        return_code = """
        value
            ^ self calculateValue
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_block_structures(return_code, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertEqual(boundaries[0]['type'], 'method_return')
        
        # Test block with temp variables
        temp_vars_code = """
        calculate
            | temp1 temp2 |
            temp1 := 10.
            temp2 := 20.
            ^ temp1 + temp2
        """
        
        boundaries = []
        self.smalltalk_strategy._detect_block_structures(temp_vars_code, boundaries)
        
        self.assertGreaterEqual(len(boundaries), 1)
        self.assertTrue(any(b['type'] == 'temp_vars' for b in boundaries))
    
    def test_pragma_detection(self):
        """Test detection of method pragmas"""
        # Pharo-style pragma
        pharo_pragma = """
        doSomething
            <primitive: 60>
            <important>
            self doSomethingElse
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.PHARO
        self.smalltalk_strategy._detect_pragmas(pharo_pragma, boundaries)
        
        self.assertEqual(len(boundaries), 2)
        self.assertTrue(all(b['is_pragma'] for b in boundaries))
        self.assertTrue(any(b['pragma_name'] == 'primitive' for b in boundaries))
        self.assertTrue(any(b['pragma_name'] == 'important' for b in boundaries))
        
        # Dolphin-style pragma
        dolphin_pragma = """
        doSomething
            [<primitive: 60>]
            self doSomethingElse
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.DOLPHIN
        self.smalltalk_strategy._detect_pragmas(dolphin_pragma, boundaries)
        
        self.assertEqual(len(boundaries), 1)
        self.assertTrue(boundaries[0]['is_pragma'])
        self.assertEqual(boundaries[0]['pragma_name'], 'primitive')
    
    def test_trait_detection(self):
        """Test detection of trait compositions in Pharo"""
        pharo_trait = """
        Object subclass: #MyClass
            uses: TComparable
            instanceVariableNames: 'var1 var2'
            classVariableNames: ''
            package: 'MyApp-Core'
            
        Object subclass: #AnotherClass
            uses: TComparable + TEnumerable - {#collect:. #select:} @ {#at:->#fetch:}
            instanceVariableNames: 'var1 var2'
            classVariableNames: ''
            package: 'MyApp-Core'
        """
        
        boundaries = []
        self.smalltalk_strategy.dialect = SmalltalkDialect.PHARO
        self.smalltalk_strategy._detect_trait_compositions(pharo_trait, boundaries)
        
        self.assertGreaterEqual(len(boundaries), 1)
        self.assertTrue(any(b['type'] == 'trait_composition' for b in boundaries))
    
    def test_full_chunking(self):
        """Test full chunking process with a realistic Smalltalk file"""
        # Create a sample Smalltalk file with multiple classes and methods
        smalltalk_file = """
        "This is a sample Smalltalk file with multiple classes and methods"
        
        Object subclass: #Person
            instanceVariableNames: 'name age'
            classVariableNames: ''
            poolDictionaries: ''
            category: 'Examples'!
            
        !Person methodsFor: 'accessing'!
        name
            "Return the name"
            ^ name!
            
        name: aString
            "Set the name"
            name := aString!
            
        age
            "Return the age"
            ^ age!
            
        age: anInteger
            "Set the age"
            age := anInteger!
            
        !Person methodsFor: 'printing'!
        printOn: aStream
            "Print a representation of the receiver on aStream"
            super printOn: aStream.
            aStream 
                nextPutAll: ' named ';
                print: name;
                nextPutAll: ' aged ';
                print: age!
                
        !Person methodsFor: 'initialization'!
        initialize
            "Initialize a new instance of the receiver"
            super initialize.
            name := ''.
            age := 0!
            
        !Person class methodsFor: 'instance creation'!
        named: aString
            "Create a new instance with the given name"
            ^ self new
                name: aString;
                yourself!
                
        named: aString aged: anInteger
            "Create a new instance with the given name and age"
            ^ self new
                name: aString;
                age: anInteger;
                yourself!
        """
        
        # Perform chunking
        chunks = self.chunker.adaptive_chunk_text(
            smalltalk_file,
            max_tokens_per_chunk=100,  # Small to force chunking
            strategy=ChunkingStrategy.STRUCTURAL
        )
        
        # Check that we got multiple chunks
        self.assertGreater(len(chunks), 1)
        
        # Check that chunks contain context information
        context_markers = 0
        for chunk in chunks[1:]:  # Skip first chunk
            if '"Smalltalk dialect:' in chunk:
                context_markers += 1
                
        # At least some chunks should have context information
        self.assertGreater(context_markers, 0)
        
        # Check that no method is split in the middle
        incomplete_methods = 0
        for i, chunk in enumerate(chunks):
            # Look for method starts without ends
            method_starts = len(re.findall(r'^\s*\w+(?::\s*\w+\s*)*\s*, chunk, re.MULTILINE))
            method_ends = len(re.findall(r'\^\s*\w+.*?[!\n]', chunk, re.MULTILINE))
            
            if method_starts > method_ends and i < len(chunks) - 1:
                # Check if the next chunk has the ending
                if not re.search(r'^\s*\^', chunks[i+1], re.MULTILINE):
                    incomplete_methods += 1
        
        # There should be no incomplete methods
        self.assertEqual(incomplete_methods, 0, "Found methods split across chunks")
    
    def test_create_context_tracker(self):
        """Test creating and updating context tracker"""
        # Create context tracker
        context = self.smalltalk_strategy._create_context_tracker()
        
        # Check initial state
        self.assertIsNone(context['current_class'])
        self.assertIsNone(context['current_superclass'])
        self.assertEqual(context['instance_vars'], [])
        
        # Update with class definition
        class_boundary = {
            'type': 'class_definition',
            'subclass': 'TestClass',
            'superclass': 'Object',
            'instance_vars': ['var1', 'var2'],
            'class_vars': ['ClassVar'],
            'category': 'Testing'
        }
        
        self.smalltalk_strategy._update_context_tracker(context, class_boundary)
        
        # Check updated state
        self.assertEqual(context['current_class'], 'TestClass')
        self.assertEqual(context['current_superclass'], 'Object')
        self.assertEqual(context['instance_vars'], ['var1', 'var2'])
        self.assertEqual(context['current_category'], 'Testing')
        
        # Update with method definition
        method_boundary = {
            'type': 'method_definition',
            'method_name': 'testMethod',
            'is_class_method': True
        }
        
        self.smalltalk_strategy._update_context_tracker(context, method_boundary)
        
        # Check updated state
        self.assertEqual(context['current_method'], 'testMethod')
        self.assertTrue(context['is_class_side'])
        
        # Get preserved context
        preserved_context = self.smalltalk_strategy._get_preserved_context(context)
        
        # Verify content
        self.assertIn('TestClass', preserved_context)
        self.assertIn('Object', preserved_context)
        self.assertIn('var1', preserved_context)
        self.assertIn('Class side', preserved_context)
    
    def test_file_in_format_detection(self):
        """Test detection of file-in format"""
        # VisualWorks file-in format
        vw_file_in = """
        "This is a VisualWorks file-in"
        !classDefinition: #Person category: 'Examples' superclass: #Object!
        Object subclass: #Person
            instanceVariableNames: 'name age'
            classVariableNames: ''
            poolDictionaries: ''
            category: 'Examples'!
            
        !Person methodsFor: 'accessing'!
        name
            ^ name!
        """
        
        is_file_in = self.smalltalk_strategy._is_file_in_format(vw_file_in)
        self.assertTrue(is_file_in)
        
        # Non file-in format
        regular_code = """
        Object subclass: #Person
            instanceVariableNames: 'name age'
            classVariableNames: ''
            package: 'Examples'.
            
        Person >> name [
            ^ name
        ]
        """
        
        is_file_in = self.smalltalk_strategy._is_file_in_format(regular_code)
        self.assertFalse(is_file_in)
    
    def test_context_preservation(self):
        """Test context preservation between chunks"""
        # Create a sample Smalltalk file
        smalltalk_code = """
        Object subclass: #TestClass
            instanceVariableNames: 'var1 var2 var3'
            classVariableNames: 'ClassVar1'
            poolDictionaries: ''
            category: 'Test-Category'!
            
        !TestClass methodsFor: 'accessing'!
        var1
            "Return var1"
            ^ var1!
            
        var1: aValue
            "Set var1"
            var1 := aValue!
            
        !TestClass methodsFor: 'operations'!
        operation1
            "First operation"
            | temp |
            temp := self var1.
            temp doSomething.
            ^ temp!
            
        operation2
            "Second operation"
            | temp |
            temp := self var1.
            temp doSomethingElse.
            ^ temp!
        """
        
        # Force small chunk size to ensure multiple chunks
        chunks = self.chunker.adaptive_chunk_text(
            smalltalk_code,
            max_tokens_per_chunk=50,  # Very small to force multiple chunks
            strategy=ChunkingStrategy.STRUCTURAL
        )
        
        # Verify we got multiple chunks
        self.assertGreater(len(chunks), 2)
        
        # Count chunks with context preservation
        context_chunks = 0
        for chunk in chunks[1:]:  # Skip first chunk
            if 'Smalltalk dialect:' in chunk or 'Current method:' in chunk:
                context_chunks += 1
        
        # Most chunks after the first should have context
        self.assertGreater(context_chunks, len(chunks) * 0.5)
</file>

<file path="utils/__init__.py">
"""
Package initialization for utility modules
"""

# Import utility modules for easier access
from enterprise_chunker.utils.token_estimation import estimate_tokens
from enterprise_chunker.utils.format_detection import detect_content_format
from enterprise_chunker.utils.memory_optimization import MemoryManager, MemoryMonitor
from enterprise_chunker.utils.optimized_streaming import StreamingBuffer, ChunkProcessor

# List available utility functions for introspection
__all__ = [
    "estimate_tokens",
    "detect_content_format",
    "MemoryManager",
    "MemoryMonitor",
    "StreamingBuffer",
    "ChunkProcessor"
]
</file>

<file path="utils/format_detection.py">
"""
Content format detection utilities.

This module provides automatic detection of content formats based on pattern matching
and heuristic scoring. It analyzes text samples to determine the most likely format
from JSON, XML, Markdown, code, logs, CSV, or plain text.
"""

import re
import json
import logging
from typing import Dict, Any, Tuple
from functools import lru_cache

from enterprise_chunker.models.enums import ContentFormat
from enterprise_chunker.patterns.regex_patterns import RegexPatterns

# Configure logging
logger = logging.getLogger(__name__)


class FormatDetector:
    """Content format detection with confidence scoring."""
    
    def __init__(self, sample_size: int = 2500):
        """
        Initialize the format detector.
        
        Args:
            sample_size: Maximum sample size for format detection
        """
        self.sample_size = sample_size
    
    def detect_format(self, text: str) -> Tuple[ContentFormat, float]:
        """
        Detect the content format based on patterns and heuristics.
        
        Args:
            text: Input text
            
        Returns:
            Tuple of (ContentFormat, confidence_score)
        """
        # Sample the content for faster detection with large texts
        sample_size = min(len(text), self.sample_size)
        sample = text[:sample_size]
        
        # Score each format
        scores = {
            ContentFormat.JSON: self._score_json(sample),
            ContentFormat.XML: self._score_xml(sample),
            ContentFormat.MARKDOWN: self._score_markdown(sample),
            ContentFormat.CODE: self._score_code(sample),
            ContentFormat.LOGS: self._score_logs(sample),
            ContentFormat.CSV: self._score_csv(sample),
            ContentFormat.TEXT: 0.1  # Default low score for plain text
        }
        
        # Select the format with the highest score
        best_format = ContentFormat.TEXT
        best_score = 0.1  # Minimum confidence threshold
        
        for fmt, score in scores.items():
            if score > best_score:
                best_score = score
                best_format = fmt
        
        logger.debug(f"Detected content format: {best_format.value} (confidence: {best_score:.2f})")
        return best_format, best_score
    
    @lru_cache(maxsize=128)
    def _score_json(self, sample: str) -> float:
        """
        Score the likelihood of content being JSON.
        
        Args:
            sample: Content sample
            
        Returns:
            Confidence score (0-1)
        """
        try:
            # Try to parse as JSON - high confidence if valid
            json.loads(sample)
            return 0.95
        except json.JSONDecodeError:
            # Check for JSON-like patterns
            json_markers = len(re.findall(r'[{}\[\],:"]', sample)) / (len(sample) or 1)
            string_literals = len(re.findall(r'"(?:[^"\\]|\\.)*"', sample))
            json_keywords = len(re.findall(r'\b(?:true|false|null)\b', sample))
            
            # Calculate score
            score = 0
            if re.match(r'^\s*[\[{]', sample):
                score += 0.3
                
            score += min(0.4, json_markers * 5)
            score += min(0.2, (string_literals / 10) * 0.3)
            score += min(0.1, (json_keywords / 5) * 0.2)
            
            return min(0.9, score)
    
    @lru_cache(maxsize=128)
    def _score_xml(self, sample: str) -> float:
        """
        Score the likelihood of content being XML or HTML.
        
        Args:
            sample: Content sample
            
        Returns:
            Confidence score (0-1)
        """
        # Check for XML declaration or doctype
        if re.search(r'<\?xml|<!DOCTYPE', sample, re.IGNORECASE):
            return 0.9
            
        # Count tags and attributes
        open_tags = len(re.findall(r'<[a-zA-Z][^>]*>', sample))
        close_tags = len(re.findall(r'</[a-zA-Z][^>]*>', sample))
        attributes = len(re.findall(r'\s[a-zA-Z][a-zA-Z0-9]*=["\'][^"\']*["|\']', sample))
        
        # Calculate score
        score = 0
        score += min(0.4, (open_tags / 20) * 0.4)
        score += min(0.3, (close_tags / 20) * 0.3)
        score += min(0.2, (attributes / 10) * 0.2)
        
        # Check for HTML-specific patterns
        if re.search(r'<html|<body|<div|<p|<span|<h[1-6]|<a\s|<img|<table', sample, re.IGNORECASE):
            score += 0.3
            
        return min(0.9, score)
    
    @lru_cache(maxsize=128)
    def _score_markdown(self, sample: str) -> float:
        """
        Score the likelihood of content being Markdown.
        
        Args:
            sample: Content sample
            
        Returns:
            Confidence score (0-1)
        """
        # Count markdown-specific patterns
        headings = len(re.findall(r'^#{1,6}\s+.+$', sample, re.MULTILINE))
        alt_headings = len(re.findall(r'^[^\n]+\n[=\-]{2,}$', sample, re.MULTILINE))
        list_items = len(re.findall(r'^[\s]*[-*+]\s+.+$', sample, re.MULTILINE))
        numbered_items = len(re.findall(r'^[\s]*\d+\.\s+.+$', sample, re.MULTILINE))
        code_blocks = len(re.findall(r'^```[\s\S]*?```$', sample, re.MULTILINE))
        links = len(re.findall(r'\[.+?\]\(.+?\)', sample))
        emphasis = len(re.findall(r'(\*\*|__).+?(\*\*|__)', sample))
        
        # Calculate score
        score = 0
        score += min(0.3, (headings / 5) * 0.3)
        score += min(0.2, (alt_headings / 2) * 0.2)
        score += min(0.2, (list_items / 10) * 0.2)
        score += min(0.1, (numbered_items / 5) * 0.1)
        score += min(0.2, (code_blocks / 2) * 0.2)
        score += min(0.1, (links / 5) * 0.1)
        score += min(0.1, (emphasis / 5) * 0.1)
        
        # Additional checks
        if re.search(r'!\[.*?\]\(.*?\)', sample):  # Images
            score += 0.1
        if re.search(r'>\s+.*?', sample):  # Blockquotes
            score += 0.1
            
        return min(0.9, score)
    
    @lru_cache(maxsize=128)
    def _score_code(self, sample: str) -> float:
        """
        Score the likelihood of content being source code.
        
        Args:
            sample: Content sample
            
        Returns:
            Confidence score (0-1)
        """
        # Try to remove strings and comments for more accurate detection
        cleaned = re.sub(r'"(?:[^"\\]|\\.)*"', '""', sample)
        cleaned = re.sub(r'\'(?:[^\'\\]|\\.)*\'', "''", cleaned)
        cleaned = re.sub(r'/\*[\s\S]*?\*/', '', cleaned)
        cleaned = re.sub(r'//.*$', '', cleaned, flags=re.MULTILINE)
        
        # Count code-specific patterns
        keywords = [
            'function', 'class', 'if', 'else', 'for', 'while', 'return',
            'var', 'const', 'let', 'import', 'export', 'try', 'catch',
            'def', 'from', 'import', 'public', 'private', 'protected'
        ]
        
        keyword_pattern = r'\b(?:' + '|'.join(keywords) + r')\b'
        keyword_count = len(re.findall(keyword_pattern, cleaned))
        
        bracket_pairs = min(
            len(re.findall(r'\{', cleaned)), 
            len(re.findall(r'\}', cleaned))
        )
        
        semicolons = len(re.findall(r';', cleaned))
        indentation = len(re.findall(r'^\s+', cleaned, re.MULTILINE))
        
        # Language-specific checks
        js_patterns = len(re.findall(r'\b(?:typeof|undefined|null|console|document|window)\b', cleaned))
        python_patterns = len(re.findall(r'\b(?:def|elif|lambda|self|__init__|pass)\b', cleaned))
        java_patterns = len(re.findall(r'\b(?:public|private|protected|static|void|extends|implements)\b', cleaned))
        
        # Calculate score
        score = 0
        score += min(0.3, (keyword_count / 20) * 0.3)
        score += min(0.2, (bracket_pairs / 10) * 0.2)
        score += min(0.1, (semicolons / 15) * 0.1)
        score += min(0.2, (indentation / 15) * 0.2)
        score += min(0.1, (js_patterns / 5) * 0.1)
        score += min(0.1, (python_patterns / 5) * 0.1)
        score += min(0.1, (java_patterns / 5) * 0.1)
        
        # Check for common code patterns
        if re.search(r'^(?:function|class|import|export|const|let|var|if|for|while)\b', sample, re.MULTILINE):
            score += 0.2
            
        return min(0.9, score)
    
    @lru_cache(maxsize=128)
    def _score_logs(self, sample: str) -> float:
        """
        Score the likelihood of content being log file data.
        
        Args:
            sample: Content sample
            
        Returns:
            Confidence score (0-1)
        """
        # Split into lines for analysis
        lines = sample.split('\n')[:20]  # Use up to first 20 lines
        
        # Count timestamp-like patterns
        timestamp_patterns = [
            r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
            r'\d{1,2}/\d{1,2}/\d{2,4}',  # MM/DD/YYYY
            r'\d{2}:\d{2}:\d{2}',  # HH:MM:SS
            r'\[\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?\]',  # ISO timestamps with brackets
        ]
        
        timestamp_count = sum(
            1 for line in lines
            for pattern in timestamp_patterns
            if re.search(pattern, line)
        )
        
        # Count log level indicators
        log_level_count = len([
            line for line in lines 
            if re.search(r'\b(?:ERROR|WARN(?:ING)?|INFO|DEBUG|TRACE|FATAL|CRITICAL|NOTICE|SEVERE)\b', line, re.IGNORECASE)
        ])
        
        # Count stack trace indicators
        stack_trace_patterns = [
            r'at .+\(.+\.(?:java|js|ts|cs|py):\d+\)',
            r'\s+at\s+[\w.$]+(?:\.[\w.$]+)+\([^)]*\)',
            r'File ".+", line \d+'
        ]
        
        stack_trace_count = sum(
            1 for line in lines
            for pattern in stack_trace_patterns
            if re.search(pattern, line)
        )
        
        # Calculate score
        score = 0
        if lines:
            score += min(0.5, (timestamp_count / len(lines)) * 0.5)
            score += min(0.4, (log_level_count / len(lines)) * 0.4)
            score += min(0.3, (stack_trace_count / len(lines)) * 0.3)
        
        # Check for platform-specific log formats
        if re.search(r'\[\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?\]', sample):
            score += 0.2  # ISO 8601
        if re.search(r'\<\d+\>\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}\s+\w+', sample):
            score += 0.2  # RFC 3164
        if re.search(r'^(?:[A-Z])\/[^\(]+\(\s*\d+\):', sample, re.MULTILINE):
            score += 0.2  # Android
            
        return min(0.9, score)
    
    @lru_cache(maxsize=128)
    def _score_csv(self, sample: str) -> float:
        """
        Score the likelihood of content being CSV data.
        
        Args:
            sample: Content sample
            
        Returns:
            Confidence score (0-1)
        """
        lines = sample.strip().split('\n')[:10]  # Check first 10 lines at most
        if not lines:
            return 0.0
            
        # Check for consistent delimiters across lines
        delimiters = [',', ';', '\t', '|']
        consistent_structure = False
        first_line_fields = None
        
        # Analyze structure consistency
        for delimiter in delimiters:
            fields_per_line = [line.count(delimiter) + 1 for line in lines]
            
            # Check for consistent fields per line
            if len(set(fields_per_line)) <= 2:  # Allow for header + data rows
                # Check for reasonable number of fields (at least 2)
                if fields_per_line[0] >= 2:
                    consistent_structure = True
                    first_line_fields = fields_per_line[0]
                    break
                    
        # Calculate score
        score = 0
        
        if consistent_structure:
            score += 0.5  # Consistent structure is a strong indicator
            
            # Check for header-like first line
            if len(lines) > 1:
                header_line = lines[0]
                data_line = lines[1]
                
                header_has_quotes = '"' in header_line
                data_has_numeric = bool(re.search(r'\d+(?:\.\d+)?', data_line))
                
                if header_has_quotes or not data_has_numeric:
                    score += 0.1
                    
            # Check for balanced quotes
            quotes_balanced = all(
                line.count('"') % 2 == 0 for line in lines
            )
            if quotes_balanced:
                score += 0.1
                
            # Check for a good number of fields
            if first_line_fields is not None:
                if 2 <= first_line_fields <= 20:  # Typical CSV range
                    score += 0.1
                    
        return min(0.85, score)


def detect_content_format(text: str, sample_size: int = 2500) -> ContentFormat:
    """
    Convenience function to detect content format.
    
    Args:
        text: Text to analyze
        sample_size: Maximum sample size for format detection
        
    Returns:
        Detected ContentFormat
    """
    detector = FormatDetector(sample_size)
    format_type, confidence = detector.detect_format(text)
    return format_type
</file>

<file path="utils/memory_optimization.py">
"""
Memory optimization utilities for efficient processing of large files
"""

import os
import gc
import sys
import logging
import tempfile
import mmap
import io
import platform
import re

# Only import resource on Unix-like systems
if platform.system() != 'Windows':
    import resource

from typing import Generator, List, Dict, Any, Optional, BinaryIO, Union, Iterator, TextIO, Callable
from contextlib import contextmanager

# Configure logging
logger = logging.getLogger(__name__)


class MemoryManager:
    """
    Memory management utilities for efficient processing of large content
    """
    
    def __init__(self, low_memory_mode: bool = False):
        """
        Initialize the memory manager
        
        Args:
            low_memory_mode: Whether to use aggressive memory optimization
        """
        self.low_memory_mode = low_memory_mode
        self._temp_files = []
    
    def get_memory_usage(self) -> float:
        """
        Get current memory usage in MB
        
        Returns:
            Memory usage in megabytes
        """
        try:
            # Use resource module if available (Unix systems)
            if platform.system() != 'Windows' and 'resource' in sys.modules:
                return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024.0
            else:
                # Fallback for Windows and other systems
                try:
                    import psutil
                    process = psutil.Process(os.getpid())
                    return process.memory_info().rss / 1024.0 / 1024.0
                except ImportError:
                    # If psutil isn't available, use a basic estimation
                    return 0.0  # Return 0 as we can't measure it
        except (AttributeError, ValueError):
            # Final fallback
            return 0.0
    
    def estimate_memory_impact(self, text: str) -> float:
        """
        Estimate memory impact of a string in MB
        
        Args:
            text: String to estimate
            
        Returns:
            Estimated memory usage in megabytes
        """
        # String size in Python is more than just character count due to Unicode
        # and Python's string implementation overhead
        
        # Get basic size
        basic_size = sys.getsizeof(text)
        
        # Add estimated overhead for Python strings
        overhead = 48  # Typical Python string overhead
        
        # Calculate total estimated size in MB
        return (basic_size + overhead) / 1024.0 / 1024.0
    
    def reduce_memory_usage(self, force: bool = False):
        """
        Attempt to reduce memory usage through garbage collection
        
        Args:
            force: Whether to force aggressive garbage collection
        """
        # Run garbage collection
        collected = gc.collect()
        logger.debug(f"Garbage collected {collected} objects")
        
        if force or self.low_memory_mode:
            # More aggressive memory cleanup
            # Force collection of all generations
            gc.collect(0)
            gc.collect(1)
            gc.collect(2)
            
            # On some systems, explicitly requesting memory release can help
            if hasattr(os, 'malloc_trim') and callable(os.malloc_trim):
                try:
                    os.malloc_trim(0)  # Only available on some Linux systems
                except Exception as e:
                    logger.debug(f"malloc_trim failed: {e}")
    
    @contextmanager
    def memory_efficient_context(self, memory_limit_mb: Optional[float] = None):
        """
        Context manager for memory-efficient processing
        
        Args:
            memory_limit_mb: Optional memory limit in MB to enforce
            
        Yields:
            Context for memory-efficient processing
        """
        # Get initial memory usage
        initial_memory = self.get_memory_usage()
        logger.debug(f"Initial memory usage: {initial_memory:.2f} MB")
        
        if memory_limit_mb:
            # Set soft resource limit if possible (Unix only)
            if platform.system() != 'Windows' and 'resource' in sys.modules:
                try:
                    resource.setrlimit(resource.RLIMIT_AS, (
                        memory_limit_mb * 1024 * 1024,  # Soft limit
                        resource.RLIM_INFINITY  # Hard limit (no hard limit)
                    ))
                    logger.debug(f"Set memory limit to {memory_limit_mb} MB")
                except (AttributeError, ValueError):
                    logger.warning("Could not set memory limit")
            else:
                logger.debug(f"Memory limit of {memory_limit_mb} MB requested but not enforced (not supported on this platform)")
        
        try:
            # Yield control back to the caller
            yield self
        finally:
            # Clean up resources
            self.reduce_memory_usage()
            
            # Clean up any temporary files
            for temp_file in self._temp_files:
                try:
                    if os.path.exists(temp_file):
                        os.unlink(temp_file)
                except Exception as e:
                    logger.warning(f"Failed to delete temporary file {temp_file}: {e}")
            
            # Log memory usage
            final_memory = self.get_memory_usage()
            logger.debug(f"Final memory usage: {final_memory:.2f} MB")
            logger.debug(f"Memory change: {final_memory - initial_memory:.2f} MB")
    
    def to_disk_if_large(self, content: str, threshold_mb: float = 10.0) -> str:
        """
        Move large content to disk if it exceeds threshold
        
        Args:
            content: String content to potentially move to disk
            threshold_mb: Size threshold in MB
            
        Returns:
            Either the original string or a path to a temporary file (prefixed with 'file:')
        """
        # Estimate size
        estimated_size = self.estimate_memory_impact(content)
        
        # If under threshold, just return the string
        if estimated_size < threshold_mb:
            return content
            
        # Otherwise, write to temporary file
        with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8') as temp:
            temp.write(content)
            temp_path = temp.name
            self._temp_files.append(temp_path)
            
        logger.debug(f"Moved {estimated_size:.2f} MB of content to disk at {temp_path}")
        
        # Return the path prefixed with 'file:' to indicate it's a file path
        return f"file:{temp_path}"
    
    def get_content(self, content_or_path: str) -> str:
        """
        Get content from either a string or a file path
        
        Args:
            content_or_path: Either content string or a file path (prefixed with 'file:')
            
        Returns:
            The content string
        """
        # Check if it's a file path
        if isinstance(content_or_path, str) and content_or_path.startswith("file:"):
            # Extract path
            path = content_or_path[5:]
            
            # Read from file
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        
        # Otherwise, it's the content itself
        return content_or_path
    
    def cleanup(self):
        """Clean up any temporary files"""
        for temp_file in self._temp_files:
            try:
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
            except Exception as e:
                logger.warning(f"Failed to delete temporary file {temp_file}: {e}")
        
        # Clear the list
        self._temp_files = []
    
    def __del__(self):
        """Ensure cleanup on deletion"""
        self.cleanup()


class MemoryEfficientIterator:
    """
    Memory-efficient iterator for large content processing
    """
    
    def __init__(self, processor_func: Callable, buffer_size: int = 100000):
        """
        Initialize the memory-efficient iterator
        
        Args:
            processor_func: Function to process each chunk
            buffer_size: Buffer size for processing
        """
        self.processor_func = processor_func
        self.buffer_size = buffer_size
        self.memory_manager = MemoryManager()
    
    def iter_file(self, file_path: str) -> Generator[Any, None, None]:
        """
        Process a file efficiently
        
        Args:
            file_path: Path to file
            
        Yields:
            Processed results
        """
        # Process file in chunks to minimize memory usage
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            chunk_buffer = io.StringIO()
            overflow = ""
            
            while True:
                # Read a chunk
                chunk = f.read(self.buffer_size)
                if not chunk and not overflow and chunk_buffer.tell() == 0:
                    break
                
                # Reset buffer if needed
                if chunk_buffer.tell() > 0:
                    chunk_buffer.seek(0)
                    chunk_buffer.truncate(0)
                
                # Add overflow from previous iteration and new chunk to buffer
                if overflow:
                    chunk_buffer.write(overflow)
                    overflow = ""
                
                if chunk:
                    chunk_buffer.write(chunk)
                
                # If we have enough data or this is the last chunk
                buffer_size = chunk_buffer.tell()
                if buffer_size >= self.buffer_size or not chunk:
                    # Get the buffer content
                    chunk_buffer.seek(0)
                    current_buffer = chunk_buffer.read()
                    
                    # If we're not at the end, find a safe split point
                    if chunk:
                        split_index = self._find_safe_split_point(current_buffer)
                        process_text = current_buffer[:split_index]
                        overflow = current_buffer[split_index:]
                    else:
                        # Process all remaining content
                        process_text = current_buffer
                    
                    # Process the chunk
                    results = self.processor_func(process_text)
                    
                    # Yield results
                    if isinstance(results, list):
                        for result in results:
                            yield result
                    else:
                        yield results
                    
                    # Reduce memory usage
                    self.memory_manager.reduce_memory_usage()
                    
                    # Reset buffer for next iteration
                    chunk_buffer.seek(0)
                    chunk_buffer.truncate(0)
    
    def iter_text(self, text: str) -> Generator[Any, None, None]:
        """
        Process text efficiently
        
        Args:
            text: Text to process
            
        Yields:
            Processed results
        """
        # For very large strings, write to a temporary file first
        if len(text) > self.buffer_size * 10:
            with self.memory_manager.memory_efficient_context():
                # Write to temporary file
                content_or_path = self.memory_manager.to_disk_if_large(text)
                
                if content_or_path.startswith("file:"):
                    # Process from file
                    for result in self.iter_file(content_or_path[5:]):
                        yield result
                    return
        
        # Process directly for smaller strings
        # Split into manageable chunks
        start = 0
        text_len = len(text)
        
        while start < text_len:
            # Determine chunk size
            end = min(start + self.buffer_size, text_len)
            
            # Adjust to a safe boundary if not at the end
            if end < text_len:
                # Find safe split point
                segment = text[start:min(end + 1000, text_len)]  # Add some overlap for finding a good boundary
                relative_split = self._find_safe_split_point(segment)
                end = start + relative_split
            
            # Process the chunk
            chunk = text[start:end]
            results = self.processor_func(chunk)
            
            # Yield results
            if isinstance(results, list):
                for result in results:
                    yield result
            else:
                yield results
            
            # Move to next position
            start = end
            
            # Reduce memory usage
            self.memory_manager.reduce_memory_usage()
    
    def _find_safe_split_point(self, text: str) -> int:
        """
        Find a safe point to split text
        
        Args:
            text: Text to split
            
        Returns:
            Index for safe splitting
        """
        text_len = len(text)
        if text_len == 0:
            return 0
            
        # Target point (80% of the way through the buffer)
        target_point = min(int(text_len * 0.8), text_len - 1)
        
        # Try to find a good boundary
        # Check for paragraph breaks first
        for i in range(target_point, max(0, target_point - 1000), -1):
            if i < text_len - 1 and text[i] == '\n' and text[i+1] == '\n':
                return i + 2  # Include both newlines
        
        # Try to find newlines
        for i in range(target_point, max(0, target_point - 500), -1):
            if text[i] == '\n':
                return i + 1  # Include the newline
        
        # Try to find sentence ends
        for i in range(target_point, max(0, target_point - 200), -1):
            if i < text_len - 2 and text[i] in '.!?' and text[i+1] == ' ' and text[i+2].isupper():
                return i + 1  # After punctuation
        
        # Try to find spaces
        for i in range(target_point, max(0, target_point - 100), -1):
            if text[i] == ' ':
                return i + 1  # Include the space
        
        # Fallback to target point
        return target_point


class MemoryMonitor:
    """
    Memory usage monitoring for detecting and preventing memory issues
    """
    
    def __init__(self, warning_threshold_mb: float = 1000.0, critical_threshold_mb: float = 2000.0):
        """
        Initialize the memory monitor
        
        Args:
            warning_threshold_mb: Memory threshold for warnings in MB
            critical_threshold_mb: Memory threshold for critical actions in MB
        """
        self.warning_threshold = warning_threshold_mb
        self.critical_threshold = critical_threshold_mb
        self.memory_manager = MemoryManager()
        self.peak_memory = 0.0
    
    def check_memory(self) -> Dict[str, Any]:
        """
        Check current memory usage and return status
        
        Returns:
            Dictionary with memory status information
        """
        current_memory = self.memory_manager.get_memory_usage()
        
        # Update peak memory
        self.peak_memory = max(self.peak_memory, current_memory)
        
        # Determine status
        if current_memory >= self.critical_threshold:
            status = "critical"
            self.memory_manager.reduce_memory_usage(force=True)
        elif current_memory >= self.warning_threshold:
            status = "warning"
            self.memory_manager.reduce_memory_usage()
        else:
            status = "normal"
        
        return {
            "current_mb": current_memory,
            "peak_mb": self.peak_memory,
            "status": status,
            "warning_threshold_mb": self.warning_threshold,
            "critical_threshold_mb": self.critical_threshold
        }
    
    @contextmanager
    def monitor_operation(self, operation_name: str, check_interval: int = 10):
        """
        Context manager for monitoring memory during an operation
        
        Args:
            operation_name: Name of the operation for logging
            check_interval: How often to check memory (in operations)
            
        Yields:
            Memory check function to call periodically
        """
        logger.debug(f"Starting memory monitoring for operation: {operation_name}")
        operation_count = [0]  # Use list for mutable reference
        
        def check_if_needed():
            """Check memory if needed based on interval"""
            operation_count[0] += 1
            if operation_count[0] % check_interval == 0:
                status = self.check_memory()
                if status["status"] != "normal":
                    logger.warning(
                        f"Memory {status['status']} in {operation_name}: "
                        f"{status['current_mb']:.2f}MB used "
                        f"(peak: {status['peak_mb']:.2f}MB)"
                    )
                return status
            return None
        
        try:
            # Start with initial check
            initial_status = self.check_memory()
            logger.debug(
                f"Initial memory for {operation_name}: "
                f"{initial_status['current_mb']:.2f}MB"
            )
            
            # Yield the check function
            yield check_if_needed
            
        finally:
            # Final check
            final_status = self.check_memory()
            logger.debug(
                f"Final memory for {operation_name}: "
                f"{final_status['current_mb']:.2f}MB "
                f"(peak: {final_status['peak_mb']:.2f}MB)"
            )
            
            # Cleanup
            self.memory_manager.reduce_memory_usage()
</file>

<file path="utils/optimized_streaming.py">
"""
Optimized streaming utilities for processing large files
"""

import io
import os
import mmap
import logging
import tempfile
from typing import Generator, List, Dict, Any, Optional, BinaryIO, Union, Iterator, TextIO, TypeVar, Generic, overload
from contextlib import contextmanager

from enterprise_chunker.models.enums import ChunkingStrategy
from enterprise_chunker.config import ChunkingOptions

# Configure logging
logger = logging.getLogger(__name__)

# Type variable for generic typing
T = TypeVar('T', str, bytes)


class StreamingBuffer(Generic[T]):
    """
    Memory-efficient buffer for large text processing
    
    Optimized for handling very large files with minimal memory impact.
    Uses memory mapping and chunked processing to avoid loading entire
    files into memory.
    """
    
    def __init__(self, buffer_size: int = 100000, overlap_size: int = 5000):
        """
        Initialize the streaming buffer
        
        Args:
            buffer_size: Size of each processing buffer
            overlap_size: Size of overlap between buffers
        """
        self.buffer_size = buffer_size
        self.overlap_size = overlap_size
        self._temp_files = []
        # Maximum characters to search back when finding split points
        self._max_backscan = 2000
    
    @overload
    def stream_file(self, file_path: str, raw_bytes: bool = False) -> Generator[str, None, None]: ...
    
    @overload
    def stream_file(self, file_path: str, raw_bytes: bool = True) -> Generator[bytes, None, None]: ...
    
    def stream_file(self, file_path: str, raw_bytes: bool = False) -> Generator[Union[str, bytes], None, None]:
        """
        Stream a file in chunks with efficient memory usage
        
        Args:
            file_path: Path to file to stream
            raw_bytes: If True, yield raw bytes instead of decoding to UTF-8
            
        Yields:
            Text chunks from the file (str or bytes depending on raw_bytes)
        """
        # Check if file exists
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
            
        # Get file size
        file_size = os.path.getsize(file_path)
        
        # For small files, read directly
        if file_size < self.buffer_size * 2:
            if raw_bytes:
                with open(file_path, 'rb') as f:
                    yield f.read()
            else:
                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                    yield f.read()
            return
        
        # For large files, use memory mapping
        with open(file_path, 'rb') as f:
            try:
                # Try memory mapping for efficient random access
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    position = 0
                    while position < mm.size():
                        # Determine chunk size
                        chunk_size = min(self.buffer_size, mm.size() - position)
                        
                        # Read chunk
                        mm.seek(position)
                        chunk = mm.read(chunk_size)
                        
                        if raw_bytes:
                            # Find a safe boundary in bytes (e.g., newlines)
                            safe_pos = self._find_safe_split_point_bytes(chunk)
                            # Yield the raw bytes chunk
                            yield chunk[:safe_pos]
                        else:
                            # Convert to string, handling encoding errors
                            text_chunk = chunk.decode('utf-8', errors='replace')
                            
                            # Find a safe boundary to split on
                            safe_pos = self._find_safe_split_point(text_chunk)
                            
                            # Yield the chunk up to the safe point
                            yield text_chunk[:safe_pos]
                            
                            # Convert safe_pos back to bytes for position tracking
                            safe_pos_bytes = len(text_chunk[:safe_pos].encode('utf-8'))
                            position = position + safe_pos_bytes
                            
                            # Handle overlap - backtrack if needed
                            if position < mm.size() and safe_pos_bytes > self.overlap_size:
                                position -= self.overlap_size
                            
                            continue
                        
                        # Only reached for raw_bytes mode
                        # Move to next position, with overlap
                        position = position + safe_pos
                        
                        # Handle overlap - backtrack if needed
                        if position < mm.size() and safe_pos > self.overlap_size:
                            position -= self.overlap_size
            except (ValueError, OSError):
                # Fallback if memory mapping fails
                logger.warning("Memory mapping failed, falling back to chunked reading")
                # Reset file pointer
                f.seek(0)
                
                # Process in chunks
                if raw_bytes:
                    buffer = bytearray()
                    while True:
                        chunk = f.read(self.buffer_size)
                        if not chunk:
                            break
                            
                        # Add to buffer
                        buffer.extend(chunk)
                        
                        # Find a safe place to split
                        if len(buffer) > self.buffer_size:
                            safe_pos = self._find_safe_split_point_bytes(buffer)
                            
                            # Yield the chunk
                            yield bytes(buffer[:safe_pos])
                            
                            # Keep the remainder with overlap
                            if safe_pos > self.overlap_size:
                                buffer = buffer[safe_pos - self.overlap_size:]
                            else:
                                buffer = buffer[safe_pos:]
                    
                    # Yield remaining buffer
                    if buffer:
                        yield bytes(buffer)
                else:
                    # Text mode processing
                    buffer = ""
                    while True:
                        chunk = f.read(self.buffer_size)
                        if not chunk:
                            break
                            
                        # Convert to string
                        text_chunk = chunk.decode('utf-8', errors='replace')
                        
                        # Add to buffer
                        buffer += text_chunk
                        
                        # Find a safe place to split
                        if len(buffer) > self.buffer_size:
                            safe_pos = self._find_safe_split_point(buffer)
                            
                            # Yield the chunk
                            yield buffer[:safe_pos]
                            
                            # Keep the remainder with overlap
                            if safe_pos > self.overlap_size:
                                buffer = buffer[safe_pos - self.overlap_size:]
                            else:
                                buffer = buffer[safe_pos:]
                    
                    # Yield remaining buffer
                    if buffer:
                        yield buffer
    
    @overload
    def stream_string(self, text: str) -> Generator[str, None, None]: ...
    
    @overload
    def stream_string(self, text: bytes, raw_bytes: bool = True) -> Generator[bytes, None, None]: ...
    
    def stream_string(self, text: Union[str, bytes], raw_bytes: bool = False) -> Generator[Union[str, bytes], None, None]:
        """
        Stream a large string in chunks with efficient memory usage
        
        For very large strings, this writes to a temporary file first
        to avoid memory issues.
        
        Args:
            text: Large string or bytes to process
            raw_bytes: If True, treat input as bytes and yield bytes
            
        Yields:
            Text or bytes chunks
        """
        # Handle type checking for input
        if raw_bytes and isinstance(text, str):
            text = text.encode('utf-8')
        elif not raw_bytes and isinstance(text, bytes):
            text = text.decode('utf-8', errors='replace')
        
        # For small strings, just yield directly
        if len(text) < self.buffer_size * 2:
            yield text
            return
            
        # For large strings, use a temporary file
        with self._create_temp_file() as temp_file:
            # Write data to file
            if isinstance(text, str):
                temp_file.write(text.encode('utf-8'))
            else:
                temp_file.write(text)
            temp_file.flush()
            
            # Stream from the file
            for chunk in self.stream_file(temp_file.name, raw_bytes=raw_bytes):
                yield chunk
    
    @overload
    def stream_handle(self, file_handle: TextIO) -> Generator[str, None, None]: ...
    
    @overload
    def stream_handle(self, file_handle: BinaryIO, raw_bytes: bool = True) -> Generator[bytes, None, None]: ...
    
    def stream_handle(self, file_handle: Union[TextIO, BinaryIO], raw_bytes: bool = False) -> Generator[Union[str, bytes], None, None]:
        """
        Stream content from an open file handle
        
        Args:
            file_handle: Open file handle to read from
            raw_bytes: If True, handle is treated as binary and yields bytes
            
        Yields:
            Text or bytes chunks
        """
        # Check if we're dealing with a binary file in raw_bytes mode
        is_binary_handle = hasattr(file_handle, 'mode') and 'b' in file_handle.mode
        
        if raw_bytes and not is_binary_handle:
            logger.warning("Using raw_bytes=True with a text file handle may lead to unexpected results")
        elif not raw_bytes and is_binary_handle:
            logger.warning("Using raw_bytes=False with a binary file handle may lead to unexpected results")
            
        if raw_bytes:
            # Binary processing
            buffer = bytearray()
            
            while True:
                chunk = file_handle.read(self.buffer_size)
                if not chunk:
                    break
                    
                # Add to buffer
                if isinstance(chunk, str):
                    buffer.extend(chunk.encode('utf-8'))
                else:
                    buffer.extend(chunk)
                
                # Process when buffer is large enough
                if len(buffer) > self.buffer_size:
                    safe_pos = self._find_safe_split_point_bytes(buffer)
                    
                    # Yield the chunk
                    yield bytes(buffer[:safe_pos])
                    
                    # Keep the remainder with overlap
                    if safe_pos > self.overlap_size:
                        buffer = buffer[safe_pos - self.overlap_size:]
                    else:
                        buffer = buffer[safe_pos:]
            
            # Yield remaining buffer
            if buffer:
                yield bytes(buffer)
        else:
            # Text processing
            buffer = ""
            
            while True:
                chunk = file_handle.read(self.buffer_size)
                if not chunk:
                    break
                    
                # Convert to string if needed
                if isinstance(chunk, bytes):
                    chunk = chunk.decode('utf-8', errors='replace')
                    
                # Add to buffer
                buffer += chunk
                
                # Process when buffer is large enough
                if len(buffer) > self.buffer_size:
                    safe_pos = self._find_safe_split_point(buffer)
                    
                    # Yield the chunk
                    yield buffer[:safe_pos]
                    
                    # Keep the remainder with overlap
                    if safe_pos > self.overlap_size:
                        buffer = buffer[safe_pos - self.overlap_size:]
                    else:
                        buffer = buffer[safe_pos:]
            
            # Yield remaining buffer
            if buffer:
                yield buffer
    
    def _find_safe_split_point(self, text: str) -> int:
        """
        Find a safe point to split text that preserves semantic boundaries
        
        Args:
            text: Text to analyze
            
        Returns:
            Position where it's safe to split
        """
        # Ensure we have text to process
        if not text:
            return 0
            
        # Target point for splitting (80% of buffer)
        text_len = len(text)
        target_point = min(int(text_len * 0.8), text_len - 1)
        
        # Calculate maximum backscan distances with caps
        para_scan = min(int(target_point * 0.2), self._max_backscan)
        line_scan = min(int(target_point * 0.1), self._max_backscan // 2)
        sent_scan = min(int(target_point * 0.05), self._max_backscan // 4)
        word_scan = min(100, self._max_backscan // 20)
        
        # Try to find paragraph breaks first (highest priority)
        for i in range(target_point, max(0, target_point - para_scan), -1):
            if i < text_len - 1 and text[i] == '\n' and text[i+1] == '\n':
                return i + 2  # Include both newlines
                
        # Try to find single line breaks
        for i in range(target_point, max(0, target_point - line_scan), -1):
            if text[i] == '\n':
                return i + 1  # Include the newline
                
        # Try to find sentence boundaries
        for i in range(target_point, max(0, target_point - sent_scan), -1):
            if i < text_len - 2 and text[i] in '.!?' and text[i+1] == ' ' and text[i+2].isupper():
                return i + 1  # Split after the punctuation
                
        # Last resort: split at a word boundary
        for i in range(target_point, max(0, target_point - word_scan), -1):
            if text[i] == ' ':
                return i + 1  # Include the space
                
        # Absolute fallback: just use the target point
        return target_point
    
    def _find_safe_split_point_bytes(self, data: Union[bytes, bytearray]) -> int:
        """
        Find a safe point to split binary data
        
        Args:
            data: Binary data to analyze
            
        Returns:
            Position where it's safe to split
        """
        # Ensure we have data to process
        if not data:
            return 0
            
        # Target point for splitting (80% of buffer)
        data_len = len(data)
        target_point = min(int(data_len * 0.8), data_len - 1)
        
        # Calculate maximum backscan distances with caps
        para_scan = min(int(target_point * 0.2), self._max_backscan)
        line_scan = min(int(target_point * 0.1), self._max_backscan // 2)
        word_scan = min(100, self._max_backscan // 20)
        
        # Common delimiters in bytes
        NEWLINE = 10  # \n in ASCII/UTF-8
        CARRIAGE_RETURN = 13  # \r in ASCII/UTF-8
        SPACE = 32  # space in ASCII/UTF-8
        
        # Look for double newlines (paragraph breaks)
        for i in range(target_point, max(0, target_point - para_scan), -1):
            if i < data_len - 1 and data[i] == NEWLINE and data[i+1] == NEWLINE:
                return i + 2
        
        # Look for single newlines
        for i in range(target_point, max(0, target_point - line_scan), -1):
            if data[i] == NEWLINE:
                return i + 1
            
            # Also check for Windows-style line endings (\r\n)
            if i < data_len - 1 and data[i] == CARRIAGE_RETURN and data[i+1] == NEWLINE:
                return i + 2
        
        # Last resort: look for spaces
        for i in range(target_point, max(0, target_point - word_scan), -1):
            if data[i] == SPACE:
                return i + 1
        
        # Fallback to target point
        return target_point
    
    @contextmanager
    def _create_temp_file(self) -> Generator[BinaryIO, None, None]:
        """
        Create a temporary file and ensure it's cleaned up
        
        Yields:
            Binary file handle
        """
        temp_file = tempfile.NamedTemporaryFile(delete=False)
        self._temp_files.append(temp_file.name)
        
        try:
            yield temp_file
        finally:
            temp_file.close()
    
    def __del__(self):
        """Clean up any temporary files on deletion"""
        for temp_file in self._temp_files:
            try:
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
            except Exception as e:
                logger.warning(f"Failed to delete temporary file {temp_file}: {e}")


class ChunkProcessor:
    """
    Optimized processor for chunking large content
    """
    
    def __init__(self, options: ChunkingOptions):
        """
        Initialize the chunk processor
        
        Args:
            options: Chunking options
        """
        self.options = options
        self.buffer = StreamingBuffer(
            buffer_size=options.stream_buffer_size,
            overlap_size=self._get_overlap_size(options)
        )
    
    def process_large_file(
        self, 
        file_path: str, 
        chunker_func: callable,
        raw_bytes: bool = False
    ) -> Generator[Union[str, bytes], None, None]:
        """
        Process a large file with optimized memory usage
        
        Args:
            file_path: Path to file
            chunker_func: Function that chunks text or bytes
            raw_bytes: If True, process and yield raw bytes
            
        Yields:
            Processed text or bytes chunks
        """
        for buffer_chunk in self.buffer.stream_file(file_path, raw_bytes=raw_bytes):
            # Process this buffer chunk
            for result_chunk in chunker_func(buffer_chunk):
                yield result_chunk
    
    def process_large_text(
        self, 
        text: Union[str, bytes], 
        chunker_func: callable,
        raw_bytes: bool = False
    ) -> Generator[Union[str, bytes], None, None]:
        """
        Process large text with optimized memory usage
        
        Args:
            text: Large text or bytes to process
            chunker_func: Function that chunks text or bytes
            raw_bytes: If True, process and yield raw bytes
            
        Yields:
            Processed text or bytes chunks
        """
        for buffer_chunk in self.buffer.stream_string(text, raw_bytes=raw_bytes):
            # Process this buffer chunk
            for result_chunk in chunker_func(buffer_chunk):
                yield result_chunk
    
    def process_stream(
        self, 
        stream: Union[TextIO, BinaryIO], 
        chunker_func: callable,
        raw_bytes: bool = False
    ) -> Generator[Union[str, bytes], None, None]:
        """
        Process a text or binary stream
        
        Args:
            stream: Text or binary stream to process
            chunker_func: Function that chunks text or bytes
            raw_bytes: If True, process and yield raw bytes
            
        Yields:
            Processed text or bytes chunks
        """
        for buffer_chunk in self.buffer.stream_handle(stream, raw_bytes=raw_bytes):
            # Process this buffer chunk
            for result_chunk in chunker_func(buffer_chunk):
                yield result_chunk
    
    def _get_overlap_size(self, options: ChunkingOptions) -> int:
        """
        Calculate appropriate overlap size in characters
        
        Args:
            options: Chunking options
            
        Returns:
            Overlap size in characters
        """
        # Conservative character-to-token ratio (varies by language and content)
        chars_per_token = 4.0
        return int(options.overlap_tokens * chars_per_token * 1.2)  # Add 20% margin
</file>

<file path="utils/parallel_processing.py">
"""
Parallel processing utilities for multi-threaded chunking
"""

import os
import logging
import multiprocessing
from typing import List, Dict, Any, Optional, Generator, Callable, Tuple, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from queue import Queue, Empty
import threading
import time

from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.models.enums import ChunkingStrategy

# Configure logging
logger = logging.getLogger(__name__)


class ParallelChunker:
    """
    Parallel processing implementation for chunking large content across multiple threads/processes
    """
    
    def __init__(
        self, 
        options: ChunkingOptions,
        max_workers: Optional[int] = None,
        use_processes: bool = False
    ):
        """
        Initialize the parallel chunker
        
        Args:
            options: Chunking options
            max_workers: Maximum number of worker threads/processes (None = auto)
            use_processes: Whether to use processes instead of threads
        """
        self.options = options
        self.use_processes = use_processes
        
        # Determine optimal number of workers
        if max_workers is None:
            # Use CPU count for processes, more for threads
            if use_processes:
                self.max_workers = max(1, os.cpu_count() - 1)  # Leave one CPU free
            else:
                # For I/O bound operations, more threads can be beneficial
                self.max_workers = min(32, os.cpu_count() * 4)
        else:
            self.max_workers = max_workers
            
        # Create shared queue for results
        self.results_queue = Queue()
    
    def chunk_segments(
        self, 
        segments: List[str],
        chunker_func: Callable[[str], List[str]]
    ) -> List[str]:
        """
        Process multiple segments in parallel
        
        Args:
            segments: List of text segments to process
            chunker_func: Function that chunks a segment
            
        Returns:
            List of processed chunks in order
        """
        # Skip parallel processing for small inputs
        if len(segments) <= 1 or self.max_workers <= 1:
            results = []
            for segment in segments:
                results.extend(chunker_func(segment))
            return results
        
        # Choose executor type
        executor_class = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor
        
        # Use context manager to ensure cleanup
        with executor_class(max_workers=min(self.max_workers, len(segments))) as executor:
            # Submit all tasks
            future_to_index = {
                executor.submit(chunker_func, segment): i 
                for i, segment in enumerate(segments)
            }
            
            # Collect results in order
            ordered_results = [[] for _ in range(len(segments))]
            
            # Wait for tasks to complete
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    result = future.result()
                    ordered_results[index] = result
                except Exception as e:
                    logger.error(f"Error processing segment {index}: {e}")
                    # Add empty result for failed segment
                    ordered_results[index] = []
            
            # Flatten results in proper order
            all_results = []
            for segment_results in ordered_results:
                all_results.extend(segment_results)
                
            return all_results
    
    def stream_chunks(
        self, 
        segment_gen: Generator[str, None, None],
        chunker_func: Callable[[str], List[str]]
    ) -> Generator[str, None, None]:
        """
        Process a stream of segments with parallel workers
        
        Args:
            segment_gen: Generator that yields text segments
            chunker_func: Function that chunks a segment
            
        Yields:
            Processed chunks
        """
        # Start worker thread
        stop_event = threading.Event()
        worker_thread = threading.Thread(
            target=self._worker_thread,
            args=(segment_gen, chunker_func, stop_event)
        )
        worker_thread.daemon = True
        worker_thread.start()
        
        try:
            # Read results from queue
            done = False
            while not done:
                try:
                    item = self.results_queue.get(timeout=0.1)
                    
                    # Check for end marker
                    if item == "DONE":
                        done = True
                        # Mark sentinel as done so queue.join() won't hang
                        self.results_queue.task_done()
                    elif item == "ERROR":
                        # An error occurred
                        logger.error("Error in worker thread")
                        done = True
                        # Mark sentinel as done so queue.join() won't hang
                        self.results_queue.task_done()
                    else:
                        # Yield the result chunk
                        yield item
                        self.results_queue.task_done()
                except Empty:
                    # Queue is empty, check if worker is still alive
                    if not worker_thread.is_alive():
                        # Worker died, likely due to an exception
                        done = True
        finally:
            # Signal worker to stop
            stop_event.set()
            
            # Wait for worker to finish (with timeout)
            worker_thread.join(timeout=1.0)
    
    def _worker_thread(
        self, 
        segment_gen: Generator[str, None, None],
        chunker_func: Callable[[str], List[str]],
        stop_event: threading.Event
    ):
        """
        Worker thread that processes segments and puts results in queue
        
        Args:
            segment_gen: Generator that yields text segments
            chunker_func: Function that chunks a segment
            stop_event: Event to signal thread to stop
        """
        try:
            # Choose executor type
            executor_class = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor
            
            # Use context manager to ensure cleanup
            with executor_class(max_workers=self.max_workers) as executor:
                # Create a buffer for segments
                segments_buffer = []
                futures = []
                
                # Process segments in batches
                for segment in segment_gen:
                    # Check if we should stop
                    if stop_event.is_set():
                        break
                        
                    # Add segment to buffer
                    segments_buffer.append(segment)
                    
                    # Process buffer when it's large enough
                    if len(segments_buffer) >= self.max_workers:
                        # Submit tasks for all segments in buffer
                        for seg in segments_buffer:
                            futures.append(executor.submit(chunker_func, seg))
                            
                        # Reset buffer
                        segments_buffer = []
                        
                        # Process completed futures
                        self._process_futures(futures)
                        futures = []
                
                # Process any remaining segments in buffer
                if segments_buffer and not stop_event.is_set():
                    for seg in segments_buffer:
                        futures.append(executor.submit(chunker_func, seg))
                        
                # Wait for all remaining futures
                self._process_futures(futures)
                
            # Signal completion
            self.results_queue.put("DONE")
                
        except Exception as e:
            logger.error(f"Error in worker thread: {e}")
            # Signal error
            self.results_queue.put("ERROR")
    
    def _process_futures(self, futures: List):
        """
        Process a list of futures and put results in queue
        
        Args:
            futures: List of futures to process
        """
        # Process results as they complete
        for future in as_completed(futures):
            try:
                result = future.result()
                
                # Put each chunk in the results queue
                for chunk in result:
                    self.results_queue.put(chunk)
            except Exception as e:
                logger.error(f"Error processing future: {e}")


class AdaptiveParallelChunker:
    """
    Adaptive parallel chunking with runtime performance monitoring
    """
    
    def __init__(self, options: ChunkingOptions):
        """
        Initialize the adaptive parallel chunker
        
        Args:
            options: Chunking options
        """
        self.options = options
        self.max_workers = os.cpu_count() or 4
        
        # Performance metrics
        self.segment_processing_times = []
        self.worker_utilization = []
    
    def chunk_in_parallel(
        self, 
        text: str,
        chunker_func: Callable[[str], List[str]]
    ) -> List[str]:
        """
        Adaptively chunk text with parallel processing
        
        Args:
            text: Text to chunk
            chunker_func: Function that chunks text
            
        Returns:
            List of processed chunks
        """
        # Skip parallel processing for small inputs
        if len(text) < 50000:  # ~50KB threshold
            return chunker_func(text)
        
        # Split text into segments
        segments = self._split_into_segments(text)
        
        # Process segments with adaptive workers
        start_time = time.time()
        
        # Start with a conservative number of workers
        current_workers = max(2, self.max_workers // 2)
        
        # Create chunker with current workers
        parallel_chunker = ParallelChunker(
            self.options,
            max_workers=current_workers,
            use_processes=False  # Start with threads
        )
        
        # Process first batch to gather metrics
        first_batch = segments[:min(len(segments), current_workers)]
        batch_start = time.time()
        first_results = parallel_chunker.chunk_segments(first_batch, chunker_func)
        batch_time = time.time() - batch_start
        
        # Calculate metrics
        avg_segment_time = batch_time / len(first_batch)
        self.segment_processing_times.append(avg_segment_time)
        
        # Adjust workers based on first batch performance
        if avg_segment_time < 0.1:  # Very fast processing
            # CPU-bound, increase workers to max
            adjusted_workers = min(self.max_workers, len(segments))
        elif avg_segment_time > 1.0:  # Slow processing
            # I/O-bound or complex processing, use more workers
            adjusted_workers = min(self.max_workers * 2, len(segments))
        else:
            # Balanced, keep current workers
            adjusted_workers = current_workers
            
        logger.debug(f"Adjusted workers from {current_workers} to {adjusted_workers} based on performance")
        
        # Process remaining segments with adjusted workers
        remaining_segments = segments[len(first_batch):]
        
        if remaining_segments:
            # Create new chunker with adjusted workers
            parallel_chunker = ParallelChunker(
                self.options,
                max_workers=adjusted_workers,
                use_processes=avg_segment_time > 0.5  # Use processes for longer-running tasks
            )
            
            # Process remaining segments
            remaining_results = parallel_chunker.chunk_segments(remaining_segments, chunker_func)
            
            # Combine results
            all_results = first_results + remaining_results
        else:
            all_results = first_results
            
        # Record metrics
        total_time = time.time() - start_time
        logger.debug(f"Processed {len(segments)} segments in {total_time:.2f}s using adaptive parallel chunking")
        
        return all_results
    
    def _split_into_segments(self, text: str) -> List[str]:
        """
        Split text into segments for parallel processing
        
        Args:
            text: Text to split
            
        Returns:
            List of text segments
        """
        # Determine appropriate segment size
        text_length = len(text)
        target_segments = min(self.max_workers * 2, max(2, text_length // 50000))
        target_size = text_length // target_segments
        
        segments = []
        current_pos = 0
        
        while current_pos < text_length:
            # Calculate end position
            end_pos = min(current_pos + target_size, text_length)
            
            # Extend to find a natural boundary
            if end_pos < text_length:
                # Look for paragraph break
                para_pos = text.find('\n\n', end_pos - 100, end_pos + 100)
                if para_pos != -1:
                    end_pos = para_pos + 2
                else:
                    # Look for line break
                    line_pos = text.find('\n', end_pos - 50, end_pos + 50)
                    if line_pos != -1:
                        end_pos = line_pos + 1
                    else:
                        # Look for sentence boundary
                        sentence_end = max(
                            text.rfind('. ', end_pos - 100, end_pos),
                            text.rfind('! ', end_pos - 100, end_pos),
                            text.rfind('? ', end_pos - 100, end_pos)
                        )
                        if sentence_end != -1:
                            end_pos = sentence_end + 2
                        else:
                            # Fall back to word boundary
                            space_pos = text.find(' ', end_pos - 20, end_pos + 20)
                            if space_pos != -1:
                                end_pos = space_pos + 1
            
            # Add segment
            segments.append(text[current_pos:end_pos])
            current_pos = end_pos
            
        return segments
</file>

<file path="utils/performance.py">
"""
Performance monitoring and benchmarking utilities

This module provides comprehensive performance measurement, profiling, and optimization
tools for evaluating and improving chunking operations across various workloads.
"""

import time
import logging
import gc
import tracemalloc
import os
import sys
import json
import statistics
from typing import Dict, Any, List, Optional, Callable, Tuple, Union, Generator
from contextlib import contextmanager
from functools import wraps, lru_cache
import platform
import threading
import tempfile
from datetime import datetime

# Import optional dependencies with fallbacks
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

try:
    import matplotlib.pyplot as plt
    import numpy as np
    HAS_PLOTTING = True
except ImportError:
    HAS_PLOTTING = False

from enterprise_chunker.models.enums import ChunkingStrategy, ContentFormat
from enterprise_chunker.config import ChunkingOptions
from enterprise_chunker.utils.memory_optimization import MemoryManager

# Configure logging
logger = logging.getLogger(__name__)


class PerformanceMetrics:
    """Container for performance measurement data"""
    
    def __init__(self):
        """Initialize performance metrics"""
        self.execution_times = {}
        self.memory_usage = {}
        self.throughput = {}
        self.cpu_usage = {}
        self.chunk_statistics = {}
        self.start_time = time.time()
        self.end_time = None
        self.total_content_size = 0
        self.total_chunks = 0
        
    def record_execution_time(self, operation: str, duration: float):
        """
        Record execution time for an operation
        
        Args:
            operation: Name of the operation
            duration: Duration in seconds
        """
        if operation not in self.execution_times:
            self.execution_times[operation] = []
        self.execution_times[operation].append(duration)
    
    def record_memory_usage(self, operation: str, usage_mb: float):
        """
        Record memory usage for an operation
        
        Args:
            operation: Name of the operation
            usage_mb: Memory usage in megabytes
        """
        if operation not in self.memory_usage:
            self.memory_usage[operation] = []
        self.memory_usage[operation].append(usage_mb)
    
    def record_throughput(self, operation: str, bytes_per_second: float):
        """
        Record throughput for an operation
        
        Args:
            operation: Name of the operation
            bytes_per_second: Throughput in bytes per second
        """
        if operation not in self.throughput:
            self.throughput[operation] = []
        self.throughput[operation].append(bytes_per_second)
    
    def record_cpu_usage(self, operation: str, percent: float):
        """
        Record CPU usage for an operation
        
        Args:
            operation: Name of the operation
            percent: CPU usage percentage
        """
        if operation not in self.cpu_usage:
            self.cpu_usage[operation] = []
        self.cpu_usage[operation].append(percent)
    
    def record_chunk_statistics(self, chunk_sizes: List[int], chunk_count: int):
        """
        Record statistics about chunks
        
        Args:
            chunk_sizes: List of chunk sizes in tokens or characters
            chunk_count: Total number of chunks
        """
        if chunk_sizes:
            self.chunk_statistics = {
                'count': chunk_count,
                'min_size': min(chunk_sizes),
                'max_size': max(chunk_sizes),
                'avg_size': statistics.mean(chunk_sizes),
                'median_size': statistics.median(chunk_sizes),
                'stdev_size': statistics.stdev(chunk_sizes) if len(chunk_sizes) > 1 else 0,
                'total_tokens': sum(chunk_sizes)
            }
        else:
            self.chunk_statistics = {
                'count': 0,
                'min_size': 0,
                'max_size': 0,
                'avg_size': 0,
                'median_size': 0,
                'stdev_size': 0,
                'total_tokens': 0
            }
        
        self.total_chunks = chunk_count
    
    def finalize(self):
        """Finalize metrics collection"""
        self.end_time = time.time()
    
    def get_summary(self) -> Dict[str, Any]:
        """
        Get summary of performance metrics
        
        Returns:
            Dictionary with summarized metrics
        """
        if not self.end_time:
            self.finalize()
            
        result = {
            'total_time': self.end_time - self.start_time,
            'operations': {},
            'chunk_statistics': self.chunk_statistics
        }
        
        # Process operation-specific metrics
        for operation, times in self.execution_times.items():
            result['operations'][operation] = {
                'execution_time': {
                    'avg': statistics.mean(times),
                    'min': min(times),
                    'max': max(times),
                    'total': sum(times)
                }
            }
            
        # Add memory metrics
        for operation, usages in self.memory_usage.items():
            if operation not in result['operations']:
                result['operations'][operation] = {}
                
            result['operations'][operation]['memory_usage'] = {
                'avg': statistics.mean(usages),
                'min': min(usages),
                'max': max(usages),
                'peak': max(usages)
            }
            
        # Add throughput metrics
        for operation, throughputs in self.throughput.items():
            if operation not in result['operations']:
                result['operations'][operation] = {}
                
            result['operations'][operation]['throughput'] = {
                'avg_bytes_per_second': statistics.mean(throughputs),
                'peak_bytes_per_second': max(throughputs)
            }
            
        # Add CPU metrics
        for operation, cpu_usages in self.cpu_usage.items():
            if operation not in result['operations']:
                result['operations'][operation] = {}
                
            result['operations'][operation]['cpu_usage'] = {
                'avg_percent': statistics.mean(cpu_usages),
                'peak_percent': max(cpu_usages)
            }
            
        # Add overall processing rates
        if self.total_content_size > 0 and self.total_chunks > 0:
            overall_time = self.end_time - self.start_time
            result['overall'] = {
                'bytes_per_second': self.total_content_size / overall_time,
                'chunks_per_second': self.total_chunks / overall_time,
                'avg_bytes_per_chunk': self.total_content_size / self.total_chunks
            }
            
        return result
    
    def get_detailed_report(self) -> str:
        """
        Generate a detailed text report
        
        Returns:
            Formatted text report
        """
        summary = self.get_summary()
        
        report = []
        report.append("==== Performance Report ====")
        report.append(f"Total execution time: {summary['total_time']:.2f} seconds")
        
        if 'overall' in summary:
            report.append("\n== Overall Processing ==")
            report.append(f"Processing rate: {summary['overall']['bytes_per_second'] / 1024 / 1024:.2f} MB/sec")
            report.append(f"Chunk generation rate: {summary['overall']['chunks_per_second']:.2f} chunks/sec")
            report.append(f"Average chunk size: {summary['overall']['avg_bytes_per_chunk']:.2f} bytes")
        
        report.append("\n== Operation Details ==")
        for operation, metrics in summary['operations'].items():
            report.append(f"\nOperation: {operation}")
            
            if 'execution_time' in metrics:
                report.append("  Execution Time:")
                report.append(f"    Avg: {metrics['execution_time']['avg']:.4f} sec")
                report.append(f"    Min: {metrics['execution_time']['min']:.4f} sec")
                report.append(f"    Max: {metrics['execution_time']['max']:.4f} sec")
                report.append(f"    Total: {metrics['execution_time']['total']:.4f} sec")
                
            if 'memory_usage' in metrics:
                report.append("  Memory Usage:")
                report.append(f"    Avg: {metrics['memory_usage']['avg']:.2f} MB")
                report.append(f"    Peak: {metrics['memory_usage']['peak']:.2f} MB")
                
            if 'throughput' in metrics:
                report.append("  Throughput:")
                report.append(f"    Avg: {metrics['throughput']['avg_bytes_per_second'] / 1024 / 1024:.2f} MB/sec")
                report.append(f"    Peak: {metrics['throughput']['peak_bytes_per_second'] / 1024 / 1024:.2f} MB/sec")
                
            if 'cpu_usage' in metrics:
                report.append("  CPU Usage:")
                report.append(f"    Avg: {metrics['cpu_usage']['avg_percent']:.2f}%")
                report.append(f"    Peak: {metrics['cpu_usage']['peak_percent']:.2f}%")
        
        if 'chunk_statistics' in summary and summary['chunk_statistics']:
            report.append("\n== Chunk Statistics ==")
            chunk_stats = summary['chunk_statistics']
            report.append(f"Total chunks: {chunk_stats['count']}")
            report.append(f"Min chunk size: {chunk_stats['min_size']} tokens")
            report.append(f"Max chunk size: {chunk_stats['max_size']} tokens")
            report.append(f"Avg chunk size: {chunk_stats['avg_size']:.2f} tokens")
            report.append(f"Median chunk size: {chunk_stats['median_size']} tokens")
            report.append(f"Std dev of chunk sizes: {chunk_stats['stdev_size']:.2f} tokens")
            report.append(f"Total tokens: {chunk_stats['total_tokens']} tokens")
        
        return "\n".join(report)
    
    def export_to_json(self, file_path: str):
        """
        Export metrics to a JSON file
        
        Args:
            file_path: Path to save the JSON file
        """
        with open(file_path, 'w') as f:
            json.dump(self.get_summary(), f, indent=2)
    
    def generate_visualizations(self, output_dir: Optional[str] = None):
        """
        Generate performance visualization charts
        
        Args:
            output_dir: Directory to save visualization files
        """
        if not HAS_PLOTTING:
            logger.warning("Matplotlib and numpy are required for visualizations. Skipping.")
            return
            
        if not output_dir:
            output_dir = tempfile.mkdtemp(prefix="chunker_perf_")
        os.makedirs(output_dir, exist_ok=True)
        
        # Generate execution time chart
        if self.execution_times:
            plt.figure(figsize=(10, 6))
            operations = list(self.execution_times.keys())
            avg_times = [statistics.mean(self.execution_times[op]) for op in operations]
            
            plt.bar(operations, avg_times)
            plt.title('Average Execution Time by Operation')
            plt.xlabel('Operation')
            plt.ylabel('Time (seconds)')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'execution_times.png'))
            plt.close()
        
        # Generate memory usage chart
        if self.memory_usage:
            plt.figure(figsize=(10, 6))
            operations = list(self.memory_usage.keys())
            peak_memory = [max(self.memory_usage[op]) for op in operations]
            
            plt.bar(operations, peak_memory)
            plt.title('Peak Memory Usage by Operation')
            plt.xlabel('Operation')
            plt.ylabel('Memory Usage (MB)')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'memory_usage.png'))
            plt.close()
            
        # Generate chunk size distribution
        if self.chunk_statistics and self.chunk_statistics.get('count', 0) > 0:
            # We need actual chunk sizes for histogram, not just statistics
            # This is a placeholder - actual implementation would need to capture the raw sizes
            logger.info(f"Visualizations saved to {output_dir}")


class PerformanceMonitor:
    """Real-time performance monitoring for chunking operations"""
    
    def __init__(self, interval: float = 1.0):
        """
        Initialize performance monitor
        
        Args:
            interval: Monitoring interval in seconds
        """
        self.interval = interval
        self.metrics = PerformanceMetrics()
        self.memory_manager = MemoryManager()
        self._monitoring_thread = None
        self._stop_event = threading.Event()
        self._cpu_percent = 0
        self._memory_mb = 0
        
        # Initialize CPU and memory monitoring if psutil is available
        self.has_monitoring = HAS_PSUTIL
        if self.has_monitoring:
            self.process = psutil.Process(os.getpid())
    
    def start_monitoring(self):
        """Start background monitoring thread"""
        if not self.has_monitoring:
            logger.warning("psutil is required for real-time monitoring. Running with limited metrics.")
            return
            
        self._stop_event.clear()
        self._monitoring_thread = threading.Thread(target=self._monitor_resources)
        self._monitoring_thread.daemon = True
        self._monitoring_thread.start()
        logger.debug("Started performance monitoring thread")
    
    def stop_monitoring(self):
        """Stop background monitoring thread"""
        if self._monitoring_thread and self._monitoring_thread.is_alive():
            self._stop_event.set()
            self._monitoring_thread.join(timeout=self.interval * 2)
            logger.debug("Stopped performance monitoring thread")
    
    def _monitor_resources(self):
        """Background monitoring function"""
        while not self._stop_event.is_set():
            try:
                # Get current CPU and memory usage
                self._cpu_percent = self.process.cpu_percent(interval=None)
                memory_info = self.process.memory_info()
                self._memory_mb = memory_info.rss / 1024 / 1024
                
                # Record metrics
                self.metrics.record_cpu_usage("process", self._cpu_percent)
                self.metrics.record_memory_usage("process", self._memory_mb)
                
            except Exception as e:
                logger.error(f"Error in monitoring thread: {e}")
                
            # Sleep for interval duration
            time.sleep(self.interval)
    
    def get_current_usage(self) -> Dict[str, float]:
        """
        Get current resource usage
        
        Returns:
            Dictionary with current CPU and memory usage
        """
        if self.has_monitoring:
            # Return latest monitored values
            return {
                "cpu_percent": self._cpu_percent,
                "memory_mb": self._memory_mb
            }
        else:
            # Fallback for systems without psutil
            return {
                "cpu_percent": 0,
                "memory_mb": self.memory_manager.get_memory_usage()
            }
    
    @contextmanager
    def measure_operation(self, operation_name: str, content_size: Optional[int] = None):
        """
        Context manager for measuring performance of an operation
        
        Args:
            operation_name: Name of the operation
            content_size: Size of content being processed (for throughput calculation)
            
        Yields:
            Context for the operation
        """
        # Record starting metrics
        start_time = time.time()
        start_memory = self.memory_manager.get_memory_usage()
        
        try:
            # Yield control back to the caller
            yield
        finally:
            # Record ending metrics
            end_time = time.time()
            end_memory = self.memory_manager.get_memory_usage()
            duration = end_time - start_time
            
            # Record execution time
            self.metrics.record_execution_time(operation_name, duration)
            
            # Record memory usage
            memory_increase = max(0, end_memory - start_memory)
            self.metrics.record_memory_usage(operation_name, memory_increase)
            
            # Calculate and record throughput if content size is known
            if content_size:
                throughput = content_size / max(duration, 0.001)  # Avoid division by zero
                self.metrics.record_throughput(operation_name, throughput)
                
            # Get current CPU usage
            usage = self.get_current_usage()
            self.metrics.record_cpu_usage(operation_name, usage["cpu_percent"])
            
            logger.debug(
                f"Operation '{operation_name}' completed in {duration:.4f}s "
                f"(Memory: {memory_increase:.2f}MB, CPU: {usage['cpu_percent']:.1f}%)"
            )
    
    def record_chunk_metadata(self, chunks, content_size: int):
        """
        Record metadata about chunks for analysis
        
        Args:
            chunks: List of chunks or chunk metadata
            content_size: Size of original content
        """
        # Extract sizes if available or use length as fallback
        chunk_sizes = []
        for chunk in chunks:
            if isinstance(chunk, dict) and 'token_count' in chunk:
                chunk_sizes.append(chunk['token_count'])
            elif isinstance(chunk, dict) and 'size' in chunk:
                chunk_sizes.append(chunk['size'])
            elif hasattr(chunk, 'token_count'):
                chunk_sizes.append(chunk.token_count)
            elif hasattr(chunk, 'size'):
                chunk_sizes.append(chunk.size)
            elif isinstance(chunk, str):
                # Estimate token count as character count / 4 as a rough approximation
                chunk_sizes.append(len(chunk) // 4)
            else:
                # Fallback to estimating from string representation
                chunk_sizes.append(len(str(chunk)) // 4)
        
        # Record chunk statistics
        self.metrics.record_chunk_statistics(chunk_sizes, len(chunks))
        
        # Update total content size
        self.metrics.total_content_size += content_size


def timing_decorator(func=None, log_level='debug', logger_name=None):
    """
    Simple decorator for timing function execution
    
    Args:
        func: The function to decorate (used when decorator is called without arguments)
        log_level: Logging level to use ('debug', 'info', 'warning', 'error')
        logger_name: Name of logger to use (defaults to module logger)
    
    Returns:
        Decorated function
    """
    # Handle the case when decorator is called without arguments
    if func is not None:
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Set up logging
            log = logger
                
            # Get logging function based on level
            log_fn = getattr(log, log_level.lower())
            
            # Start timing
            start_time = time.time()
            
            # Execute function
            result = func(*args, **kwargs)
            
            # Calculate duration
            duration = time.time() - start_time
            
            # Log the timing
            log_fn(f"{func.__name__} executed in {duration:.4f} seconds")
            
            return result
        return wrapper
    
    # Regular case - decorator called with arguments
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Set up logging
            if logger_name:
                log = logging.getLogger(logger_name)
            else:
                log = logger
                
            # Get logging function based on level
            log_fn = getattr(log, log_level.lower())
            
            # Start timing
            start_time = time.time()
            
            # Execute function
            result = func(*args, **kwargs)
            
            # Calculate duration
            duration = time.time() - start_time
            
            # Log the timing
            log_fn(f"{func.__name__} executed in {duration:.4f} seconds")
            
            return result
        return wrapper
    return decorator


def profile_function(operation_name: Optional[str] = None):
    """
    Decorator for profiling function performance
    
    Args:
        operation_name: Name of the operation (defaults to function name)
    
    Returns:
        Decorated function
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Determine operation name
            nonlocal operation_name
            if operation_name is None:
                operation_name = func.__name__
                
            # Set up monitoring
            monitor = PerformanceMonitor()
            
            # Determine content size if possible from args/kwargs
            content_size = None
            for arg in args:
                if isinstance(arg, str):
                    content_size = len(arg.encode('utf-8'))
                    break
                elif isinstance(arg, bytes):
                    content_size = len(arg)
                    break
            
            # Measure the operation
            with monitor.measure_operation(operation_name, content_size):
                result = func(*args, **kwargs)
                
            return result
        return wrapper
    return decorator


class BenchmarkRunner:
    """Benchmarking utility for comparing different chunking strategies"""
    
    def __init__(self, output_dir: Optional[str] = None):
        """
        Initialize benchmark runner
        
        Args:
            output_dir: Directory to store benchmark results
        """
        self.output_dir = output_dir or tempfile.mkdtemp(prefix="chunker_benchmark_")
        os.makedirs(self.output_dir, exist_ok=True)
        self.results = {}
        self.monitors = {}
    
    def benchmark_strategy(
        self,
        strategy_name: str,
        chunker_func: Callable,
        test_data: Union[str, bytes, List[str]],
        content_format: Optional[ContentFormat] = None,
        iterations: int = 3,
        warmup_iterations: int = 1
    ):
        """
        Benchmark a specific chunking strategy
        
        Args:
            strategy_name: Name of the strategy being benchmarked
            chunker_func: Function that implements the chunking strategy
            test_data: Data to use for benchmarking
            content_format: Format of the content (for logging)
            iterations: Number of test iterations
            warmup_iterations: Number of warmup iterations
        """
        logger.info(f"Benchmarking strategy: {strategy_name}")
        
        # Prepare test data
        if isinstance(test_data, list):
            test_content = "".join(test_data)
            content_size = len(test_content.encode('utf-8'))
        elif isinstance(test_data, str):
            test_content = test_data
            content_size = len(test_content.encode('utf-8'))
        else:  # bytes
            test_content = test_data
            content_size = len(test_content)
            
        # Initial warmup runs
        logger.debug(f"Performing {warmup_iterations} warmup iterations")
        for i in range(warmup_iterations):
            chunker_func(test_content)
            # Clear memory after warmup
            gc.collect()
        
        # Create performance monitor
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        try:
            # Run benchmark iterations
            logger.info(f"Running {iterations} benchmark iterations")
            for i in range(iterations):
                logger.debug(f"Iteration {i+1}/{iterations}")
                
                # Measure chunking performance
                with monitor.measure_operation(f"{strategy_name}", content_size):
                    chunks = chunker_func(test_content)
                    
                # Record chunk metadata
                monitor.record_chunk_metadata(chunks, content_size)
                
                # Clear memory between iterations
                gc.collect()
        finally:
            # Stop monitoring
            monitor.stop_monitoring()
        
        # Store results
        self.monitors[strategy_name] = monitor
        self.results[strategy_name] = monitor.metrics.get_summary()
        
        # Log summary
        logger.info(f"Strategy '{strategy_name}' - "
                    f"Avg time: {self.results[strategy_name]['operations'][strategy_name]['execution_time']['avg']:.4f}s, "
                    f"Chunks: {monitor.metrics.chunk_statistics['count']}")
    
    def compare_results(self) -> Dict[str, Any]:
        """
        Compare benchmark results across strategies
        
        Returns:
            Dictionary with comparison metrics
        """
        if not self.results:
            logger.warning("No benchmark results to compare")
            return {}
            
        # Get baseline strategy (first one) for normalization
        baseline = next(iter(self.results.keys()))
        baseline_metrics = self.results[baseline]
        
        comparison = {
            'timestamp': datetime.now().isoformat(),
            'baseline': baseline,
            'strategies': {},
            'fastest': None,
            'lowest_memory': None,
            'highest_throughput': None
        }
        
        # Find fastest strategy
        fastest_time = float('inf')
        lowest_memory = float('inf')
        highest_throughput = 0
        
        for strategy, metrics in self.results.items():
            # Get operation metrics for the main strategy operation
            op_metrics = metrics['operations'][strategy]
            avg_time = op_metrics['execution_time']['avg']
            
            # Find fastest
            if avg_time < fastest_time:
                fastest_time = avg_time
                comparison['fastest'] = strategy
                
            # Find lowest memory if available
            if 'memory_usage' in op_metrics:
                memory_usage = op_metrics['memory_usage']['peak']
                if memory_usage < lowest_memory:
                    lowest_memory = memory_usage
                    comparison['lowest_memory'] = strategy
                    
            # Find highest throughput if available
            if 'throughput' in op_metrics:
                throughput = op_metrics['throughput']['avg_bytes_per_second']
                if throughput > highest_throughput:
                    highest_throughput = throughput
                    comparison['highest_throughput'] = strategy
            
            # Calculate comparison metrics
            comparison['strategies'][strategy] = {
                'execution_time': op_metrics['execution_time']['avg'],
                'chunks': self.monitors[strategy].metrics.chunk_statistics['count'],
                'relative_speed': baseline_metrics['operations'][baseline]['execution_time']['avg'] / avg_time
            }
            
            # Add memory usage if available
            if 'memory_usage' in op_metrics:
                comparison['strategies'][strategy]['peak_memory'] = op_metrics['memory_usage']['peak']
                
            # Add throughput if available
            if 'throughput' in op_metrics:
                comparison['strategies'][strategy]['throughput'] = op_metrics['throughput']['avg_bytes_per_second']
            
        return comparison
    
    def generate_report(self) -> str:
        """
        Generate a comprehensive benchmark report
        
        Returns:
            Benchmark report as formatted text
        """
        if not self.results:
            return "No benchmark results available"
            
        comparison = self.compare_results()
        
        report = []
        report.append("===== Chunking Strategy Benchmark Report =====")
        report.append(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"System: {platform.system()} {platform.release()} {platform.machine()}")
        report.append(f"Python: {platform.python_version()}")
        report.append("\n=== Performance Comparison ===")
        
        # Create comparison table
        headers = ["Strategy", "Execution Time (s)", "Chunks", "Relative Speed", "Peak Memory (MB)", "Throughput (MB/s)"]
        report.append("  ".join(headers))
        report.append("-" * 80)
        
        for strategy, metrics in comparison['strategies'].items():
            row = [
                strategy,
                f"{metrics['execution_time']:.4f}",
                f"{metrics['chunks']}",
                f"{metrics['relative_speed']:.2f}x"
            ]
            
            # Add memory if available
            if 'peak_memory' in metrics:
                row.append(f"{metrics['peak_memory']:.2f}")
            else:
                row.append("N/A")
                
            # Add throughput if available
            if 'throughput' in metrics:
                row.append(f"{metrics['throughput'] / 1024 / 1024:.2f}")
            else:
                row.append("N/A")
                
            report.append("  ".join(row))
            
        # Add summary
        report.append("\n=== Summary ===")
        report.append(f"Fastest strategy: {comparison['fastest']}")
        
        if comparison['lowest_memory']:
            report.append(f"Lowest memory usage: {comparison['lowest_memory']}")
            
        if comparison['highest_throughput']:
            report.append(f"Highest throughput: {comparison['highest_throughput']}")
            
        # Add detailed reports for each strategy
        report.append("\n=== Detailed Strategy Reports ===")
        for strategy, monitor in self.monitors.items():
            report.append(f"\n--- {strategy} ---")
            report.append(monitor.metrics.get_detailed_report())
            
        return "\n".join(report)
    
    def save_results(self, filename_prefix: str = "benchmark"):
        """
        Save benchmark results to disk
        
        Args:
            filename_prefix: Prefix for output files
        """
        # Create timestamp string for filenames
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save comparison report
        comparison = self.compare_results()
        comparison_path = os.path.join(self.output_dir, f"{filename_prefix}_comparison_{timestamp}.json")
        with open(comparison_path, "w") as f:
            json.dump(comparison, f, indent=2)
            
        # Save full report
        report = self.generate_report()
        report_path = os.path.join(self.output_dir, f"{filename_prefix}_report_{timestamp}.txt")
        with open(report_path, "w") as f:
            f.write(report)
            
        # Save individual strategy results
        for strategy, metrics in self.results.items():
            strategy_path = os.path.join(
                self.output_dir, f"{filename_prefix}_{strategy.lower()}_{timestamp}.json"
            )
            with open(strategy_path, "w") as f:
                json.dump(metrics, f, indent=2)
                
        # Generate visualizations if matplotlib is available
        if HAS_PLOTTING:
            viz_dir = os.path.join(self.output_dir, f"{filename_prefix}_visualizations_{timestamp}")
            os.makedirs(viz_dir, exist_ok=True)
            
            # Create strategy comparison charts
            self._generate_comparison_charts(comparison, viz_dir)
            
            # Create individual strategy visualizations
            for strategy, monitor in self.monitors.items():
                strategy_viz_dir = os.path.join(viz_dir, strategy.lower())
                os.makedirs(strategy_viz_dir, exist_ok=True)
                monitor.metrics.generate_visualizations(strategy_viz_dir)
                
        logger.info(f"Benchmark results saved to {self.output_dir}")
        return {
            'comparison_path': comparison_path,
            'report_path': report_path,
            'output_dir': self.output_dir
        }
    
    def _generate_comparison_charts(self, comparison: Dict[str, Any], output_dir: str):
        """
        Generate comparison charts between strategies
        
        Args:
            comparison: Comparison data dictionary
            output_dir: Directory to save charts
        """
        if not HAS_PLOTTING:
            return
            
        strategies = list(comparison['strategies'].keys())
        
        # Execution time comparison
        plt.figure(figsize=(10, 6))
        times = [comparison['strategies'][s]['execution_time'] for s in strategies]
        plt.bar(strategies, times)
        plt.title('Execution Time by Strategy')
        plt.xlabel('Strategy')
        plt.ylabel('Time (seconds)')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'execution_time_comparison.png'))
        plt.close()
        
        # Relative speed comparison
        plt.figure(figsize=(10, 6))
        speeds = [comparison['strategies'][s]['relative_speed'] for s in strategies]
        plt.bar(strategies, speeds)
        plt.title('Relative Speed by Strategy')
        plt.xlabel('Strategy')
        plt.ylabel('Speed (relative to baseline)')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'relative_speed_comparison.png'))
        plt.close()
        
        # Memory usage comparison if available
        if all('peak_memory' in comparison['strategies'][s] for s in strategies):
            plt.figure(figsize=(10, 6))
            memory = [comparison['strategies'][s]['peak_memory'] for s in strategies]
            plt.bar(strategies, memory)
            plt.title('Peak Memory Usage by Strategy')
            plt.xlabel('Strategy')
            plt.ylabel('Memory (MB)')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'memory_usage_comparison.png'))
            plt.close()
            
        # Throughput comparison if available
        if all('throughput' in comparison['strategies'][s] for s in strategies):
            plt.figure(figsize=(10, 6))
            throughput = [comparison['strategies'][s]['throughput'] / 1024 / 1024 for s in strategies]
            plt.bar(strategies, throughput)
            plt.title('Throughput by Strategy')
            plt.xlabel('Strategy')
            plt.ylabel('Throughput (MB/s)')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'throughput_comparison.png'))
            plt.close()


class PerformanceOptimizer:
    """
    Optimizer for finding ideal chunking parameters for performance
    """
    
    def __init__(self, base_options: ChunkingOptions):
        """
        Initialize performance optimizer
        
        Args:
            base_options: Base chunking options to optimize
        """
        self.base_options = base_options
        self.results = {}
        self.best_params = None
        self.best_score = None
    
    def optimize_chunk_size(
        self,
        chunker_factory: Callable[[ChunkingOptions], Callable],
        test_data: Union[str, bytes],
        size_range: Tuple[int, int, int] = (100, 1000, 100),
        iterations: int = 2
    ):
        """
        Find optimal chunk size for performance
        
        Args:
            chunker_factory: Function that creates a chunker from options
            test_data: Test data to use for optimization
            size_range: Tuple of (min_size, max_size, step) for chunk sizes
            iterations: Number of iterations per size
        """
        logger.info(f"Optimizing chunk size from {size_range[0]} to {size_range[1]} by {size_range[2]}")
        
        # Run benchmark for each chunk size
        sizes = range(size_range[0], size_range[1] + 1, size_range[2])
        benchmark = BenchmarkRunner()
        
        for chunk_size in sizes:
            # Create options with this chunk size
            options = self._create_options_with_size(chunk_size)
            
            # Create chunker function
            chunker = chunker_factory(options)
            
            # Benchmark this configuration
            strategy_name = f"chunk_size_{chunk_size}"
            benchmark.benchmark_strategy(
                strategy_name=strategy_name,
                chunker_func=chunker,
                test_data=test_data,
                iterations=iterations
            )
        
        # Compare results
        comparison = benchmark.compare_results()
        
        # Find best parameters
        self._find_best_params(comparison)
        
        # Save detailed results
        self.results = benchmark.results
        
        return {
            'best_size': self.best_params,
            'comparison': comparison,
            'detailed_report': benchmark.generate_report()
        }
    
    def optimize_overlap(
        self,
        chunker_factory: Callable[[ChunkingOptions], Callable],
        test_data: Union[str, bytes],
        overlap_range: Tuple[int, int, int] = (0, 200, 20),
        iterations: int = 2
    ):
        """
        Find optimal overlap size for performance
        
        Args:
            chunker_factory: Function that creates a chunker from options
            test_data: Test data to use for optimization
            overlap_range: Tuple of (min_overlap, max_overlap, step) for overlap
            iterations: Number of iterations per overlap value
        """
        logger.info(f"Optimizing overlap from {overlap_range[0]} to {overlap_range[1]} by {overlap_range[2]}")
        
        # Run benchmark for each overlap
        overlaps = range(overlap_range[0], overlap_range[1] + 1, overlap_range[2])
        benchmark = BenchmarkRunner()
        
        for overlap in overlaps:
            # Create options with this overlap
            options = self._create_options_with_overlap(overlap)
            
            # Create chunker function
            chunker = chunker_factory(options)
            
            # Benchmark this configuration
            strategy_name = f"overlap_{overlap}"
            benchmark.benchmark_strategy(
                strategy_name=strategy_name,
                chunker_func=chunker,
                test_data=test_data,
                iterations=iterations
            )
        
        # Compare results
        comparison = benchmark.compare_results()
        
        # Find best parameters
        self._find_best_params(comparison)
        
        # Save detailed results
        self.results = benchmark.results
        
        return {
            'best_overlap': self.best_params,
            'comparison': comparison,
            'detailed_report': benchmark.generate_report()
        }
    
    def optimize_strategy(
        self,
        chunker_factories: Dict[ChunkingStrategy, Callable[[ChunkingOptions], Callable]],
        test_data: Union[str, bytes],
        iterations: int = 3
    ):
        """
        Find optimal chunking strategy for performance
        
        Args:
            chunker_factories: Dictionary of strategy to chunker factory functions
            test_data: Test data to use for optimization
            iterations: Number of iterations per strategy
        """
        logger.info(f"Optimizing chunking strategy across {len(chunker_factories)} strategies")
        
        # Run benchmark for each strategy
        benchmark = BenchmarkRunner()
        
        for strategy, factory in chunker_factories.items():
            # Create options with this strategy
            options = self._create_options_with_strategy(strategy)
            
            # Create chunker function
            chunker = factory(options)
            
            # Benchmark this configuration
            strategy_name = strategy.value
            benchmark.benchmark_strategy(
                strategy_name=strategy_name,
                chunker_func=chunker,
                test_data=test_data,
                iterations=iterations
            )
        
        # Compare results
        comparison = benchmark.compare_results()
        
        # Find best parameters
        self._find_best_params(comparison)
        
        # Save detailed results
        self.results = benchmark.results
        
        return {
            'best_strategy': self.best_params,
            'comparison': comparison,
            'detailed_report': benchmark.generate_report()
        }
    
    def _create_options_with_size(self, chunk_size: int) -> ChunkingOptions:
        """
        Create chunking options with specified chunk size
        
        Args:
            chunk_size: Target chunk size in tokens
            
        Returns:
            Modified chunking options
        """
        # Create a copy of base options with new chunk size
        options = ChunkingOptions(
            chunk_size=chunk_size,
            overlap_tokens=self.base_options.overlap_tokens,
            chunking_strategy=self.base_options.chunking_strategy,
            overlap_enforcement=self.base_options.overlap_enforcement,
            stream_buffer_size=self.base_options.stream_buffer_size
        )
        return options
    
    def _create_options_with_overlap(self, overlap: int) -> ChunkingOptions:
        """
        Create chunking options with specified overlap
        
        Args:
            overlap: Target overlap in tokens
            
        Returns:
            Modified chunking options
        """
        # Create a copy of base options with new overlap
        options = ChunkingOptions(
            chunk_size=self.base_options.chunk_size,
            overlap_tokens=overlap,
            chunking_strategy=self.base_options.chunking_strategy,
            overlap_enforcement=self.base_options.overlap_enforcement,
            stream_buffer_size=self.base_options.stream_buffer_size
        )
        return options
    
    def _create_options_with_strategy(self, strategy: ChunkingStrategy) -> ChunkingOptions:
        """
        Create chunking options with specified strategy
        
        Args:
            strategy: Chunking strategy
            
        Returns:
            Modified chunking options
        """
        # Create a copy of base options with new strategy
        options = ChunkingOptions(
            chunk_size=self.base_options.chunk_size,
            overlap_tokens=self.base_options.overlap_tokens,
            chunking_strategy=strategy,
            overlap_enforcement=self.base_options.overlap_enforcement,
            stream_buffer_size=self.base_options.stream_buffer_size
        )
        return options
    
    def _find_best_params(self, comparison: Dict[str, Any]):
        """
        Find best parameters from comparison results
        
        Args:
            comparison: Comparison results dictionary
        """
        # Use fastest by default
        self.best_params = comparison['fastest']
        self.best_score = comparison['strategies'][self.best_params]['execution_time']
        
        logger.info(f"Best performance from: {self.best_params} "
                   f"(execution time: {self.best_score:.4f}s)")
        
    def get_optimized_options(self) -> ChunkingOptions:
        """
        Get chunking options optimized for performance
        
        Returns:
            Optimized chunking options
        """
        if not self.best_params:
            logger.warning("No optimization has been performed")
            return self.base_options
            
        # Parse the best parameter
        param_str = self.best_params.lower()
        
        if param_str.startswith("chunk_size_"):
            size = int(param_str.split("_")[-1])
            return self._create_options_with_size(size)
            
        elif param_str.startswith("overlap_"):
            overlap = int(param_str.split("_")[-1])
            return self._create_options_with_overlap(overlap)
            
        elif any(s.value.lower() == param_str for s in ChunkingStrategy):
            # Find matching strategy
            for strategy in ChunkingStrategy:
                if strategy.value.lower() == param_str:
                    return self._create_options_with_strategy(strategy)
                    
        # Fallback to base options
        logger.warning(f"Could not parse best parameter: {param_str}")
        return self.base_options
</file>

<file path="utils/token_estimation.py">
"""
Token estimation utilities for text analysis

This module provides accurate estimation of token counts for different types of text,
which is essential for planning chunking strategies and managing model context limits.
"""

import re
import math
import hashlib
from functools import lru_cache
from typing import Dict, Any, Optional, List, Set, Union, Pattern, Match, Iterable, Tuple

try:
    import regex  # Try to import the third-party regex library
    HAS_REGEX_LIB = True
except ImportError:
    HAS_REGEX_LIB = False

from enterprise_chunker.models.enums import TokenEstimationStrategy
from enterprise_chunker.models.content_features import ContentFeatures
from enterprise_chunker.patterns.regex_patterns import RegexPatterns


class BaseTokenEstimator:
    """Base class for token estimation strategies"""
    
    def __init__(self):
        """Initialize the token estimator with caching and regex patterns"""
        self._token_cache = {}
        self._token_cache_keys = []
        self._cache_size = 1000
        self._patterns = RegexPatterns.get_token_estimation_patterns()
        
        # Create additional patterns for non-Latin scripts
        self._arabic_pattern = re.compile(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]')
        self._hebrew_pattern = re.compile(r'[\u0590-\u05FF\uFB1D-\uFB4F]')
    
    def estimate(self, text: str) -> int:
        """
        Estimate tokens in text
        
        Args:
            text: Input text
            
        Returns:
            Estimated token count
        """
        # Handle empty or trivial inputs
        if not text:
            return 0
        if len(text) <= 1:
            return 1
        
        # For long inputs, use cache
        if len(text) > 100:
            # Generate a cache key based on length and samplings
            # Take samples from beginning, middle, and end for better cache hit rate
            middle_idx = len(text) // 2
            cache_sample = f"{text[:20]}:{text[middle_idx-10:middle_idx+10]}:{text[-20:]}"
            cache_key = f"{len(text)}:{hashlib.md5(cache_sample.encode()).hexdigest()}"
            
            if cache_key in self._token_cache:
                return self._token_cache[cache_key]
        else:
            cache_key = None
        
        # Extract features for estimation
        features = self._extract_text_features(text)
        
        # Calculate estimate based on features
        count = self._calculate_estimate(features, text)
        
        # Update cache for non-trivial inputs
        if cache_key:
            # LRU-like cache management
            if len(self._token_cache) >= self._cache_size:
                old_key = self._token_cache_keys.pop(0)
                self._token_cache.pop(old_key, None)
                
            self._token_cache[cache_key] = count
            self._token_cache_keys.append(cache_key)
            
        return count
    
    def _extract_text_features(self, text: str) -> ContentFeatures:
        """
        Extract features from text for token estimation
        
        Args:
            text: Input text
            
        Returns:
            ContentFeatures object with extracted features
        """
        # Handle empty text
        if not text:
            return ContentFeatures(
                length=0,
                word_count=0,
                whitespace_ratio=0,
                symbol_density=0,
                has_cjk=False,
                has_emoji=False,
                avg_word_length=0
            )
            
        # Basic counts
        word_matches = re.findall(r'\b\w+\b', text)
        word_count = len(word_matches)
        whitespace_count = len(self._patterns['whitespace'].findall(text))
        symbol_count = len(self._patterns['punctuation'].findall(text))
        
        # Calculate ratios
        whitespace_ratio = whitespace_count / max(1, len(text))  # Avoid division by zero
        symbol_density = symbol_count / max(1, len(text))  # Avoid division by zero
        
        # Language checks
        has_cjk = bool(self._patterns['cjk'].search(text))
        has_emoji = bool(self._patterns['emoji'].search(text))
        has_arabic = bool(self._arabic_pattern.search(text))
        has_hebrew = bool(self._hebrew_pattern.search(text))
        
        # Calculate average word length
        avg_word_length = (
            sum(len(word) for word in word_matches) / max(1, word_count)  # Avoid division by zero
        )
        
        # Approximate sentence count
        sentences_count = len(re.findall(r'[.!?]+\s+', text)) + 1
        
        # Check for markdown features - use native format detection from RegexPatterns
        from enterprise_chunker.models.enums import ContentFormat
        format_patterns = RegexPatterns.get_format_patterns(ContentFormat.MARKDOWN)  # Use markdown format
        has_code_blocks = bool(format_patterns['code_blocks'].search(text))
        has_list_items = bool(format_patterns['list_items'].search(text))
        has_headings = bool(format_patterns['headers'].search(text))
        has_tables = bool(re.search(r'\|.*\|.*\n\|[-:]+\|', text))
        
        features = ContentFeatures(
            length=len(text),
            word_count=word_count,
            whitespace_ratio=whitespace_ratio,
            symbol_density=symbol_density,
            has_cjk=has_cjk,
            has_emoji=has_emoji,
            avg_word_length=avg_word_length,
            sentences_count=sentences_count,
            has_code_blocks=has_code_blocks,
            has_list_items=has_list_items,
            has_tables=has_tables,
            has_headings=has_headings
        )
        
        # Store Arabic/Hebrew detection as attributes even though they're not in the dataclass
        setattr(features, 'has_arabic', has_arabic)
        setattr(features, 'has_hebrew', has_hebrew)
        
        return features
    
    def _calculate_estimate(self, features: ContentFeatures, text: str) -> int:
        """
        Calculate estimate based on features (to be overridden)
        
        Args:
            features: Content features
            text: Original text
            
        Returns:
            Estimated token count
        """
        raise NotImplementedError("Subclasses must implement this method")
    
    def _count_emoji(self, text: str) -> Tuple[int, int]:
        """
        Count emojis and their byte length in text
        
        Args:
            text: Input text
            
        Returns:
            Tuple of (emoji_count, total_emoji_bytes)
        """
        emoji_pattern = self._patterns['emoji']
        emoji_matches = emoji_pattern.findall(text)
        
        # Calculate total bytes used by emojis
        total_emoji_bytes = sum(len(emoji.encode('utf-8')) for emoji in emoji_matches)
        
        return len(emoji_matches), total_emoji_bytes


class PrecisionTokenEstimator(BaseTokenEstimator):
    """High-precision token estimation strategy"""
    
    def _calculate_estimate(self, features: ContentFeatures, text: str) -> int:
        """
        Calculate high-precision token estimate
        
        Args:
            features: Content features
            text: Original text
            
        Returns:
            Estimated token count
        """
        # Start with word count as a baseline
        base_estimate = features.word_count * 1.25  # Average token-per-word ratio
        
        # Adjust for different content characteristics
        base_estimate *= 1 + (features.symbol_density * 0.2)
        
        # Language-specific adjustments
        if features.has_cjk:  # CJK languages have different tokenization
            cjk_pattern = self._patterns['cjk']
            cjk_chars = sum(len(match.group(0)) for match in cjk_pattern.finditer(text))
            base_estimate += cjk_chars * 0.8  # CJK chars often tokenize 1:1
        
        # Handle emoji (which often use multiple tokens)
        if features.has_emoji:
            emoji_count, emoji_bytes = self._count_emoji(text)
            # Emojis typically use more tokens than regular characters
            emoji_tokens = emoji_count * 2.0  # Conservative estimate
            base_estimate += emoji_tokens
        
        # Handle Arabic and Hebrew scripts
        if getattr(features, 'has_arabic', False) or getattr(features, 'has_hebrew', False):
            # Count Arabic/Hebrew characters
            arabic_chars = sum(1 for _ in self._arabic_pattern.finditer(text))
            hebrew_chars = sum(1 for _ in self._hebrew_pattern.finditer(text))
            # Adjust the estimate - these often tokenize differently than Latin script
            base_estimate += (arabic_chars + hebrew_chars) * 0.5
        
        # Adjust for whitespace and punctuation
        whitespace_count = len(self._patterns['whitespace'].findall(text))
        punctuation_count = len(self._patterns['punctuation'].findall(text))
        
        base_estimate += whitespace_count * 0.1
        base_estimate += punctuation_count * 0.3
        
        # Special case for code
        if features.has_code_blocks or re.search(r'(?:function|class|def|if|for|while|var|let|const)\s', text):
            # Code tends to have more specialized tokens
            base_estimate *= 1.15
        
        return math.ceil(base_estimate)


class BalancedTokenEstimator(BaseTokenEstimator):
    """Balanced token estimation strategy"""
    
    def _calculate_estimate(self, features: ContentFeatures, text: str) -> int:
        """
        Calculate balanced token estimate (compromise between speed and accuracy)
        
        Args:
            features: Content features
            text: Original text
            
        Returns:
            Estimated token count
        """
        # Normalize whitespace for more consistent results
        normalized_text = re.sub(r'\s+', ' ', text)
        
        # Perform a basic language check to choose better defaults
        non_latin_chars = len(re.findall(r'[^\x00-\x7F]', normalized_text))
        non_latin_ratio = non_latin_chars / max(1, len(normalized_text))  # Avoid division by zero
        
        # Determine appropriate chars per token based on content
        if non_latin_ratio > 0.5:
            # Likely CJK or other non-Latin script
            chars_per_token = 1.5
        elif re.search(r'\S+', normalized_text) and not re.search(r'\s', normalized_text):
            # Content with no whitespace (e.g., URLs, identifiers)
            chars_per_token = 3.0
        elif re.search(r'[\{\}\[\]()=><]', normalized_text):
            # Content with programming symbols
            chars_per_token = 3.5
        else:
            # Standard English text
            chars_per_token = 4.0
        
        # Check for emoji (which tokenize differently)
        emoji_count, emoji_bytes = self._count_emoji(normalized_text)
        
        if emoji_count > 0:
            # Calculate token estimate without emoji bytes
            text_bytes = len(normalized_text.encode('utf-8'))
            non_emoji_bytes = text_bytes - emoji_bytes
            
            # Base token count from non-emoji characters
            # Convert back to approximate char count
            non_emoji_chars = non_emoji_bytes / 2  # Rough approximation for UTF-8
            token_count = non_emoji_chars / chars_per_token
            
            # Add emoji tokens (each emoji typically takes 1-2 tokens)
            emoji_tokens = emoji_count * 1.5  # Conservative average
            
            return math.ceil(token_count + emoji_tokens)
        else:
            # No emoji - use simple character-based estimation
            return math.ceil(len(normalized_text) / chars_per_token)


class PerformanceTokenEstimator(BaseTokenEstimator):
    """Performance-optimized token estimation strategy"""
    
    def _calculate_estimate(self, features: ContentFeatures, text: str) -> int:
        """
        Calculate fast token estimate
        
        Args:
            features: Content features
            text: Original text
            
        Returns:
            Estimated token count
        """
        # Simple character-based estimation based on approximate chars per token
        chars_per_token = 4.0  # Typical ratio for English text
        
        # Adjust for language characteristics
        if self._patterns['cjk'].search(text):
            # CJK languages have much different tokenization
            chars_per_token = 1.5
        elif self._arabic_pattern.search(text) or self._hebrew_pattern.search(text):
            # Arabic and Hebrew scripts
            chars_per_token = 3.0
        
        # Handle emoji if present
        if features.has_emoji:
            emoji_count, emoji_bytes = self._count_emoji(text)
            
            # Subtract emoji bytes and calculate tokens separately
            text_bytes = len(text.encode('utf-8'))
            non_emoji_bytes = max(0, text_bytes - emoji_bytes)  # Ensure non-negative
            
            # Estimate tokens for non-emoji content
            non_emoji_tokens = (non_emoji_bytes / 4) / chars_per_token  # Approx 4 bytes per UTF-8 char on average
            
            # Add emoji tokens
            emoji_tokens = emoji_count * 1.5  # Simple multiplier for emojis
            
            return math.ceil(non_emoji_tokens + emoji_tokens)
            
        # Basic estimate
        return math.ceil(len(text) / chars_per_token)


class TokenEstimatorFactory:
    """Factory for creating token estimators based on strategy"""
    
    _instances = {}  # Cache for singleton-like behavior
    
    @classmethod
    def create_estimator(cls, strategy: TokenEstimationStrategy) -> BaseTokenEstimator:
        """
        Create token estimator for the specified strategy
        
        Args:
            strategy: Token estimation strategy
            
        Returns:
            Token estimator instance
        """
        # Use cached instance if available for better performance
        if strategy in cls._instances:
            return cls._instances[strategy]
            
        # Create new instance if needed
        if strategy == TokenEstimationStrategy.PRECISION:
            estimator = PrecisionTokenEstimator()
        elif strategy == TokenEstimationStrategy.PERFORMANCE:
            estimator = PerformanceTokenEstimator()
        else:  # BALANCED (default)
            estimator = BalancedTokenEstimator()
            
        # Cache the instance
        cls._instances[strategy] = estimator
        return estimator


@lru_cache(maxsize=128)
def estimate_tokens(text: str, strategy: TokenEstimationStrategy = TokenEstimationStrategy.BALANCED) -> int:
    """
    Cached function for estimating tokens in text
    
    Args:
        text: Input text
        strategy: Estimation strategy to use
        
    Returns:
        Estimated token count
    """
    estimator = TokenEstimatorFactory.create_estimator(strategy)
    return estimator.estimate(text)


# Advanced token estimation using the 'regex' library if available
if HAS_REGEX_LIB:
    # Create enhanced patterns with proper Unicode property support
    class EnhancedTokenEstimation:
        """Advanced token estimation utilities using the 'regex' library"""
        
        # Initialize once as class variables
        ARABIC_PATTERN = regex.compile(r'\p{Arabic}')
        HEBREW_PATTERN = regex.compile(r'\p{Hebrew}')
        CJK_PATTERN = regex.compile(r'\p{Han}|\p{Hangul}|\p{Hiragana}|\p{Katakana}')
        EMOJI_PATTERN = regex.compile(r'\p{Emoji}')
        
        @classmethod
        def estimate_with_unicode_props(cls, text: str) -> int:
            """
            Estimate tokens with full Unicode property support
            
            Args:
                text: Input text
                
            Returns:
                Estimated token count
            """
            # Determine script composition
            has_arabic = bool(cls.ARABIC_PATTERN.search(text))
            has_hebrew = bool(cls.HEBREW_PATTERN.search(text))
            has_cjk = bool(cls.CJK_PATTERN.search(text))
            has_emoji = bool(cls.EMOJI_PATTERN.search(text))
            
            # Base character-to-token ratio
            if has_cjk:
                chars_per_token = 1.5
            elif has_arabic or has_hebrew:
                chars_per_token = 3.0
            else:
                chars_per_token = 4.0
                
            # Basic token count
            token_count = len(text) / chars_per_token
            
            # Emoji adjustment
            if has_emoji:
                emoji_matches = cls.EMOJI_PATTERN.findall(text)
                emoji_count = len(emoji_matches)
                token_count += emoji_count * 0.5  # Additional tokens for emoji
                
            return math.ceil(token_count)
    
    # Enhanced estimation function
    def enhanced_estimate_tokens(text: str) -> int:
        """
        Estimate tokens using enhanced Unicode support
        
        Args:
            text: Input text
            
        Returns:
            Estimated token count
        """
        return EnhancedTokenEstimation.estimate_with_unicode_props(text)
</file>

</files>
